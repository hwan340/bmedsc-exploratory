{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This notebook contains the ground-truth copy of the code mixed with literate comments. All code in this directory is produced by running the cells in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cortical visual prostheses** (CVPs) are devices implanted on the brain which aim to restore vision to blind people by stimulating the visual cortex.\n",
    "\n",
    "Experiments have shown that stimulating the visual cortex produces visual percepts in both blind and sighted patients. These visual percepts are called **phosphenes** and are typically described as small round dots of colourless light like \"a star in the sky.\" The idea behind most CVPs is to build images out of phosphenes, like how graphical displays are made out of pixels. \n",
    "\n",
    "But phosphenes are very limited. At the moment, we can only control the intensity of phosphenes (and, to some extent, their size). The locations of phosphenes follow the retinotopic mapping of the visual fields on the cortex, but the complexity of this mapping makes precisely controlling the positions of phosphenes in the visual field very difficult; it is much easier to instead place a regular grid of electrodes on the brain and map the phosphene locations after implantation. Other properties of phosphenes, such as their shape, individual brightness or colour, are highly variable and uncontrollable.\n",
    "\n",
    "In addition, there are no empirical studies which describe what exactly is seen when we try to stimulate more than about five simultaneous phosphenes. The largest implant in a human used 64 electrodes, but there is inadequate description on what the implantee saw and how useful these percepts were (though the implantee purportedly was able to read large letterforms). New CVPs generally intend to stimulate on the order of hundreds of electrodes (such as 473 for the Monash Vision Group); it is not yet known what people will perceive when many electrodes are stimulate at once. Some studies have suggested that stimulating five electrodes, for example, does not produce five discrete phosphenes or a blended picture; rather, phosphenes which were stimulated by an electrode when only one electrode was stimulated completely disappear upon simultaneous stimulation as though they were not stimulated at all.\n",
    "\n",
    "This begs the question: **how can phosphenes be used to convey useful information, given their known and unknown perceptual limitations?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern-Based Approach\n",
    "\n",
    "Using a pattern-based approach, we focus on **object identification**. That is, we attempt to create recognisable patterns that are matchable against object identities, and discard any location-based information.\n",
    "\n",
    "The advantages of this approach are that:\n",
    "\n",
    "1. By focusing on identity rather than location, stable patterns can be presented using the same phosphenes each time an object occurs in a scene. As the exact same pattern is presented each time, it may be easier to learn.\n",
    "\n",
    "The disadvantages of this approach are that:\n",
    "\n",
    "1. Only one object at a time can be shown.\n",
    "2. If there is no location information, it may be hard to justify how this is useful in comparison to something less invasive; for example, an external camera linked to an earpiece could easily just speak out the identity of an object instead of presenting patterned vision. You could argue that presenting object identities through vision is beneficial because it doesn't take up another sensory modality, but this is a relatively small upside compared to the invasiveness of a brain implant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location-Based Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a location-based approach, we focus on **object localisation**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rationale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phosphene Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Phosphene Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {
    "code_folding": [
     32,
     79,
     164,
     249,
     278,
     315,
     379
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting phosphenes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile phosphenes.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from math import e\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from skimage import color\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "def safebound(value: float, width: float, lower: float, upper: float):\n",
    "    \"\"\" \n",
    "    Returns the bounded min and max about value with width.\n",
    "    \"\"\"\n",
    "    vmin = int(max(lower, value - width / 2))\n",
    "    vmax = int(min(upper, value + width / 2))\n",
    "    return vmin, vmax\n",
    "\n",
    "def bound(value:float, lower: float, upper:float):\n",
    "    \"\"\"\n",
    "    Returns a bounded value.\n",
    "    \"\"\"\n",
    "    if value > lower:\n",
    "        if value < upper:\n",
    "            return value\n",
    "        return upper\n",
    "    return lower\n",
    "\n",
    "# Electrodes, which produce phosphenes.\n",
    "\n",
    "class Electrode:\n",
    "    \"\"\"\n",
    "    Produces a phosphene for a single electrode.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 x: float,\n",
    "                 y: float,\n",
    "                 xsize: float,\n",
    "                 ysize: float,\n",
    "                 strength: float,\n",
    "                 xdim: int,\n",
    "                 ydim: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: float         - position in range [0, 1]. \n",
    "            y: float         - position in range [0, 1]\n",
    "            xsize: int       - x size of the electrode (in units of output image)\n",
    "            ysize: int       - y size of the electrode (in units of output image)\n",
    "            strength: float  - relative brightness of the electrode in range [0, 1]\n",
    "            xdim: int        - x dim of the output image\n",
    "            ydim: int        - y dim of the output image \n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.xdim = xdim\n",
    "        self.ydim = ydim\n",
    "        self.xsize = xsize\n",
    "        self.ysize = ysize\n",
    "        self.strength = strength\n",
    "\n",
    "        self.rendered = self.render()\n",
    "\n",
    "    def render(self):\n",
    "        xmin, xmax = safebound(self.xdim * self.x, self.xsize, 0, self.xdim)\n",
    "        ymin, ymax = safebound(self.ydim * self.y, self.ysize, 0, self.ydim)\n",
    "\n",
    "        base = np.zeros((self.ydim, self.xdim))\n",
    "        base[ymin:ymax, xmin:xmax] = self.strength\n",
    "        \n",
    "        blurred = gaussian_filter(base, (self.xsize * self.ysize) ** 0.5, mode='constant')\n",
    "        \n",
    "        # Rescale up to 1\n",
    "        if blurred.max() <= 0:\n",
    "            return blurred\n",
    "        else:\n",
    "            return blurred / blurred.max()\n",
    "\n",
    "class DistortedElectrode(Electrode):\n",
    "    \"\"\"\n",
    "    This class introduced random distortions to the rendered phosphene.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 x: float,\n",
    "                 y: float,\n",
    "                 xsize: float,\n",
    "                 ysize: float,\n",
    "                 xdim: int,\n",
    "                 ydim: int):\n",
    "        \n",
    "        x = bound(self.randomise(x), 0, 1)\n",
    "        y = bound(self.randomise(y), 0, 1)\n",
    "        xsize = max(0, int(self.randomise(xsize)))\n",
    "        ysize = max(0, int(self.randomise(ysize)))\n",
    "        strength = random.random()\n",
    "        \n",
    "        Electrode.__init__(self, x, y, xsize, ysize, strength, xdim, ydim)\n",
    "        \n",
    "    def randomise(self, value):\n",
    "        randomised = value * (1 + (random.random() - 1) / 10)\n",
    "        return randomised\n",
    "\n",
    "# Grids, which are composed of electrodes.\n",
    "\n",
    "class Grid(ABC): \n",
    "    def __init__(self,\n",
    "                 ndim1: int, \n",
    "                 ndim2: int, \n",
    "                 xdim: int, \n",
    "                 ydim: int):\n",
    "        \"\"\"\n",
    "        Base class for a rendering grid.\n",
    "        \n",
    "        Args:\n",
    "            ndim1: int - number of electrodes for dimension 1\n",
    "            ndim2: int - number of electrodes for dimension 2\n",
    "            xdim: int  - x dimension of output image\n",
    "            ydim: int  - y dimension of output image\n",
    "        \"\"\"\n",
    "        self.ndim1 = ndim1\n",
    "        self.ndim2 = ndim2\n",
    "        self.vector_size = ndim1 * ndim2\n",
    "        self.xdim = xdim\n",
    "        self.ydim = ydim\n",
    "        \n",
    "        self.grid = self.make_grid()\n",
    "        self.prerendered = np.array([electrode.rendered for electrode in self.grid])\n",
    "        self.prerendered_tensor = tf.convert_to_tensor(self.prerendered, dtype=tf.float32)\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "    @abstractmethod\n",
    "    def make_grid(self):\n",
    "        pass\n",
    "    \n",
    "    def render(self, values: np.ndarray):\n",
    "        \n",
    "        # Multiply the values with the renders and sum\n",
    "        product = values.reshape(self.vector_size, 1, 1) * self.prerendered\n",
    "        summed = sum(product)\n",
    "\n",
    "        # Clip, then scale between -1 and 1\n",
    "        scaled = np.clip(summed, 0, 1) * 2 - 1\n",
    "        \n",
    "        return scaled\n",
    "    \n",
    "    def render_tensor(self, tensor):\n",
    "        \n",
    "        # Preprocessing\n",
    "        tiled = tf.tile(tensor, tf.constant([self.xdim]))\n",
    "        reshaped = tf.reshape(tiled, (self.xdim, self.vector_size, 1))\n",
    "        transposed = tf.transpose(reshaped, perm=[1, 0, 2])\n",
    "        \n",
    "        # Multiply the values with the renders and sum\n",
    "        product = transposed * self.prerendered_tensor\n",
    "        summed = tf.reduce_sum(product, axis=0)\n",
    "        \n",
    "        # Clip, then scale by -1 and 1\n",
    "        scaled = tf.clip_by_value(summed, 0, 1) * 2 - 1\n",
    "        \n",
    "        return scaled\n",
    "        \n",
    "\n",
    "class CartesianGrid(Grid):\n",
    "    \"\"\"\n",
    "    A regular grid of electrodes with even spacing and even size. \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 nxelectrode: int,\n",
    "                 nyelectrode: int,\n",
    "                 xdim: int,\n",
    "                 ydim: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            nxelectrode: int - number of electrodes on x axis\n",
    "            nyelectrode: int - number of electrodes on y axis\n",
    "            xdim: int       - output x dimension of image\n",
    "            ydim: int       - output y dimension of image\n",
    "        \"\"\"\n",
    "        Grid.__init__(self, nxelectrode, nyelectrode, xdim, ydim)\n",
    "        \n",
    "    def make_grid(self):\n",
    "        \n",
    "        grid = [\n",
    "            Electrode(x = x / self.ndim1,\n",
    "                      y = y / self.ndim2,\n",
    "                      xsize = np.sqrt(self.xdim // self.ndim1),\n",
    "                      ysize = np.sqrt(self.ydim // self.ndim2),\n",
    "                      strength = 1,\n",
    "                      xdim = self.xdim,\n",
    "                      ydim = self.ydim)\n",
    "            for x in range(self.ndim1)\n",
    "            for y in range(self.ndim2)\n",
    "        ]\n",
    "        \n",
    "        return grid\n",
    "\n",
    "class PolarGrid(Grid):\n",
    "    \"\"\"\n",
    "    A polar regular grid of electrodes with even spacing \n",
    "    and size increasing with eccentricity. \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 nradius: int,\n",
    "                 ntheta: int,\n",
    "                 xdim: int,\n",
    "                 ydim: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            nradius: int - number of radii to place electrodes on\n",
    "            ntheta: int  - number of angles to place electrodes on\n",
    "            xdim: int    - output x dimension of image\n",
    "            ydim: int    - output y dimension of image\n",
    "        \"\"\"\n",
    "        Grid.__init__(self, nradius, ntheta, xdim, ydim)\n",
    "        \n",
    "    def iangle(self, i):\n",
    "        \"\"\"\n",
    "        Calculates the angle for angle of index i in range(self.ndim2)\n",
    "        \"\"\"\n",
    "        angle = (np.pi / (self.ndim2 - 1) * i) - (np.pi / 2)\n",
    "        return angle\n",
    "        \n",
    "    def make_grid(self):\n",
    "        \n",
    "        k = self.xdim / 2 + self.ydim / 2\n",
    "        a = e * (self.xdim + self.ydim) / 128\n",
    "        \n",
    "        xys = [\n",
    "            (0.5 + (ir / self.ndim1 * np.cos(self.iangle(itheta))) / 2,\n",
    "             0.5 + (ir / self.ndim1 * np.sin(self.iangle(itheta))) / 2,)\n",
    "            for ir in range(1, self.ndim1 + 1)\n",
    "            for itheta in range(self.ndim2)\n",
    "        ]\n",
    "        \n",
    "        grid = [\n",
    "            Electrode(x = x,\n",
    "                      y = y,\n",
    "                      xsize = np.log(k * ((x-0.5)**2 + (y-0.5)**2) + a),\n",
    "                      ysize = np.log(k * ((x-0.5)**2 + (y-0.5)**2) + a),\n",
    "                      strength = 1,\n",
    "                      xdim = self.xdim,\n",
    "                      ydim = self.ydim)\n",
    "            for (x, y) in xys\n",
    "        ]\n",
    "        \n",
    "        return grid\n",
    "    \n",
    "class DistortedPolarGrid(PolarGrid):\n",
    "    \"\"\"\n",
    "    A polar grid with distorted electrodes.\n",
    "    \"\"\"\n",
    "        \n",
    "    def make_grid(self):\n",
    "        \n",
    "        k = self.xdim / 2 + self.ydim / 2\n",
    "        a = e * (self.xdim + self.ydim) / 128\n",
    "        \n",
    "        xys = [\n",
    "            (0.5 + (ir / self.ndim1 * np.cos(self.iangle(itheta))) / 2,\n",
    "             0.5 + (ir / self.ndim1 * np.sin(self.iangle(itheta))) / 2,)\n",
    "            for ir in range(1, self.ndim1 + 1)\n",
    "            for itheta in range(self.ndim2)\n",
    "        ]\n",
    "        \n",
    "        grid = [\n",
    "            DistortedElectrode(x = x,\n",
    "                               y = y,\n",
    "                               xsize = np.log(k * ((x-0.5)**2 + (y-0.5)**2) + a),\n",
    "                               ysize = np.log(k * ((x-0.5)**2 + (y-0.5)**2) + a),\n",
    "                               xdim = self.xdim,\n",
    "                               ydim = self.ydim)\n",
    "            for (x, y) in xys\n",
    "        ]\n",
    "        \n",
    "        return grid\n",
    "    \n",
    "class RescalingDistortedPolarGrid(DistortedPolarGrid):\n",
    "    \"\"\"\n",
    "    A polar grid with distorted electrodes and non-summative rendering\n",
    "    (rendering rescales the brightness to max). \n",
    "    \"\"\"\n",
    "    \n",
    "    def render(self, values):\n",
    "        \n",
    "        # Multiply the values with the renders and sum\n",
    "        product = values.reshape(self.vector_size, 1, 1) * self.prerendered\n",
    "        summed = sum(product)\n",
    "        summax = np.max(summed)\n",
    "\n",
    "        # Rescale, then scale between -1 and 1\n",
    "        scaled = (summed ** 2 / summax ** 2) * 2 - 1\n",
    "        \n",
    "        return scaled\n",
    "    \n",
    "    def render_tensor(self, tensor):\n",
    "        \n",
    "        # Preprocessing\n",
    "        tiled = tf.tile(tensor, tf.constant([self.xdim]))\n",
    "        reshaped = tf.reshape(tiled, (self.xdim, self.vector_size, 1))\n",
    "        transposed = tf.transpose(reshaped, perm=[1, 0, 2])\n",
    "        \n",
    "        # Multiply the values with the renders and sum\n",
    "        product = transposed * self.prerendered_tensor\n",
    "        summed = tf.reduce_sum(product, axis=0)\n",
    "        summax = tf.reduce_max(summed)\n",
    "        \n",
    "        # Rescale, then scale between -1 and 1\n",
    "        scaled = tf.divide(summed ** 2, summax ** 2) * 2 - 1\n",
    "        \n",
    "        return scaled      \n",
    "        \n",
    "# STIMULUS\n",
    "\n",
    "class Stimulus:\n",
    "    def __init__(self, image, grid, xpos=0, ypos=0):\n",
    "        self.shape = image.shape\n",
    "        \n",
    "        if len(self.shape) == 2:\n",
    "            self.original = image.reshape(*self.shape, 1)\n",
    "            self.shape = self.original.shape\n",
    "        else:\n",
    "            self.original = image\n",
    "            \n",
    "        # Normalise between -1 and 1 for an RGB255 image\n",
    "        self.original = (self.original / 127.5) - 1\n",
    "        \n",
    "        self.padder = np.zeros((3 * self.shape[0], 3 * self.shape[1], self.shape[2])) - 1\n",
    "        self.padder[self.shape[0]:2*self.shape[0], self.shape[1]:2*self.shape[1], :] = self.original\n",
    "        \n",
    "        self.xpos = xpos\n",
    "        self.ypos = ypos\n",
    "        \n",
    "        self.image = self.getImage()\n",
    "        \n",
    "        self.grid = grid\n",
    "        self.sampleWidth = 6\n",
    "        \n",
    "        self.vector = self.process()\n",
    "            \n",
    "    def get_params(self, x : float, y : float):\n",
    "        \n",
    "        ymin = bound(int(self.shape[0] * y - self.sampleWidth // 2), 0, self.shape[0] - 1)\n",
    "        ymax = bound(int(self.shape[0] * y + self.sampleWidth // 2), 0, self.shape[0] - 1)\n",
    "        xmin = bound(int(self.shape[1] * x - self.sampleWidth // 2), 0, self.shape[1] - 1)            \n",
    "        xmax = bound(int(self.shape[1] * x + self.sampleWidth // 2), 0, self.shape[1] - 1)\n",
    "\n",
    "        vals  = self.image[ymin:ymax, xmin:xmax, :]\n",
    "        return np.mean(vals)\n",
    "    \n",
    "    def getImage(self):\n",
    "        \"\"\" Based on xpos and ypos, get the image view from the padder.\n",
    "        \"\"\"\n",
    "        \n",
    "        xstart = self.shape[0] - int(self.xpos * self.shape[0])\n",
    "        ystart = self.shape[1] - int(self.ypos * self.shape[1])\n",
    "        \n",
    "        return self.padder[ystart:ystart+self.shape[1], xstart:xstart+self.shape[0], :]\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\" Converts the stimulus into a brightness vector for the\n",
    "        \"\"\"\n",
    "\n",
    "        params = np.array([self.get_params(e.x, e.y) for e in self.grid.grid])\n",
    "        # Normalise to between 0 and 1\n",
    "        params = params - (np.min(params))\n",
    "        if np.max(params) > 0:\n",
    "            params /= np.max(params)\n",
    "        return params\n",
    "    \n",
    "    def setPos(self, xpos: float, ypos: float):\n",
    "        \"\"\"Translate the image. xpos and ypos lie in the range (-1, 1)\n",
    "        \"\"\"\n",
    "        self.xpos = xpos\n",
    "        self.ypos = ypos\n",
    "        self.image = self.getImage()\n",
    "        self.vector = self.process()\n",
    "\n",
    "class StimulusNet(Stimulus):\n",
    "\n",
    "    def __init__(self, image, grid, encoder_path):\n",
    "        self.encoder = tf.keras.models.load_model(encoder_path)        \n",
    "        Stimulus.__init__(self, image, grid)\n",
    "    \n",
    "    def process(self):\n",
    "        image_tensor = tf.convert_to_tensor(np.array([self.image]), dtype=tf.float32)\n",
    "        return self.encoder(image_tensor).numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run phosphenes.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Digit Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting digits.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile digits.py\n",
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "This script runs a digit recognition psychophysics session.\n",
    "\"\"\"\n",
    "\n",
    "# # Setup\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2\n",
    "import pickle\n",
    "from phosphenes import *\n",
    "from datetime import datetime\n",
    "from argparse import ArgumentParser\n",
    "from psychopy import visual, core, gui, event\n",
    "from box import Box\n",
    "from psychopy.sound.backend_pygame import SoundPygame\n",
    "from psychopy.tools.filetools import fromFile, toFile\n",
    "from skimage import color\n",
    "from imageio import imread\n",
    "from random import random, choices\n",
    "from PIL import Image\n",
    "\n",
    "# I'm setting up a config dictionary with dot-syntax so it can be serialised \n",
    "# and saved with the session. I prefer explicitly keeping track of state.\n",
    "\n",
    "config = Box({})\n",
    "\n",
    "# Parsing the command line arguments, especially for testing.\n",
    "parser = ArgumentParser(description='Digit recognition task.')\n",
    "\n",
    "# Define command line arguments.\n",
    "argspec = {\n",
    "    'testing': {\n",
    "        'action': 'store_const',\n",
    "        'const': True,\n",
    "        'default': False,\n",
    "        'dest': 'testing',\n",
    "        'help': 'Test the experiment and save the data.'\n",
    "    },\n",
    "    'ntrials': {\n",
    "        'type': int,\n",
    "        'nargs': '?',\n",
    "        'default': 5,\n",
    "        'help': 'Number of trials for the experiment.'\n",
    "    },\n",
    "    'ncues': {\n",
    "        'type': int,\n",
    "        'nargs': '?',\n",
    "        'default': 10,\n",
    "        'help': 'Number of cues per trial. Should be a multiple of 10 (for now) for digit stream.'\n",
    "    },\n",
    "    'grid': {\n",
    "        'type': str,\n",
    "        'nargs': '?',\n",
    "        'default': 'polar',\n",
    "        'help': 'The grid type for rendering. One of cartesian, polar, distortedPolar, or rescalingDistortedPolar, or a filepath to the grid to load.'\n",
    "    },\n",
    "    'processor': {\n",
    "        'type': str,\n",
    "        'nargs': '?',\n",
    "        'default': 'direct',\n",
    "        'help': 'The processor for the session. One of direct or net.'\n",
    "    },\n",
    "    'encoder': {\n",
    "        'type': str,\n",
    "        'nargs': '?',\n",
    "        'default': None,\n",
    "        'help': 'If the processor is a net, specify the filepath of the encoder to be used. '\n",
    "    },\n",
    "    'no-numpad': {\n",
    "        'action': 'store_const',\n",
    "        'const': True,\n",
    "        'default': False,\n",
    "        'dest': 'noNumpad',\n",
    "        'help': 'Flags that normal number keys instead of numpad should be used.'\n",
    "    },\n",
    "    'with-scanning': {\n",
    "        'action': 'store_const',\n",
    "        'const': True,\n",
    "        'default': False,\n",
    "        'dest': 'withScanning',\n",
    "        'help': 'Flags that scanning with the mouse should be enabled.'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add arguments to the parser.\n",
    "[parser.add_argument(f'--{k}', **v) for k, v in argspec.items()]\n",
    "\n",
    "# Parse the arguments and save into config.\n",
    "args = parser.parse_args()\n",
    "config.TESTING        = args.testing\n",
    "config.NTRIALS        = args.ntrials\n",
    "config.NCUES          = args.ncues\n",
    "config.GRID_TYPE      = args.grid\n",
    "config.PROCESSOR_TYPE = args.processor\n",
    "config.NO_NUMPAD      = args.noNumpad\n",
    "config.ENCODER        = args.encoder\n",
    "config.WITH_SCANNING  = args.withScanning\n",
    "\n",
    "\n",
    "# First, we define the constants for the window size of the experiment.\n",
    "# `XSIZE` and `YSIZE` refer to the size of the window on the screen.\n",
    "# `EXSIZE` and `EYSIZE` refer to the size of the image data (i.e. how many \n",
    "# electrodes there are). \n",
    "# `SCALE` links the two. \n",
    "\n",
    "config.XSIZE  = 100\n",
    "config.YSIZE  = 100\n",
    "config.SCALE  = 10\n",
    "config.EXSIZE = config.XSIZE // config.SCALE\n",
    "config.EYSIZE = config.YSIZE // config.SCALE\n",
    "config.INPUT_XSIZE = 64\n",
    "config.INPUT_YSIZE = 64\n",
    "\n",
    "# Next, we load the stimulus. Opening the image files can be expensive\n",
    "# so we're doing at this at the very start and loading them into a \n",
    "# variable. \n",
    "\n",
    "# `IMAGE_TEMPLATE` is a string of the filepath of the stimulus digit images.\n",
    "config.IMAGE_TEMPLATE = './data/digit-images-aliased/{}.png'\n",
    "\n",
    "# `IMAGE_SIZE` is an (int, int) tuple of the image size of the first image.\n",
    "# We assume that each image is of the same size as the image labelled \"0\"\n",
    "config.IMAGE_SIZE = np.shape(imread(config.IMAGE_TEMPLATE.format(0)))  \n",
    "\n",
    "# `IMAGE_SCALE` is an int describing the ratio of electrode size to image size.\n",
    "# It assumes that EXSIZE == EYSIZE and the input images are square.\n",
    "# This may need changing later. \n",
    "config.IMAGE_SCALE = config.EXSIZE / config.IMAGE_SIZE[0]  \n",
    "\n",
    "# `IMAGES` holds the original digit images.\n",
    "config.IMAGES = [cv2.cvtColor(cv2.resize(imread(config.IMAGE_TEMPLATE.format(digit)),\n",
    "                                         dsize=(config.INPUT_XSIZE, config.INPUT_YSIZE)),\n",
    "                              cv2.COLOR_RGBA2RGB)\n",
    "                            for digit in range(10)]\n",
    "\n",
    "# `STIMULI` contains a list of numpy arrays.\n",
    "# Each element in the list holds the image data \n",
    "# for the digit equal to its index. Normalised to \n",
    "# between -1 and 1\n",
    "config.STIMULI = [\n",
    "    np.array(Image.fromarray(image).resize((config.EXSIZE, config.EYSIZE)))\n",
    "    for image in config.IMAGES\n",
    "]\n",
    "\n",
    "# We initiate a grid of electrodes.\n",
    "grids = {\n",
    "    'cartesian': lambda: CartesianGrid(\n",
    "        config.EXSIZE,\n",
    "        config.EYSIZE, \n",
    "        config.XSIZE, \n",
    "        config.YSIZE\n",
    "    ),\n",
    "    'polar': lambda: PolarGrid(\n",
    "        config.EXSIZE, \n",
    "        config.EYSIZE, \n",
    "        config.XSIZE, \n",
    "        config.YSIZE\n",
    "    ),\n",
    "    'distortedPolar': lambda: DistortedPolarGrid(\n",
    "        config.EXSIZE,\n",
    "        config.EYSIZE, \n",
    "        config.XSIZE, \n",
    "        config.YSIZE\n",
    "    ),\n",
    "    'rescalingDistortedPolar': lambda: RescalingDistortedPolarGrid(\n",
    "        config.EXSIZE, \n",
    "        config.EYSIZE, \n",
    "        config.XSIZE, \n",
    "        config.YSIZE\n",
    "    ),\n",
    "}\n",
    "\n",
    "try:\n",
    "    grid = grids[config.GRID_TYPE]()\n",
    "except KeyError:\n",
    "    with open(config.GRID_TYPE, 'rb') as infile:\n",
    "        grid = pickle.load(infile)\n",
    "\n",
    "config.GRID = grid\n",
    "\n",
    "# We initiate the stimulus processor type.\n",
    "\n",
    "processors = {\n",
    "    'direct': Stimulus,\n",
    "    'net': lambda image, grid: StimulusNet(image, grid, config.ENCODER),\n",
    "}\n",
    "\n",
    "config.PROCESSOR = processors[config.PROCESSOR_TYPE]\n",
    "\n",
    "# Templates for data paths.\n",
    "config.DATETIME_FORMAT       = '%Y-%m-%d_%H-%M-%S'\n",
    "config.DIGIT_SOUND_TEMPLATE  = './data/digit-voice/{}-alt.wav'\n",
    "\n",
    "base_dir = './data/psychophysics-sessions/'\n",
    "\n",
    "if config.TESTING:\n",
    "    config.CONFIG_FILE_TEMPLATE  = base_dir + 'tests/{}_{}_config.json'\n",
    "    config.SESSION_FILE_TEMPLATE = base_dir + 'tests/{}_{}_session.csv'\n",
    "    config.MOUSE_FILE_TEMPLATE   = base_dir + 'tests/{}_{}_mouse.csv'\n",
    "else:\n",
    "    config.CONFIG_FILE_TEMPLATE  = base_dir + 'participants/{}_{}_config.json'\n",
    "    config.SESSION_FILE_TEMPLATE = base_dir + 'participants/{}_{}_session.csv'\n",
    "    config.MOUSE_FILE_TEMPLATE   = base_dir + 'participants/{}_{}_mouse.csv'\n",
    "\n",
    "# Parameters for sound.\n",
    "config.CORRECT_NOTE   = 'G'\n",
    "config.INCORRECT_NOTE = 'Csh'\n",
    "config.NOTE_DURATION  = 0.1\n",
    "config.NOTE_VOLUME    = 0.5\n",
    "\n",
    "# Session data.\n",
    "config.SESSION_VARS = ['trial', 'cue', 'digit', 'keypress', 'cuetime', 'trialtime', 'sessiontime']\n",
    "config.MOUSE_VARS   = ['trial', 'cue', 'digit', 'xmouse', 'ymouse', 'cuetime', 'trialtime', 'sessiontime']\n",
    "\n",
    "# Output templates based on session data.\n",
    "config.SESSION_HEADER       = ','.join(config.SESSION_VARS) + '\\n'\n",
    "config.SESSION_ROW_TEMPLATE = ','.join(['{' + word + '}' for word in config.SESSION_VARS]) + '\\n'\n",
    "config.MOUSE_HEADER         = ','.join(config.MOUSE_VARS) + '\\n'\n",
    "config.MOUSE_ROW_TEMPLATE   = ','.join(['{' + word + '}' for word in config.MOUSE_VARS]) + '\\n'\n",
    "\n",
    "# Mouse recording interval in seconds.\n",
    "config.MOUSE_RECORD_INTERVAL = 0.2\n",
    "\n",
    "# Text.\n",
    "config.PROMPT_TEXT = \"{}% complete.\\n\\nPress any key when ready.\"\n",
    "config.END_TEXT    = \"Thank you. \\n\\nPress any key to exit.\"\n",
    "\n",
    "# If testing, the blank image.\n",
    "if config.TESTING:\n",
    "    config.BLANK_FILE = config.IMAGE_TEMPLATE.format('blank')\n",
    "    config.BLANK_IMAGE = cv2.resize(np.flipud(imread(config.BLANK_FILE)), dsize=(config.INPUT_XSIZE, config.INPUT_YSIZE))\n",
    "    config.TEST_WINDOW_XSIZE = 480\n",
    "    config.TEST_WINDOW_YSIZE = 480\n",
    "\n",
    "# Keypress during a trial.\n",
    "if config.NO_NUMPAD:\n",
    "    config.KEY_LIST=[str(x) for x in range(10)]\n",
    "else:\n",
    "    config.KEY_LIST = [\"num_\" + str(x) for x in range(10)]\n",
    "\n",
    "# When saving the config, excluding some variables due to size.\n",
    "config.EXCLUDED = ['STIMULI', 'GRID', 'IMAGES', 'BLANK_IMAGE', 'PROCESSOR']\n",
    "\n",
    "\n",
    "# Here, we make our main experiment, only if called from the command line.\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # We initiate the user details and present a dialog to the user to get those details.\n",
    "    config.details = {\"datetime\": datetime.strftime(datetime.now(), config.DATETIME_FORMAT), \"participant\": \"\"}\n",
    "    dialog         = gui.DlgFromDict(config.details, title=\"PROTOTYPE\", fixed=[\"datetime\"])\n",
    "    \n",
    "    # We interpret the dialog actions and initiate data files if proceeding.\n",
    "    if dialog.OK:\n",
    "        config.configFile  = config.CONFIG_FILE_TEMPLATE.format(config.details[\"participant\"], config.details[\"datetime\"])\n",
    "        config.sessionFile = config.SESSION_FILE_TEMPLATE.format(config.details[\"participant\"], config.details[\"datetime\"])\n",
    "        config.mouseFile = config.MOUSE_FILE_TEMPLATE.format(config.details[\"participant\"], config.details[\"datetime\"])\n",
    "    else:\n",
    "        core.quit()\n",
    "\n",
    "    # Clocks that keep track of the experiment.\n",
    "    clockSession = core.Clock()\n",
    "    clockTrial   = core.Clock()\n",
    "    clockCue     = core.Clock()\n",
    "    mouseRecord  = core.Clock()\n",
    "\n",
    "    # We initiate some generic sounds for correct and incorrect.\n",
    "    correctSound   = SoundPygame(value=config.CORRECT_NOTE, secs=config.NOTE_DURATION)\n",
    "    incorrectSound = SoundPygame(value=config.INCORRECT_NOTE, secs=config.NOTE_DURATION)\n",
    "    \n",
    "    correctSound.setVolume(config.NOTE_VOLUME)\n",
    "    incorrectSound.setVolume(config.NOTE_VOLUME)\n",
    "    \n",
    "    # And we initiate the sounds for each digit.\n",
    "    digitSounds = [SoundPygame(value=config.DIGIT_SOUND_TEMPLATE.format(digit)) for digit in range(10)]\n",
    "    \n",
    "    # Now we save the config for this session.\n",
    "    with open(config.configFile, 'w+') as configFile:\n",
    "        json.dump({k:v for k, v in config.items() if k not in config.EXCLUDED}, configFile)\n",
    "\n",
    "    # We initiate a testing window if this is a testing run.\n",
    "    if config.TESTING:\n",
    "        testWin = visual.Window([config.TEST_WINDOW_XSIZE, config.TEST_WINDOW_YSIZE],\n",
    "                                pos=(200,200), allowGUI=False, winType='pyglet')\n",
    "        win = visual.Window([config.TEST_WINDOW_XSIZE, config.TEST_WINDOW_YSIZE],\n",
    "                            pos=(200+config.TEST_WINDOW_XSIZE, 200), allowGUI=False, winType='pyglet', color=-1)\n",
    "    else:\n",
    "        # We make a window for the experiment.\n",
    "        win = visual.Window(fullscr=True, allowGUI=False, winType='pyglet', color=-1)\n",
    "\n",
    "    # Start the mouse event\n",
    "    mouse = event.Mouse(visible=False, win=win)\n",
    "        \n",
    "    # We now start the experiment loop.\n",
    "    with open(config.sessionFile, 'w+') as outfile, open(config.mouseFile, 'w+') as mousefile:\n",
    "\n",
    "        # We first write the header of the csv file.\n",
    "        outfile.write(config.SESSION_HEADER)\n",
    "        mousefile.write(config.MOUSE_HEADER)\n",
    "\n",
    "        # Start the trial loop.\n",
    "        for trial in range(config.NTRIALS):\n",
    "\n",
    "            # Set the trial clock to 0.\n",
    "            # This clock will start counting from the wait screen, so includes that time..\n",
    "            clockTrial.reset()\n",
    "            \n",
    "            # If testing, show the blank.\n",
    "            if config.TESTING:\n",
    "                blankStimulus = config.PROCESSOR(config.BLANK_IMAGE, config.GRID)\n",
    "                rendered = config.GRID.render(blankStimulus.vector)\n",
    "                imageStimulus = visual.ImageStim(testWin, image=rendered, size=(2,2))\n",
    "                imageStimulus.draw(); testWin.flip()\n",
    "\n",
    "            # Show a prompt on grey background at the beginning of the trial and wait for a keypress.\n",
    "            bg     = visual.GratingStim(win, tex=None, mask=None, size=2, units='norm', color=0)\n",
    "            prompt = visual.TextStim(win, text=config.PROMPT_TEXT.format(trial * 100 // config.NTRIALS))\n",
    "            bg.draw(); prompt.draw(); win.flip(); event.waitKeys(clearEvents=True)\n",
    "\n",
    "            # Create a stream of digits of length NCUES for the trial.\n",
    "            stream = choices(range(10), k=config.NCUES)\n",
    "\n",
    "            # Start the cue loop.\n",
    "            for cue in range(config.NCUES):\n",
    "                \n",
    "                # Get a digit from the stream and initialise the stimulus.\n",
    "                digit    = stream.pop()\n",
    "                image    = config.IMAGES[digit]\n",
    "                stimulus = config.PROCESSOR(image, config.GRID)\n",
    "                \n",
    "                # If this is a testing run, also draw the original image.\n",
    "                if config.TESTING:\n",
    "                    originalImage = visual.ImageStim(testWin, image=np.flipud(color.rgb2gray(image)), size=(2,2))\n",
    "                    originalImage.draw(); testWin.flip()\n",
    "                    \n",
    "                # Clear the event buffer\n",
    "                event.clearEvents()      \n",
    "                \n",
    "                # Set the mouse to the center. Might turn off, not sure which is better.\n",
    "                mouse.setPos((0,0))    \n",
    " \n",
    "                # Initialise a False keypress\n",
    "                keypressRaw = False\n",
    "        \n",
    "                # Set the cue clock to 0.\n",
    "                clockCue.reset()\n",
    "\n",
    "                # Set the mouse recording clock to 0\n",
    "                mouseRecord.reset()\n",
    "                \n",
    "                if not config.WITH_SCANNING:\n",
    "                    while not keypressRaw:\n",
    "                        # Set the stimulus in the right half of the grid\n",
    "                        stimulus.setPos(0.20, 0)\n",
    "                        rendered = np.flipud(config.GRID.render(stimulus.vector))\n",
    "                        imstim = visual.ImageStim(win, image=rendered, size = (2 * win.size[1] / win.size[0], 2))\n",
    "                        imstim.draw(); win.flip()\n",
    "\n",
    "                        keypresses = event.waitKeys(keyList=config.KEY_LIST, clearEvents=True)\n",
    "                        if keypresses:\n",
    "                            keypressRaw = keypresses[0]                    \n",
    "                else:\n",
    "                    # Loop until the keypress\n",
    "                    while not keypressRaw:\n",
    "\n",
    "                        # Get the mouse position and set the stimulus to the position.\n",
    "                        newPos = mouse.getPos()\n",
    "                        newPos = [newPos[0], -newPos[1]]\n",
    "                        stimulus.setPos(*newPos)\n",
    "\n",
    "                        if mouseRecord.getTime() > config.MOUSE_RECORD_INTERVAL:\n",
    "\n",
    "                            mouseRow = config.MOUSE_ROW_TEMPLATE.format(\n",
    "                                trial=trial,\n",
    "                                cue=cue,\n",
    "                                digit=digit,\n",
    "                                xmouse=newPos[0],\n",
    "                                ymouse=newPos[1],\n",
    "                                cuetime=clockCue.getTime(),\n",
    "                                trialtime=clockTrial.getTime(),\n",
    "                                sessiontime=clockSession.getTime(),\n",
    "                            )\n",
    "                            mousefile.write(mouseRow)\n",
    "\n",
    "                            mouseRecord.reset()\n",
    "\n",
    "                        # Render the stimulus\n",
    "                        rendered = np.flipud(config.GRID.render(stimulus.vector))\n",
    "\n",
    "                        # Create an image stimulus out of the rendered image.\n",
    "                        # Then show the stimulus.\n",
    "                        # Ensure stimulus is square on full screen window, assuming window has greater x dim than y dim.\n",
    "                        imstim = visual.ImageStim(win, image=rendered, size = (2 * win.size[1] / win.size[0], 2))\n",
    "                        imstim.draw(); win.flip()\n",
    "\n",
    "                        # Wait for a keypress. \n",
    "                        # We only need the first keypress, and want the key input from the numpage.\n",
    "                        keypresses = event.getKeys(keyList = config.KEY_LIST)\n",
    "                        if keypresses:\n",
    "                            keypressRaw = keypresses[0]\n",
    "                        #keypressRaw, *_ = event.waitKeys(clearEvents=True, keyList=config.KEY_LIST)\n",
    "                \n",
    "                # Check if their input was correct. \n",
    "                # Numpad keys are prepended with 'num_', so we strip it out.\n",
    "                keypress = keypressRaw.strip('num_')\n",
    "                correct  = (digit == int(keypress))\n",
    "                \n",
    "                # Create the data line.\n",
    "                row = config.SESSION_ROW_TEMPLATE.format(\n",
    "                    trial=trial,\n",
    "                    cue=cue,\n",
    "                    digit=digit,\n",
    "                    keypress=keypress, \n",
    "                    cuetime=clockCue.getTime(),\n",
    "                    trialtime=clockTrial.getTime(),\n",
    "                    sessiontime=clockSession.getTime(),\n",
    "                )\n",
    "                \n",
    "                # Write the data line to the session file.\n",
    "                outfile.write(row)\n",
    "\n",
    "                # Play the feedback sound.\n",
    "                correctSound.play() if correct else incorrectSound.play()\n",
    "                \n",
    "                # Play the digit sound.\n",
    "                digitSounds[digit].play()\n",
    "                \n",
    "        # At the end of all the trials, show an end screen and wait for key press\n",
    "        # to exit.\n",
    "        bg  = visual.GratingStim(win, tex=None, mask=None, size=2, units='norm', color=0)\n",
    "        end = visual.TextStim(win, text=config.END_TEXT)\n",
    "        bg.draw(); end.draw(); win.flip(); event.waitKeys(clearEvents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# %run digits.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import imageio\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from skimage import color\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from IPython import display\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"xtick.direction\"] = \"in\"\n",
    "plt.rcParams[\"ytick.direction\"] = \"in\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gridType= PolarGrid\n",
    "gridParam1 = 12\n",
    "gridParam2 = 12\n",
    "gridVectorLength = gridParam1 * gridParam2\n",
    "gridSize1 = 64\n",
    "gridSize2 = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "grid = gridType(gridParam1, gridParam2, gridSize1, gridSize2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_dir = \"./data/training-intermediate-data/\"\n",
    "base = \"{dir}/{time}_{type}_{gridType}_{gridParam1}-{gridParam2}_{gridSize1}-{gridSize2}.{ext}\"\n",
    "\n",
    "now = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "\n",
    "common_format = {\n",
    "    'time': now,\n",
    "    'gridType': gridType.__name__,\n",
    "    'gridParam1': gridParam1,\n",
    "    'gridParam2': gridParam2,\n",
    "    'gridSize1': gridSize1,\n",
    "    'gridSize2': gridSize2,\n",
    "}\n",
    "\n",
    "mnist_filepath = save_dir + base.format(\n",
    "    dir=\"training-decoders\",\n",
    "    type=\"mnist_decoder\",\n",
    "    ext=\"h5\",\n",
    "    **common_format\n",
    ")\n",
    "\n",
    "loss_filepath = save_dir + base.format(\n",
    "    dir=\"training-losses\",\n",
    "    type='loss',\n",
    "    ext='log',\n",
    "    **common_format\n",
    ")\n",
    "\n",
    "gif_filepath = save_dir + base.format(\n",
    "    dir=\"training-gifs\",\n",
    "    type='evolution',\n",
    "    ext='gif',\n",
    "    **common_format\n",
    ")\n",
    "\n",
    "grid_filepath = save_dir + base.format(\n",
    "    dir='training-grids',\n",
    "    type='grid',\n",
    "    ext='pkl',\n",
    "    **common_format\n",
    ")\n",
    "\n",
    "encoder_filepath = save_dir + base.format(\n",
    "    dir='training-encoders',\n",
    "    type='encoder',\n",
    "    ext='h5',\n",
    "    **common_format\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Creating Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def digit_to_image(digit : int):\n",
    "    fig = plt.figure(figsize=(1,1))\n",
    "    canvas = FigureCanvas(fig)\n",
    "    ax = plt.gca()\n",
    "    fig.patch.set_facecolor('black')\n",
    "    plt.axis('off')\n",
    "    ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "    ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.yaxis.set_visible(False)\n",
    "    plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0, hspace = 0, wspace = 0)\n",
    "    plt.text(0.25 + random.random() / 2, 0.25 + random.random() / 2, str(int(digit)),\n",
    "             size=48,\n",
    "             color='white',\n",
    "             clip_box=ax.clipbox,\n",
    "             clip_on=True,\n",
    "             horizontalalignment = 'center',\n",
    "             verticalalignment = 'center',\n",
    "             linespacing = 0,\n",
    "             #bbox=dict(facecolor='red', alpha=0.5),\n",
    "             transform=ax.transAxes)\n",
    "    #plt.savefig(f'{digit}.png', pad_inches=0, facecolor='black')\n",
    "    canvas.draw()\n",
    "    width, height = fig.get_size_inches() * fig.get_dpi()\n",
    "    image = np.frombuffer(canvas.tostring_rgb(), dtype='uint8').reshape(int(height), int(width), 3)\n",
    "    image = (image - 127.5) / 127.5\n",
    "    plt.close(fig)\n",
    "    return cv2.resize(image, dsize=(gridSize1,gridSize2)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Making a Digit Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # %%time\n",
    "# digits = np.random.randint(0, 10, (BUFFER_SIZE,))\n",
    "# digit_images = np.array([digit_to_image(d) for d in digits])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "digits_path = \"./data/training-input-data/digits.npy\"\n",
    "digit_images_path = \"./data/training-input-data/digit_images.npy\"\n",
    "\n",
    "# np.save(digits_path, digits)\n",
    "# np.save(digit_images_path, digit_images)\n",
    "\n",
    "digits = np.load(digits_path)\n",
    "digit_images = np.load(digit_images_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Training a Digit Recogniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# train_labels = np.random.randint(0, 10, (10000))\n",
    "# train_images = np.array([digit_to_image(d) for d in train_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 43s 4ms/sample - loss: 0.6205 - accuracy: 0.7777\n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 44s 4ms/sample - loss: 0.0015 - accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 43s 4ms/sample - loss: 0.0071 - accuracy: 0.9986\n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 42s 4ms/sample - loss: 3.3057e-04 - accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 42s 4ms/sample - loss: 5.8929e-05 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1bb0f691d68>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BUFFER_SIZE = 10000\n",
    "# BATCH_SIZE = 128\n",
    "\n",
    "# model = Sequential([\n",
    "#     layers.Conv2D(32, (16,16), padding='same', strides=(2,2), activation=tf.nn.relu, input_shape=(gridSize1, gridSize2, 3)),\n",
    "#     layers.Conv2D(16, (4,4), padding='same', strides=(1,1), activation=tf.nn.relu),\n",
    "#     layers.MaxPooling2D(2),\n",
    "#     layers.Dropout(0.1),\n",
    "#     layers.Flatten(),\n",
    "#     layers.Dense(10, activation=tf.nn.softmax)\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer='adam', \n",
    "#               loss='sparse_categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model.fit(train_images, train_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# test_labels = np.random.randint(0, 10, (500))\n",
    "# test_images = np.array([digit_to_image(d) for d in test_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 889us/sample - loss: 1.5473e-05 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5472710139874836e-05, 1.0]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "digit_aliased_model_path = \"./data/digit-models/digit-aliased.h5\"\n",
    "# model.save(digit_aliased_model_path)\n",
    "digit_aliased_model = tf.keras.models.load_model(digit_aliased_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Training on MNIST Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "def process_images(images):\n",
    "    processed = np.stack([cv2.resize(image, dsize=(64, 64)).astype('float32') for image in images], 0)\n",
    "    processed = (processed - 127.5) / 127.5\n",
    "    return processed\n",
    "\n",
    "train_images, test_images = process_images(train_images), process_images(test_images)\n",
    "\n",
    "def make_garbage(images):\n",
    "    global grid\n",
    "    num_images, ydim, xdim = images.shape\n",
    "\n",
    "    num_garbage = num_images # // 10\n",
    "    garbage_class = 10\n",
    "\n",
    "    garbage = np.array([grid.render(np.random.random(gridVectorLength)) \n",
    "                        for j in range(num_garbage)])\n",
    "    garbage_labels = np.full((num_garbage,), garbage_class)\n",
    "    \n",
    "    return garbage, garbage_labels\n",
    "\n",
    "train_garbage, train_garbage_labels = make_garbage(train_images)\n",
    "train_images = np.concatenate([train_images, train_garbage], axis=0)\n",
    "train_labels = np.concatenate([train_labels, train_garbage_labels], axis=0)\n",
    "\n",
    "test_garbage, test_garbage_labels = make_garbage(test_images)\n",
    "test_images = np.concatenate([test_images, test_garbage], axis=0)\n",
    "test_labels = np.concatenate([test_labels, test_garbage_labels], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "120000/120000 [==============================] - 115s 962us/sample - loss: 0.1117 - accuracy: 0.9665\n",
      "Epoch 2/5\n",
      "120000/120000 [==============================] - 114s 947us/sample - loss: 0.0777 - accuracy: 0.9777\n",
      "Epoch 3/5\n",
      "120000/120000 [==============================] - 115s 962us/sample - loss: 0.0752 - accuracy: 0.9797\n",
      "Epoch 4/5\n",
      "120000/120000 [==============================] - 114s 951us/sample - loss: 0.0779 - accuracy: 0.9800\n",
      "Epoch 5/5\n",
      "120000/120000 [==============================] - 114s 950us/sample - loss: 0.0792 - accuracy: 0.9799 loss: 0.0795 - accu - ETA: 0s - l\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24738e5ef28>"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "model = Sequential([\n",
    "    layers.Reshape((gridSize1, gridSize2, 1), input_shape=(gridSize1, gridSize2)),\n",
    "    \n",
    "    layers.Conv2D(64, (8, 8), padding='same', strides=(4,4)),\n",
    "    layers.LeakyReLU(),\n",
    "    layers.Dropout(0.25),\n",
    "    \n",
    "    layers.Conv2D(64, (4, 4), padding='same', strides=(2,2)),\n",
    "    layers.LeakyReLU(),\n",
    "    layers.Dropout(0.25),    \n",
    "    \n",
    "    layers.Conv2D(64, (2, 2), padding='same', strides=(1,1)),\n",
    "    layers.LeakyReLU(),\n",
    "    layers.Dropout(0.25),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(11, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 5s 231us/sample - loss: 0.0446 - accuracy: 0.9882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.04455112054591664, 0.98815]"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# model.save(mnist_filepath)\n",
    "\n",
    "# Reusing a trained MNIST model\n",
    "# mnist_filepath = \"./data/training-intermediate-data/training-decoders/2019-06-19_15-48_mnist_decoder_PolarGrid_12-12_64-64.h5\"\n",
    "\n",
    "mnist_model = tf.keras.models.load_model(mnist_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 8, 1, 64)"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_model.layers[1].get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_layer_kernels(layer):\n",
    "    \n",
    "    weights, biases = layer.get_weights()\n",
    "    \n",
    "    dim1, dim2, depth, n = weights.shape\n",
    "    #print(dim1, dim2, n)\n",
    "    \n",
    "    #n = depth * nkernels\n",
    "    \n",
    "    subplotx = int(np.sqrt(n))\n",
    "    subploty = n // subplotx # will not show all the kernels, but will show most.\n",
    "    n_plot = subplotx * subploty\n",
    "    \n",
    "    fig, ax = plt.subplots(subplotx, subploty, figsize=(6,6), sharex=True, sharey=True)\n",
    "    \n",
    "    for i in range(n_plot):\n",
    "        x = i % subplotx\n",
    "        y = i // subplotx\n",
    "        ax[x, y].imshow(weights[:, :, 0, i], cmap='gray') # only plot a few kernels\n",
    "        ax[x, y].axis('off')\n",
    "        #ax[x, y].set_title('bias={:.2f}'.format(biases[i]))\n",
    "    \n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_layer_kernels(mnist_model.layers[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_layer_kernels(mnist_model.layers[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Putting in a Generative Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mnist_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_6 (Reshape)          (None, 64, 64, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 16, 16, 64)        4160      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 8, 8, 64)          65600     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 8, 8, 64)          16448     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 11)                45067     \n",
      "=================================================================\n",
      "Total params: 262,550\n",
      "Trainable params: 131,275\n",
      "Non-trainable params: 131,275\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mnist_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "digit_aliased_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        24608     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 16)        8208      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 147,572\n",
      "Trainable params: 73,786\n",
      "Non-trainable params: 73,786\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "digit_aliased_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "encoder = Sequential([\n",
    "    digit_aliased_model,\n",
    "    #layers.Dense(12, use_bias=False),\n",
    "    #layers.LeakyReLU(),\n",
    "    #layers.Dropout(0.1),\n",
    "    #layers.Dense(gridParam1),\n",
    "    #layers.Dense(gridParam1*gridParam2, activation=tf.nn.relu),\n",
    "    layers.Dense(gridParam1*gridParam2, activation=tf.nn.sigmoid)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_3 (Sequential)    (None, 10)                73786     \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 144)               1584      \n",
      "=================================================================\n",
      "Total params: 75,370\n",
      "Trainable params: 1,584\n",
      "Non-trainable params: 73,786\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Training the Generative Network (Polar Regular Grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 250\n",
    "BATCHES = BUFFER_SIZE // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "def encoder_loss(input_digit, output_digit, encoded_images):\n",
    "    loss = cross_entropy(input_digit, output_digit) #+ (1 / (BATCH_SIZE * gridSize1 * gridSize2)) * tf.reduce_sum(tf.math.cos(tf.constant(np.pi / 2) * encoded_images)) #+ (1 / (BATCH_SIZE * gridSize1 * gridSize2)) * tf.reduce_sum(encoded_images)\n",
    "    return loss\n",
    "\n",
    "encoder_optimizer = tf.keras.optimizers.Adam(learning_rate=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "display_digits = np.array([i for i in range(10)])\n",
    "display_digit_images = np.array(list(map(digit_to_image, display_digits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "image_directory = \"./data/training-intermediate-data/training-images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    predictions = model(test_input, training=False)\n",
    "\n",
    "    fig = plt.figure(figsize=(5,3))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(2, 5, i+1)\n",
    "        plt.imshow(grid.render(np.array(predictions[i, :])), cmap='gray', vmin=-1, vmax=1)\n",
    "        plt.title(i)\n",
    "        plt.axis('off')\n",
    "        \n",
    "    plt.suptitle(f\"Epoch {epoch}\")\n",
    "\n",
    "    plt.savefig(image_directory + 'generated-epoch-{0:02d}.png'.format(epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(digits, digit_images, logfile):\n",
    "    with tf.GradientTape() as enc_tape:\n",
    "        encodings = encoder(digit_images, training=True)\n",
    "        encoded_images = tf.map_fn(grid.render_tensor, encodings)\n",
    "        #print(encoded_images.shape)\n",
    "\n",
    "        output_digits = mnist_model(encoded_images)\n",
    "        # expected_digits = digit_aliased_model(digit_images)\n",
    "        expected_digits = tf.one_hot(digits, 11) # 11 with garbage\n",
    "\n",
    "        enc_loss = encoder_loss(expected_digits, output_digits, encoded_images)\n",
    "        #print(enc_loss)\n",
    "\n",
    "    tf.print(enc_loss, output_stream=logfile)\n",
    "    \n",
    "    gradients_of_encoder = enc_tape.gradient(enc_loss, encoder.trainable_variables)\n",
    "    #print(gradients_of_encoder)\n",
    "\n",
    "    encoder_optimizer.apply_gradients(zip(gradients_of_encoder, encoder.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def train(epochs, logpath):\n",
    "    \n",
    "    logfile = \"file://\" + logpath \n",
    "    \n",
    "    # Fresh log\n",
    "    open(logpath, 'w').close()\n",
    "        \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        for i in range(BATCHES):\n",
    "            imin = i * BATCH_SIZE\n",
    "            imax = (i+1) * BATCH_SIZE\n",
    "            \n",
    "            digits_slice = digits[imin:imax]\n",
    "            digit_images_slice = digit_images[imin:imax]\n",
    "            \n",
    "            if len(digits_slice) > 0:\n",
    "                train_step(digits_slice, digit_images_slice, logfile)\n",
    "            \n",
    "            #print(f\"BATCH {i} DONE\")\n",
    "\n",
    "        # Produce images for the GIF as we go\n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_images(encoder,\n",
    "                                 epoch + 1,\n",
    "                                 display_digit_images\n",
    "                                 )\n",
    "        \n",
    "    #     if (epoch + 1) % 2 == 0:\n",
    "    #         checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "\n",
    "        #accuracy = find_accuracy()\n",
    "        print(f'Time for epoch {epoch+1} is {time.time()-start} sec.')\n",
    "\n",
    "    # Generate after the final epoch\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(encoder,\n",
    "                           epochs,\n",
    "                           display_digit_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generate_and_save_images(encoder,\n",
    "                         0,\n",
    "                         display_digit_images\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "EPOCHS = 50\n",
    "train(EPOCHS, loss_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_loss(logpath):\n",
    "    with open(logpath) as infile:\n",
    "        loss_raw = infile.readlines()\n",
    "\n",
    "    loss_data = np.array(list(map(float, loss_raw)))\n",
    "\n",
    " \n",
    "    plt.plot(range(loss_data.shape[0]), loss_data)\n",
    "    plt.ylabel(\"Categorical Cross Entropy between\\nPrediction and One-Hot Class\")\n",
    "    plt.xlabel(\"Training Batch (each containing 500 images)\")\n",
    "    plt.title(\"Decoder Loss per Epoch\")\n",
    "\n",
    "    plot_file = logpath.replace(\"losses\", \"graphs\").replace(\".log\", \".png\")\n",
    "    plt.savefig(plot_file)\n",
    "    print(\"./03-psychophysics/\" + plot_file)\n",
    "    return plot_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_loss(loss_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "generated_images = glob.glob(image_directory + (\"*.png\"))\n",
    "images = [PIL.Image.open(image) for image in generated_images]\n",
    "\n",
    "# save the first image 10 times\n",
    "images[0].save(gif_filepath,\n",
    "               save_all=True,\n",
    "               append_images=[images[0]] * 10 + images + [images[-1]]*10,\n",
    "               duration=100,\n",
    "               loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(grid_filepath, 'wb') as outfile:\n",
    "    pickle.dump(grid, outfile)\n",
    "encoder.save(encoder_filepath)\n",
    "# encoder = tf.keras.models.load_model(encoder_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Static Digit Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script generates static images of centered digits as PNG files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "baseColour = (0, 0, 0)\n",
    "baseSizeX, baseSizeY = (16, 16)\n",
    "textColour = (255, 255, 255)\n",
    "\n",
    "saveExtension = \"png\"\n",
    "\n",
    "for digit in range(10):\n",
    "    baseImage = Image.new(\"RGB\", (baseSizeY, baseSizeX), baseColour)\n",
    "    base = ImageDraw.Draw(baseImage)\n",
    "\n",
    "    text = str(digit)\n",
    "    textSizeX, textSizeY = base.textsize(text)\n",
    "    textPosition = (baseSizeX / 2 - textSizeX / 2, baseSizeY / 2 - textSizeY / 2)\n",
    "    base.text(textPosition, text, textColour)\n",
    "\n",
    "    saveName = str(digit)\n",
    "    baseImage.save(f\"./data/digit-images/{saveName}.{saveExtension}\")\n",
    "\n",
    "# Blank white\n",
    "baseImage = Image.new(\"RGB\", (baseSizeY, baseSizeX), (255,255,255))\n",
    "base = ImageDraw.Draw(baseImage)\n",
    "baseImage.save(f'./data/digit-images/blank.{saveExtension}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "155px",
    "width": "209.983px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
