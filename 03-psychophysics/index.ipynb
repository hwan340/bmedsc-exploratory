{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This notebook contains the ground-truth copy of the code mixed with literate comments. All code in this directory is produced by running the cells in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cortical visual prostheses** (CVPs) are devices implanted on the brain which aim to restore vision to blind people by stimulating the visual cortex.\n",
    "\n",
    "Experiments have shown that stimulating the visual cortex produces visual percepts in both blind and sighted patients. These visual percepts are called **phosphenes** and are typically described as small round dots of colourless light like \"a star in the sky.\" The idea behind most CVPs is to build images out of phosphenes, like how graphical displays are made out of pixels. \n",
    "\n",
    "But phosphenes are very limited. At the moment, we can only control the intensity of phosphenes (and, to some extent, their size). The locations of phosphenes follow the retinotopic mapping of the visual fields on the cortex, but the complexity of this mapping makes precisely controlling the positions of phosphenes in the visual field very difficult; it is much easier to instead place a regular grid of electrodes on the brain and map the phosphene locations after implantation. Other properties of phosphenes, such as their shape, individual brightness or colour, are highly variable and uncontrollable.\n",
    "\n",
    "In addition, there are no empirical studies which describe what exactly is seen when we try to stimulate more than about five simultaneous phosphenes. The largest implant in a human used 64 electrodes, but there is inadequate description on what the implantee saw and how useful these percepts were (though the implantee purportedly was able to read large letterforms). New CVPs generally intend to stimulate on the order of hundreds of electrodes (such as 473 for the Monash Vision Group); it is not yet known what people will perceive when many electrodes are stimulate at once. Some studies have suggested that stimulating five electrodes, for example, does not produce five discrete phosphenes or a blended picture; rather, phosphenes which were stimulated by an electrode when only one electrode was stimulated completely disappear upon simultaneous stimulation as though they were not stimulated at all.\n",
    "\n",
    "This begs the question: **how can phosphenes be used to convey useful information, given their known and unknown perceptual limitations?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern-Based Approach\n",
    "\n",
    "Using a pattern-based approach, we focus on **object identification**. That is, we attempt to create recognisable patterns that are matchable against object identities, and discard any location-based information.\n",
    "\n",
    "The advantages of this approach are that:\n",
    "\n",
    "1. By focusing on identity rather than location, stable patterns can be presented using the same phosphenes each time an object occurs in a scene. As the exact same pattern is presented each time, it may be easier to learn.\n",
    "\n",
    "The disadvantages of this approach are that:\n",
    "\n",
    "1. Only one object at a time can be shown.\n",
    "2. If there is no location information, it may be hard to justify how this is useful in comparison to something less invasive; for example, an external camera linked to an earpiece could easily just speak out the identity of an object instead of presenting patterned vision. You could argue that presenting object identities through vision is beneficial because it doesn't take up another sensory modality, but this is a relatively small upside compared to the invasiveness of a brain implant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location-Based Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a location-based approach, we focus on **object localisation**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rationale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phosphene Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Phosphene Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../phosphenes_test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile phosphenes.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import random\n",
    "import math\n",
    "from skimage import color\n",
    "\n",
    "# CONSTANTS\n",
    "\n",
    "XSIZE = 64\n",
    "YSIZE = 64\n",
    "PBASE = 2\n",
    "SCALE = 6\n",
    "EXSIZE = XSIZE // SCALE\n",
    "EYSIZE = YSIZE // SCALE\n",
    "\n",
    "\n",
    "def safebound(value: float, width: float, lower: float, upper: float):\n",
    "    \"\"\" \n",
    "    Returns the bounded min and max about value with width.\n",
    "    \"\"\"\n",
    "    vmin = int(max(lower, value - width))\n",
    "    vmax = int(min(upper, value + width))\n",
    "    return vmin, vmax\n",
    "\n",
    "def bound(value:float, lower: float, upper:float):\n",
    "    \"\"\"\n",
    "    Returns a bounded value.\n",
    "    \"\"\"\n",
    "    if value > lower:\n",
    "        if value < upper:\n",
    "            return value\n",
    "        else:\n",
    "            return upper\n",
    "    else:\n",
    "        return lower\n",
    "\n",
    "# Electrodes, which produce phosphenes.\n",
    "\n",
    "class Electrode:\n",
    "    def __init__(self, x: float, y: float, xsize : int = XSIZE, ysize : int = YSIZE, randomPos: float = 0):\n",
    "        \"\"\"\n",
    "        Produces a phosphene for a single electrode.\n",
    "        \n",
    "        Args:\n",
    "            x: float         - position in range [0, 1]. \n",
    "            y: float         - position in range [0, 1]\n",
    "            randomPos: float - a scaling factor for random positioning. \n",
    "        \"\"\"\n",
    "        self.randomPos = randomPos\n",
    "        self.x = bound(x + (random.random() - 0.5) * self.randomPos, 0, 1)\n",
    "        self.y = bound(y + (random.random() - 0.5) * self.randomPos, 0, 1)\n",
    "        self.xsize = xsize\n",
    "        self.ysize = ysize\n",
    "\n",
    "        self.size = PBASE * (0.5 + (4 * np.sqrt((self.x - 0.5) ** 2 + (self.y - 0.5) ** 2)) ** 2)\n",
    "\n",
    "        self.rendered = self.render()\n",
    "\n",
    "    def render(self):\n",
    "        xmin, xmax = safebound(self.xsize * self.x, self.size, 0, self.xsize)\n",
    "        ymin, ymax = safebound(self.ysize * self.y, self.size, 0, self.ysize)\n",
    "\n",
    "        base = np.zeros((self.ysize, self.xsize))\n",
    "        base[ymin:ymax, xmin:xmax] = 1\n",
    "\n",
    "        return gaussian_filter(base, self.size)\n",
    "\n",
    "class UniqueElectrode:\n",
    "    \"\"\"\n",
    "    This class implements electrodes with unique characteristics such as colour and shape.\n",
    "    \"\"\"\n",
    "    def __init__(self, x: float, y: float, xsize : int = XSIZE, ysize : int = YSIZE, randomPos: float = 0.001):\n",
    "        self.x = bound(x + (random.random() - 0.5) * randomPos, 0, 1)\n",
    "        self.y = bound(y + (random.random() - 0.5) * randomPos, 0, 1)\n",
    "        self.size = PBASE * (0.5 + (4 * np.sqrt((self.x - 0.5) ** 2 + (self.y - 0.5) ** 2)) ** 2)\n",
    "        self.colour = np.random.random(3)\n",
    "        # xmod and ymod modify the shape of the phosphene\n",
    "        self.xmod = 1 + (random.random()-0.5) * 3\n",
    "        self.ymod = 1 + (random.random()-0.5) * 3\n",
    "        self.xsize = xsize\n",
    "        self.ysize = ysize\n",
    "\n",
    "        self.rendered = self.render()\n",
    "\n",
    "    def render(self):\n",
    "        xmin, xmax = safebound(self.xsize * self.x, self.size*self.xmod, 0, self.xsize)\n",
    "        ymin, ymax = safebound(self.ysize * self.y, self.size*self.ymod, 0, self.ysize)\n",
    "\n",
    "        base = np.zeros((self.ysize, self.xsize, 3))\n",
    "        base[ymin:ymax, xmin:xmax, :] = self.colour\n",
    "\n",
    "        return gaussian_filter(base, self.size * (random.random() ** 0.3))\n",
    "\n",
    "# Grids, which are composed of electrodes.\n",
    "\n",
    "class RegularGrid:\n",
    "    def __init__(self, exsize: int = EXSIZE, eysize: int = EYSIZE, xsize=XSIZE, ysize=YSIZE):\n",
    "        \"\"\"\n",
    "        \n",
    "        Args:\n",
    "            exsize: int - x size of electrode grid \n",
    "            eysize: int - y size of electrode grid\n",
    "        \"\"\"\n",
    "        self.exsize = EXSIZE\n",
    "        self.eysize = EYSIZE\n",
    "        self.grid = [\n",
    "            Electrode(x / exsize, y / eysize, xsize=xsize, ysize=ysize)\n",
    "            for x in range(exsize)\n",
    "            for y in range(eysize)\n",
    "        ]\n",
    "\n",
    "    def render(self, values):\n",
    "        product = [v * e.rendered for (v, e) in zip(values, self.grid)]\n",
    "        summed = sum(product)\n",
    "        summax = np.max(summed)\n",
    "        return np.clip(summed, 0, 1) * 2 - 1\n",
    "        # return (summed / summax) * 2 - 1\n",
    "\n",
    "class IrregularGrid:\n",
    "    def __init__(self, randomPos=2, exsize=EXSIZE, eysize=EYSIZE, xsize=XSIZE, ysize=YSIZE):\n",
    "        self.exsize = EXSIZE\n",
    "        self.eysize = EYSIZE\n",
    "        self.grid = [\n",
    "            Electrode(0.5 + (x / exsize) / 2, y / eysize, xsize=xsize, ysize=ysize, randomPos=randomPos )\n",
    "            for x in range(exsize)\n",
    "            for y in range(eysize)\n",
    "        ]\n",
    "\n",
    "    def render(self, values):\n",
    "        product = [v * e.rendered for (v, e) in zip(values, self.grid)]\n",
    "        summed = sum(product)\n",
    "        summax = np.max(summed)\n",
    "        return np.clip(summed, 0, 1) * 2 - 1\n",
    "        # return (summed / summax) * 2 - 1\n",
    "\n",
    "class PolarRegularGrid:\n",
    "    def __init__(self, nrho, ntheta, xsize=XSIZE, ysize=YSIZE):\n",
    "        self.nrho   = nrho\n",
    "        self.ntheta = ntheta\n",
    "        self.grid = [\n",
    "            # Need to think of better way to scale.\n",
    "            Electrode(((math.exp(rho**0.6) / math.exp(nrho**0.6) * math.cos((math.pi * theta / ntheta) - math.pi/2)) + 1) / 2,\n",
    "                      ((math.exp(rho**0.6) / math.exp(nrho**0.6) * math.sin((math.pi * theta / ntheta) - math.pi/2)) + 1) / 2,\n",
    "                      xsize = xsize,\n",
    "                      ysize = ysize,\n",
    "                     )\n",
    "            # Ensure the central electrodes are actually visible by adding 1 to zero.\n",
    "            for rho in range(1, nrho+1)\n",
    "            for theta in range(ntheta)\n",
    "        ]\n",
    "\n",
    "    def render(self, values):\n",
    "        product = [v * e.rendered for (v, e) in zip(values, self.grid)]\n",
    "        summed = sum(product)\n",
    "        summax = np.max(summed)\n",
    "        return np.clip(summed, 0, 1) * 2 - 1\n",
    "        # return (summed / summax) * 2 - 1\n",
    "        \n",
    "    def render_tensor(self, tensor):\n",
    "        reshaped = tf.transpose(tf.reshape(tf.tile(tensor, tf.constant([64])), (64, 144, 1)), perm=[1, 0, 2])\n",
    "        product = reshaped * self.renders\n",
    "        summed = tf.reduce_sum(product, axis=0)\n",
    "        return tf.clip_by_value(summed, 0, 1) * 2 - 1\n",
    "\n",
    "class PolarRegularUniqueGrid:\n",
    "    def __init__(self, nrho, ntheta, xsize=XSIZE, ysize=YSIZE):\n",
    "        self.nrho   = nrho\n",
    "        self.ntheta = ntheta\n",
    "        self.grid = [\n",
    "            # Need to think of better way to scale.\n",
    "            UniqueElectrode(((math.exp(rho**0.6) / math.exp(nrho**0.6) * math.cos((math.pi * theta / ntheta) - math.pi/2)) + 1) / 2,\n",
    "                            ((math.exp(rho**0.6) / math.exp(nrho**0.6) * math.sin((math.pi * theta / ntheta) - math.pi/2)) + 1) / 2,\n",
    "                            xsize = xsize,\n",
    "                            ysize = ysize,\n",
    "                           )\n",
    "            # Ensure the central electrodes are actually visible by adding 1 to zero.\n",
    "            for rho in range(1, nrho+1)\n",
    "            for theta in range(ntheta)\n",
    "        ]\n",
    "\n",
    "    def render(self, values):\n",
    "        product = [v * e.rendered for (v, e) in zip(values, self.grid)]\n",
    "        summed = sum(product)\n",
    "        summax = np.max(summed)\n",
    "        return np.clip(summed, 0, 1)\n",
    "        # return (summed / summax) * 2 - 1\n",
    "        \n",
    "    def render_tensor(self, tensor):\n",
    "        reshaped = tf.transpose(tf.reshape(tf.tile(tensor, tf.constant([64])), (64, 144, 1)), perm=[1, 0, 2])\n",
    "        product = reshaped * self.renders\n",
    "        summed = tf.reduce_sum(product, axis=0)\n",
    "        return tf.clip_by_value(summed, 0, 1) * 2 - 1\n",
    "\n",
    "class NonLinearInteractionGrid:\n",
    "    def __init__(self, nrho, ntheta, xsize=XSIZE, ysize=YSIZE):\n",
    "        self.nrho   = nrho\n",
    "        self.ntheta = ntheta\n",
    "        self.grid = [\n",
    "            # Need to think of better way to scale.\n",
    "            UniqueElectrode(((math.exp(rho**0.6) / math.exp(nrho**0.6) * math.cos((math.pi * theta / ntheta) - math.pi/2)) + 1) / 2,\n",
    "                            ((math.exp(rho**0.6) / math.exp(nrho**0.6) * math.sin((math.pi * theta / ntheta) - math.pi/2)) + 1) / 2,\n",
    "                            xsize = xsize,\n",
    "                            ysize = ysize,\n",
    "                           )\n",
    "            # Ensure the central electrodes are actually visible by adding 1 to zero.\n",
    "            for rho in range(1, nrho+1)\n",
    "            for theta in range(ntheta)\n",
    "        ]\n",
    "\n",
    "    def render(self, values):\n",
    "        product = [v * e.rendered for (v, e) in zip(values, self.grid)]\n",
    "        summed = sum(product)\n",
    "        summax = np.max(summed)\n",
    "        #return np.clip(summed, 0, 1)\n",
    "        return (summed / summax) * 2 - 1\n",
    "\n",
    "        \n",
    "        \n",
    "# STIMULUS\n",
    "\n",
    "class Stimulus:\n",
    "    def __init__(self, image, grid, xpos=0, ypos=0):\n",
    "        self.original = image\n",
    "        self.shape = self.original.shape\n",
    "        \n",
    "        self.padder = np.zeros((3 * self.shape[0], 3 * self.shape[1]))\n",
    "        self.padder[self.shape[0]:2*self.shape[0], self.shape[1]:2*self.shape[1]] = self.original\n",
    "        \n",
    "        self.xpos = xpos\n",
    "        self.ypos = ypos\n",
    "        \n",
    "        self.image = self.getImage()\n",
    "        \n",
    "        self.grid = grid\n",
    "        self.sampleWidth = 6\n",
    "        \n",
    "        self.vector = self.process()\n",
    "            \n",
    "    def get_params(self, x : float, y : float):\n",
    "        \n",
    "        ymin = bound(int(self.shape[0] * y - self.sampleWidth // 2), 0, self.shape[0] - 1)\n",
    "        ymax = bound(int(self.shape[0] * y + self.sampleWidth // 2), 0, self.shape[0] - 1)\n",
    "        xmin = bound(int(self.shape[1] * x - self.sampleWidth // 2), 0, self.shape[1] - 1)            \n",
    "        xmax = bound(int(self.shape[1] * x + self.sampleWidth // 2), 0, self.shape[1] - 1)\n",
    "\n",
    "        vals  = self.image[ymin:ymax, xmin:xmax]\n",
    "        return np.mean(vals)\n",
    "    \n",
    "    def getImage(self):\n",
    "        \"\"\" Based on xpos and ypos, get the image view from the padder.\n",
    "        \"\"\"\n",
    "        \n",
    "        xstart = self.shape[0] - int(self.xpos * self.shape[0])\n",
    "        ystart = self.shape[1] - int(self.ypos * self.shape[1])\n",
    "        \n",
    "        return self.padder[ystart:ystart+self.shape[1], xstart:xstart+self.shape[0]]\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\" Converts the stimulus into a brightness vector for the\n",
    "        \"\"\"\n",
    "\n",
    "        params = [self.get_params(e.x, e.y) for e in self.grid.grid]\n",
    "        return params\n",
    "        #flattened = self.image.flatten(order=\"C\")\n",
    "    \n",
    "    def setPos(self, xpos: float, ypos: float):\n",
    "        \"\"\"Translate the image. xpos and ypos lie in the range (-1, 1)\n",
    "        \"\"\"\n",
    "        self.xpos = xpos\n",
    "        self.ypos = ypos\n",
    "        self.image = self.getImage()\n",
    "        self.vector = self.process()\n",
    "        \n",
    "        \n",
    "# TESTING ONLY \n",
    "\n",
    "import tensorflow as tf\n",
    "tf.executing_eagerly()\n",
    "import keras\n",
    "\n",
    "input_shape = (72, 72)\n",
    "\n",
    "def make_encoder_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten(input_shape=input_shape))\n",
    "    model.add(tf.keras.layers.Dense(10*10*6))\n",
    "    model.add(tf.keras.layers.Dense(10*10))\n",
    "    #print(model.output_shape)\n",
    "    return model\n",
    "\n",
    "encoder = make_encoder_model()\n",
    "\n",
    "class StimulusNet(Stimulus):\n",
    "    \n",
    "    def process(self):\n",
    "        image_tensor = tf.convert_to_tensor(np.array([color.rgb2gray(self.image)]), dtype=tf.float32)\n",
    "        return encoder(image_tensor).numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%run phosphenes.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digit Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../digits.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile digits.py\n",
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "This script runs a digit recognition psychophysics session.\n",
    "\"\"\"\n",
    "\n",
    "# # Setup\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import phosphenes\n",
    "from phosphenes import Stimulus\n",
    "from datetime import datetime\n",
    "from argparse import ArgumentParser\n",
    "from psychopy import visual, core, gui, event\n",
    "from box import Box\n",
    "from psychopy.sound.backend_pygame import SoundPygame\n",
    "from psychopy.tools.filetools import fromFile, toFile\n",
    "from skimage import color\n",
    "from imageio import imread\n",
    "from random import random, choices\n",
    "from PIL import Image\n",
    "\n",
    "# I'm setting up a config dictionary with dot-syntax so it can be serialised \n",
    "# and saved with the session. I prefer explicitly keeping track of state.\n",
    "\n",
    "config = Box({})\n",
    "\n",
    "# Parsing the command line arguments, especially for testing.\n",
    "parser = ArgumentParser(description='Digit recognition task.')\n",
    "\n",
    "# Define command line arguments.\n",
    "argspec = {\n",
    "    'testing': {\n",
    "        'action': 'store_const',\n",
    "        'const': True,\n",
    "        'default': False,\n",
    "        'dest': 'testing',\n",
    "        'help': 'Test the experiment and save the data.'\n",
    "    },\n",
    "    'ntrials': {\n",
    "        'type': int,\n",
    "        'nargs': '?',\n",
    "        'default': 5,\n",
    "        'help': 'Number of trials for the experiment.'\n",
    "    },\n",
    "    'ncues': {\n",
    "        'type': int,\n",
    "        'nargs': '?',\n",
    "        'default': 10,\n",
    "        'help': 'Number of cues per trial. Should be a multiple of 10 (for now) for digit stream.'\n",
    "    },\n",
    "    'grid': {\n",
    "        'type': str,\n",
    "        'nargs': '?',\n",
    "        'default': 'polarRegular',\n",
    "        'help': 'The grid type for rendering. One of regular, irregular, polarRegular, polarRegularUnique, or nonLinear.'\n",
    "    },\n",
    "    'processor': {\n",
    "        'type': str,\n",
    "        'nargs': '?',\n",
    "        'default': 'direct',\n",
    "        'help': 'The processor for the session. One of brightness of learner.'\n",
    "    },\n",
    "    'no-numpad': {\n",
    "        'action': 'store_const',\n",
    "        'const': True,\n",
    "        'default': False,\n",
    "        'dest': 'noNumpad',\n",
    "        'help': 'Flags that normal number keys instead of numpad should be used.'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add arguments to the parser.\n",
    "[parser.add_argument(f'--{k}', **v) for k, v in argspec.items()]\n",
    "\n",
    "# Parse the arguments and save into config.\n",
    "args = parser.parse_args()\n",
    "config.TESTING        = args.testing\n",
    "config.NTRIALS        = args.ntrials\n",
    "config.NCUES          = args.ncues\n",
    "config.GRID_TYPE      = args.grid\n",
    "config.PROCESSOR_TYPE = args.processor\n",
    "config.NO_NUMPAD      = args.noNumpad\n",
    "\n",
    "\n",
    "# First, we define the constants for the window size of the experiment.\n",
    "# `XSIZE` and `YSIZE` refer to the size of the window on the screen.\n",
    "# `EXSIZE` and `EYSIZE` refer to the size of the image data (i.e. how many \n",
    "# electrodes there are). \n",
    "# `SCALE` links the two. \n",
    "\n",
    "config.XSIZE  = 128\n",
    "config.YSIZE  = 128\n",
    "config.SCALE  = 12\n",
    "config.EXSIZE = config.XSIZE // config.SCALE\n",
    "config.EYSIZE = config.YSIZE // config.SCALE\n",
    "\n",
    "# Next, we load the stimulus. Opening the image files can be expensive\n",
    "# so we're doing at this at the very start and loading them into a \n",
    "# variable. \n",
    "\n",
    "# `IMAGE_TEMPLATE` is a string of the filepath of the stimulus digit images.\n",
    "config.IMAGE_TEMPLATE = './data/digit-images-aliased/{}.png'\n",
    "\n",
    "# `IMAGE_SIZE` is an (int, int) tuple of the image size of the first image.\n",
    "# We assume that each image is of the same size as the image labelled \"0\"\n",
    "config.IMAGE_SIZE = np.shape(imread(config.IMAGE_TEMPLATE.format(0)))  \n",
    "\n",
    "# `IMAGE_SCALE` is an int describing the ratio of electrode size to image size.\n",
    "# It assumes that EXSIZE == EYSIZE and the input images are square.\n",
    "# This may need changing later. \n",
    "config.IMAGE_SCALE = config.EXSIZE / config.IMAGE_SIZE[0]  \n",
    "\n",
    "# `IMAGES` holds the original digit images.\n",
    "config.IMAGES = [np.flipud(color.rgb2gray(imread(config.IMAGE_TEMPLATE.format(digit)))) for digit in range(10)]\n",
    "\n",
    "# `STIMULI` contains a list of numpy arrays.\n",
    "# Each element in the list holds the image data (in greyscale at the moment) \n",
    "# for the digit equal to its index.\n",
    "config.STIMULI = [\n",
    "    np.array(Image.fromarray(image).resize((config.EXSIZE, config.EYSIZE)))\n",
    "    for image in config.IMAGES\n",
    "]\n",
    "\n",
    "# We initiate a grid of electrodes.\n",
    "grids = {\n",
    "    'regular':   lambda: phosphenes.RegularGrid(exsize=config.EXSIZE, eysize=config.EYSIZE, xsize=config.XSIZE, ysize=config.YSIZE),\n",
    "    'irregular': lambda: phosphenes.IrregularGrid(exsize=config.EXSIZE, eysize=config.EYSIZE, randomPos=0.1, xsize=config.XSIZE, ysize=config.YSIZE),\n",
    "    'polarRegular': lambda: phosphenes.PolarRegularGrid(nrho=config.EXSIZE, ntheta=config.EYSIZE, xsize=config.XSIZE, ysize=config.YSIZE),\n",
    "    'polarRegularUnique': lambda: phosphenes.PolarRegularUniqueGrid(nrho=config.EXSIZE, ntheta=config.EYSIZE, xsize=config.XSIZE, ysize=config.YSIZE),\n",
    "    'nonLinear': lambda: phosphenes.NonLinearInteractionGrid(nrho=config.EXSIZE, ntheta=config.EYSIZE, xsize=config.XSIZE, ysize=config.YSIZE),\n",
    "}\n",
    "\n",
    "config.GRID = grids[config.GRID_TYPE]()\n",
    "\n",
    "# We initiate the stimulus processor type.\n",
    "\n",
    "processors = {\n",
    "    'direct': phosphenes.Stimulus,\n",
    "    'net': phosphenes.StimulusNet,\n",
    "}\n",
    "\n",
    "config.PROCESSOR = processors[config.PROCESSOR_TYPE]\n",
    "\n",
    "# Templates for data paths.\n",
    "config.DATETIME_FORMAT       = '%Y-%m-%d_%H-%M-%S'\n",
    "config.DIGIT_SOUND_TEMPLATE  = './data/digit-voice/{}-alt.wav'\n",
    "\n",
    "if config.TESTING:\n",
    "    config.CONFIG_FILE_TEMPLATE  = './data/sessions/tests/{}_{}_config.json'\n",
    "    config.SESSION_FILE_TEMPLATE = './data/sessions/tests/{}_{}_session.csv'\n",
    "    config.MOUSE_FILE_TEMPLATE   = './data/sessions/tests/{}_{}_mouse.csv'\n",
    "else:\n",
    "    config.CONFIG_FILE_TEMPLATE  = './data/sessions/participants/{}_{}_config.json'\n",
    "    config.SESSION_FILE_TEMPLATE = './data/sessions/participants/{}_{}_session.csv'\n",
    "    config.MOUSE_FILE_TEMPLATE   = './data/sessions/participants/{}_{}_mouse.csv'\n",
    "\n",
    "# Parameters for sound.\n",
    "config.CORRECT_NOTE   = 'G'\n",
    "config.INCORRECT_NOTE = 'Csh'\n",
    "config.NOTE_DURATION  = 0.1\n",
    "config.NOTE_VOLUME    = 0.5\n",
    "\n",
    "# Session data.\n",
    "config.SESSION_VARS = ['trial', 'cue', 'digit', 'keypress', 'cuetime', 'trialtime', 'sessiontime']\n",
    "config.MOUSE_VARS   = ['trial', 'cue', 'digit', 'xmouse', 'ymouse', 'cuetime', 'trialtime', 'sessiontime']\n",
    "\n",
    "# Output templates based on session data.\n",
    "config.SESSION_HEADER       = ','.join(config.SESSION_VARS) + '\\n'\n",
    "config.SESSION_ROW_TEMPLATE = ','.join(['{' + word + '}' for word in config.SESSION_VARS]) + '\\n'\n",
    "config.MOUSE_HEADER         = ','.join(config.MOUSE_VARS) + '\\n'\n",
    "config.MOUSE_ROW_TEMPLATE   = ','.join(['{' + word + '}' for word in config.MOUSE_VARS]) + '\\n'\n",
    "\n",
    "# Mouse recording interval in seconds.\n",
    "config.MOUSE_RECORD_INTERVAL = 0.2\n",
    "\n",
    "# Text.\n",
    "config.PROMPT_TEXT = \"{}% complete.\\n\\nPress any key when ready.\"\n",
    "config.END_TEXT    = \"Thank you. \\n\\nPress any key to exit.\"\n",
    "\n",
    "# If testing, the blank image.\n",
    "if config.TESTING:\n",
    "    config.BLANK_FILE = config.IMAGE_TEMPLATE.format('blank')\n",
    "    config.BLANK_IMAGE = np.flipud(color.rgb2gray(imread(config.BLANK_FILE)))\n",
    "    config.TEST_WINDOW_XSIZE = 480\n",
    "    config.TEST_WINDOW_YSIZE = 480\n",
    "\n",
    "# Keypress during a trial.\n",
    "if config.NO_NUMPAD:\n",
    "    config.KEY_LIST=[str(x) for x in range(10)]\n",
    "else:\n",
    "    config.KEY_LIST = [\"num_\" + str(x) for x in range(10)]\n",
    "\n",
    "# When saving the config, excluding some variables due to size.\n",
    "config.EXCLUDED = ['STIMULI', 'GRID', 'IMAGES', 'BLANK_IMAGE', 'PROCESSOR']\n",
    "\n",
    "\n",
    "# Here, we make our main experiment, only if called from the command line.\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # We initiate the user details and present a dialog to the user to get those details.\n",
    "    config.details = {\"datetime\": datetime.strftime(datetime.now(), config.DATETIME_FORMAT), \"participant\": \"\"}\n",
    "    dialog         = gui.DlgFromDict(config.details, title=\"PROTOTYPE\", fixed=[\"datetime\"])\n",
    "    \n",
    "    # We interpret the dialog actions and initiate data files if proceeding.\n",
    "    if dialog.OK:\n",
    "        config.configFile  = config.CONFIG_FILE_TEMPLATE.format(config.details[\"participant\"], config.details[\"datetime\"])\n",
    "        config.sessionFile = config.SESSION_FILE_TEMPLATE.format(config.details[\"participant\"], config.details[\"datetime\"])\n",
    "        config.mouseFile = config.MOUSE_FILE_TEMPLATE.format(config.details[\"participant\"], config.details[\"datetime\"])\n",
    "    else:\n",
    "        core.quit()\n",
    "\n",
    "    # Clocks that keep track of the experiment.\n",
    "    clockSession = core.Clock()\n",
    "    clockTrial   = core.Clock()\n",
    "    clockCue     = core.Clock()\n",
    "    mouseRecord  = core.Clock()\n",
    "\n",
    "    # We initiate some generic sounds for correct and incorrect.\n",
    "    correctSound   = SoundPygame(value=config.CORRECT_NOTE, secs=config.NOTE_DURATION)\n",
    "    incorrectSound = SoundPygame(value=config.INCORRECT_NOTE, secs=config.NOTE_DURATION)\n",
    "    \n",
    "    correctSound.setVolume(config.NOTE_VOLUME)\n",
    "    incorrectSound.setVolume(config.NOTE_VOLUME)\n",
    "    \n",
    "    # And we initiate the sounds for each digit.\n",
    "    digitSounds = [SoundPygame(value=config.DIGIT_SOUND_TEMPLATE.format(digit)) for digit in range(10)]\n",
    "    \n",
    "    # Now we save the config for this session.\n",
    "    with open(config.configFile, 'w+') as configFile:\n",
    "        json.dump({k:v for k, v in config.items() if k not in config.EXCLUDED}, configFile)\n",
    "\n",
    "    # We initiate a testing window if this is a testing run.\n",
    "    if config.TESTING:\n",
    "        testWin = visual.Window([config.TEST_WINDOW_XSIZE, config.TEST_WINDOW_YSIZE],\n",
    "                                pos=(200,200), allowGUI=False, winType='pyglet')\n",
    "        win = visual.Window([config.TEST_WINDOW_XSIZE, config.TEST_WINDOW_YSIZE],\n",
    "                            pos=(200+config.TEST_WINDOW_XSIZE, 200), allowGUI=False, winType='pyglet', color=-1)\n",
    "    else:\n",
    "        # We make a window for the experiment.\n",
    "        win = visual.Window(fullscr=True, allowGUI=False, winType='pyglet', color=-1)\n",
    "\n",
    "    # Start the mouse event\n",
    "    mouse = event.Mouse(visible=False, win=win)\n",
    "        \n",
    "    # We now start the experiment loop.\n",
    "    with open(config.sessionFile, 'w+') as outfile, open(config.mouseFile, 'w+') as mousefile:\n",
    "\n",
    "        # We first write the header of the csv file.\n",
    "        outfile.write(config.SESSION_HEADER)\n",
    "        mousefile.write(config.MOUSE_HEADER)\n",
    "\n",
    "        # Start the trial loop.\n",
    "        for trial in range(config.NTRIALS):\n",
    "\n",
    "            # Set the trial clock to 0.\n",
    "            # This clock will start counting from the wait screen, so includes that time..\n",
    "            clockTrial.reset()\n",
    "            \n",
    "            # If testing, show the blank.\n",
    "            if config.TESTING:\n",
    "                blankStimulus = config.PROCESSOR(config.BLANK_IMAGE, config.GRID)\n",
    "                rendered = config.GRID.render(blankStimulus.vector)\n",
    "                imageStimulus = visual.ImageStim(testWin, image=rendered, size=(2,2))\n",
    "                imageStimulus.draw(); testWin.flip()\n",
    "\n",
    "            # Show a prompt on grey background at the beginning of the trial and wait for a keypress.\n",
    "            bg     = visual.GratingStim(win, tex=None, mask=None, size=2, units='norm', color=0)\n",
    "            prompt = visual.TextStim(win, text=config.PROMPT_TEXT.format(trial * 100 // config.NTRIALS))\n",
    "            bg.draw(); prompt.draw(); win.flip(); event.waitKeys(clearEvents=True)\n",
    "\n",
    "            # Create a stream of digits of length NCUES for the trial.\n",
    "            stream = choices(range(10), k=config.NCUES)\n",
    "\n",
    "            # Start the cue loop.\n",
    "            for cue in range(config.NCUES):\n",
    "                \n",
    "                # Get a digit from the stream and initialise the stimulus.\n",
    "                digit    = stream.pop()\n",
    "                image    = config.IMAGES[digit]\n",
    "                stimulus = config.PROCESSOR(image, config.GRID)\n",
    "                \n",
    "                # If this is a testing run, also draw the original image.\n",
    "                if config.TESTING:\n",
    "                    originalImage = visual.ImageStim(testWin, image=color.rgb2gray(image), size=(2,2))\n",
    "                    originalImage.draw(); testWin.flip()\n",
    "                    \n",
    "                # Clear the event buffer\n",
    "                event.clearEvents()      \n",
    "                \n",
    "                # Set the mouse to the center. Might turn off, not sure which is better.\n",
    "                mouse.setPos((0,0))    \n",
    " \n",
    "                # Initialise a False keypress\n",
    "                keypressRaw = False\n",
    "        \n",
    "                # Set the cue clock to 0.\n",
    "                clockCue.reset()\n",
    "\n",
    "                # Set the mouse recording clock to 0\n",
    "                mouseRecord.reset()\n",
    "                \n",
    "                # Loop until the keypress\n",
    "                while not keypressRaw:\n",
    "                    \n",
    "                    # Get the mouse position and set the stimulus to the position.\n",
    "                    newPos = mouse.getPos()\n",
    "                    stimulus.setPos(*newPos)\n",
    "                    \n",
    "                    if mouseRecord.getTime() > config.MOUSE_RECORD_INTERVAL:\n",
    "                    \n",
    "                        mouseRow = config.MOUSE_ROW_TEMPLATE.format(\n",
    "                            trial=trial,\n",
    "                            cue=cue,\n",
    "                            digit=digit,\n",
    "                            xmouse=newPos[0],\n",
    "                            ymouse=newPos[1],\n",
    "                            cuetime=clockCue.getTime(),\n",
    "                            trialtime=clockTrial.getTime(),\n",
    "                            sessiontime=clockSession.getTime(),\n",
    "                        )\n",
    "                        mousefile.write(mouseRow)\n",
    "                        \n",
    "                        mouseRecord.reset()\n",
    "                \n",
    "                    # Render the stimulus\n",
    "                    rendered = config.GRID.render(stimulus.vector)\n",
    "\n",
    "                    # Create an image stimulus out of the rendered image.\n",
    "                    # Then show the stimulus.\n",
    "                    # Ensure stimulus is square on full screen window, assuming window has greater x dim than y dim.\n",
    "                    imstim = visual.ImageStim(win, image=rendered, size = (2 * win.size[1] / win.size[0], 2))\n",
    "                    imstim.draw(); win.flip()\n",
    "                    \n",
    "                    # Wait for a keypress. \n",
    "                    # We only need the first keypress, and want the key input from the numpage.\n",
    "                    keypresses = event.getKeys(keyList = config.KEY_LIST)\n",
    "                    if keypresses:\n",
    "                        keypressRaw = keypresses[0]\n",
    "                    #keypressRaw, *_ = event.waitKeys(clearEvents=True, keyList=config.KEY_LIST)\n",
    "                \n",
    "                # Check if their input was correct. \n",
    "                # Numpad keys are prepended with 'num_', so we strip it out.\n",
    "                keypress = keypressRaw.strip('num_')\n",
    "                correct  = (digit == int(keypress))\n",
    "                \n",
    "                # Create the data line.\n",
    "                row = config.SESSION_ROW_TEMPLATE.format(\n",
    "                    trial=trial,\n",
    "                    cue=cue,\n",
    "                    digit=digit,\n",
    "                    keypress=keypress, \n",
    "                    cuetime=clockCue.getTime(),\n",
    "                    trialtime=clockTrial.getTime(),\n",
    "                    sessiontime=clockSession.getTime(),\n",
    "                )\n",
    "                \n",
    "                # Write the data line to the session file.\n",
    "                outfile.write(row)\n",
    "\n",
    "                # Play the feedback sound.\n",
    "                correctSound.play() if correct else incorrectSound.play()\n",
    "                \n",
    "                # Play the digit sound.\n",
    "                digitSounds[digit].play()\n",
    "                \n",
    "        # At the end of all the trials, show an end screen and wait for key press\n",
    "        # to exit.\n",
    "        bg  = visual.GratingStim(win, tex=None, mask=None, size=2, units='norm', color=0)\n",
    "        end = visual.TextStim(win, text=config.END_TEXT)\n",
    "        bg.draw(); end.draw(); win.flip(); event.waitKeys(clearEvents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run digits.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import imageio\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "from skimage import color\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"xtick.direction\"] = \"in\"\n",
    "plt.rcParams[\"ytick.direction\"] = \"in\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def digit_to_image(digit : int):\n",
    "    fig = plt.figure(figsize=(1,1))\n",
    "    canvas = FigureCanvas(fig)\n",
    "    ax = plt.gca()\n",
    "    fig.patch.set_facecolor('black')\n",
    "    plt.axis('off')\n",
    "    ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "    ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.yaxis.set_visible(False)\n",
    "    plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0, hspace = 0, wspace = 0)\n",
    "    plt.text(0.25 + random.random() / 2, 0.25 + random.random() / 2, str(int(digit)),\n",
    "             size=48,\n",
    "             color='white',\n",
    "             clip_box=ax.clipbox,\n",
    "             clip_on=True,\n",
    "             horizontalalignment = 'center',\n",
    "             verticalalignment = 'center',\n",
    "             linespacing = 0,\n",
    "             #bbox=dict(facecolor='red', alpha=0.5),\n",
    "             transform=ax.transAxes)\n",
    "    #plt.savefig(f'{digit}.png', pad_inches=0, facecolor='black')\n",
    "    canvas.draw()\n",
    "    width, height = fig.get_size_inches() * fig.get_dpi()\n",
    "    image = np.frombuffer(canvas.tostring_rgb(), dtype='uint8').reshape(int(height), int(width), 3)\n",
    "    image = (image - 127.5) / 127.5\n",
    "    plt.close(fig)\n",
    "    return cv2.resize(image, dsize=(64,64)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Digit Recogniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.random.randint(0, 10, (5000))\n",
    "train_images = np.array([digit_to_image(d) for d in train_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 5s 1ms/sample - loss: 1.9070 - accuracy: 0.3668\n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 5s 1ms/sample - loss: 0.6061 - accuracy: 0.8230\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 5s 1ms/sample - loss: 0.2158 - accuracy: 0.9444\n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 5s 1ms/sample - loss: 0.1116 - accuracy: 0.9740\n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 5s 1ms/sample - loss: 0.0556 - accuracy: 0.9914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f2dce317b8>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = 5000\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "model = Sequential([\n",
    "    layers.Conv2D(16, (4,4), padding='same', strides=(1,1), activation=tf.nn.relu, input_shape=(64, 64, 3)),\n",
    "#    layers.Conv2D(16, (2,2), padding='same', strides=(1,1), activation=tf.nn.relu),\n",
    "    layers.MaxPooling2D(2),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = np.random.randint(0, 10, (500))\n",
    "test_images = np.array([digit_to_image(d) for d in test_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 384us/sample - loss: 0.0810 - accuracy: 0.9760\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08099063700437546, 0.976]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_aliased_model_path = \"./data/models/digit-aliased.h5\"\n",
    "# model.save(digit_aliased_model_path)\n",
    "digit_aliased_model = tf.keras.models.load_model(digit_aliased_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on MNIST Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "def process_images(images):\n",
    "    processed = np.stack([color.gray2rgb(cv2.resize(image, dsize=(64, 64)).astype('float32')) for image in images], 0)\n",
    "    processed = (processed - 127.5) / 127.5\n",
    "    return processed\n",
    "\n",
    "train_images = process_images(train_images)\n",
    "test_images =  process_images(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 50s 835us/sample - loss: 0.2545 - accuracy: 0.9250\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 49s 817us/sample - loss: 0.1079 - accuracy: 0.9678\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f2e4e140f0>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "model = Sequential([\n",
    "    layers.Conv2D(16, (2,2), padding='same', strides=(1,1), activation=tf.nn.relu, input_shape=(64, 64, 3)),\n",
    "    #layers.Conv2D(32, (2,2), padding='same', strides=(1,1), activation=tf.nn.relu),\n",
    "    layers.MaxPooling2D(2),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Flatten(),\n",
    "    #layers.Dense(128, activation=tf.nn.relu),\n",
    "    layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 281us/sample - loss: 0.0802 - accuracy: 0.9751\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08023650886937976, 0.9751]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_model_path = \"./data/models/mnist.h5\"\n",
    "# model.save(mnist_model_path)\n",
    "mnist_model = tf.keras.models.load_model(mnist_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting in a Generative Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_aliased_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Sequential([\n",
    "    digit_aliased_model,\n",
    "    layers.Dense(12 * 12 * 4, use_bias=False),\n",
    "    layers.LeakyReLU(),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(12 * 12, use_bias=False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_16 (Sequential)   (None, 10)                164634    \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 48)                480       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 12)                576       \n",
      "=================================================================\n",
      "Total params: 165,690\n",
      "Trainable params: 1,056\n",
      "Non-trainable params: 164,634\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Generative Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "def encoder_loss(input_digit, output_digit):\n",
    "    return cross_entropy(input_digit, output_digit)\n",
    "\n",
    "encoder_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "display_digits = np.array([i for i in range(10)])\n",
    "display_digit_images = np.array(list(map(digit_to_image, display_digits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    predictions = model(test_input, training=False)\n",
    "\n",
    "    fig = plt.figure(figsize=(5,3))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(2, 5, i+1)\n",
    "        plt.imshow(grid.render(predictions[i, :]), cmap='gray')\n",
    "        plt.title(i)\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.savefig('../data/training_intermediate_data/training_images/generated_epoch_{:02d}.png'.format(epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(digit_images):\n",
    "    with tf.GradientTape() as enc_tape:\n",
    "        encodings = encoder(digit_images, training=True)\n",
    "        encoded_images = tf.map_fn(grid.render_tensor, encodings)\n",
    "        #print(encoded_images.shape)\n",
    "\n",
    "        output_digits = mnist_model(encoded_images)\n",
    "        expected_digits = mnist_model(digit_images)\n",
    "\n",
    "         enc_loss = encoder_loss(expected_digits, output_digits)\n",
    "         # print(enc_loss)\n",
    "\n",
    "    gradients_of_encoder = enc_tape.gradient(enc_loss, encoder.trainable_variables)\n",
    "    #print(gradients_of_encoder)\n",
    "\n",
    "    encoder_optimizer.apply_gradients(zip(gradients_of_encoder, encoder.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './data/training_intermediate_data/training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(encoder_optimizer=encoder_optimizer,\n",
    "                                 encoder=encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "    for i in range(12):\n",
    "        digits = np.random.randint(0, 10, (256,))\n",
    "        digit_images = np.array(list(map(digit_to_image, digits)))\n",
    "        train_step(digit_images)\n",
    "        #print(\"BATCH DONE\")\n",
    "\n",
    "    # Produce images for the GIF as we go\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(encoder,\n",
    "                             epoch + 1,\n",
    "                             display_digit_images\n",
    "                             )\n",
    "\n",
    "#     if (epoch + 1) % 2 == 0:\n",
    "#         checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "    # Generate after the final epoch\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(encoder,\n",
    "                           epochs,\n",
    "                           display_digit_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "train(EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Static Digit Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script generates static images of centered digits as PNG files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "baseColour = (0, 0, 0)\n",
    "baseSizeX, baseSizeY = (16, 16)\n",
    "textColour = (255, 255, 255)\n",
    "\n",
    "saveExtension = \"png\"\n",
    "\n",
    "for digit in range(10):\n",
    "    baseImage = Image.new(\"RGB\", (baseSizeY, baseSizeX), baseColour)\n",
    "    base = ImageDraw.Draw(baseImage)\n",
    "\n",
    "    text = str(digit)\n",
    "    textSizeX, textSizeY = base.textsize(text)\n",
    "    textPosition = (baseSizeX / 2 - textSizeX / 2, baseSizeY / 2 - textSizeY / 2)\n",
    "    base.text(textPosition, text, textColour)\n",
    "\n",
    "    saveName = str(digit)\n",
    "    baseImage.save(f\"./data/digit-images/{saveName}.{saveExtension}\")\n",
    "\n",
    "# Blank white\n",
    "baseImage = Image.new(\"RGB\", (baseSizeY, baseSizeX), (255,255,255))\n",
    "base = ImageDraw.Draw(baseImage)\n",
    "baseImage.save(f'./data/digit-images/blank.{saveExtension}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "155px",
    "width": "209.983px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
