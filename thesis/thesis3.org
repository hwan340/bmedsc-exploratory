#+TITLE: \textbf{Using Generative Adversarial Networks to Derive Phosphene Patterns in Simulated Prosthetic Vision}
#+AUTHOR: \textbf{Student:} Jamin Wu {{{NEWLINE}}} \textbf{Student ID:} 27025861 {{{NEWLINE}}} {{{NEWLINE}}} \textbf{Supervisor:} Dr Yan Tat Wong {{{NEWLINE}}} \textbf{Co-Supervisor:} Dr Nicholas Price {{{NEWLINE}}} {{{NEWLINE}}} Department of Physiology {{{NEWLINE}}} Department of Electrical & Computer Systems Engineering {{{NEWLINE}}} \textbf{Faculty of Medicine, Nursing and Health Sciences, Monash University} {{{NEWLINE}}} {{{NEWLINE}}} Minor thesis submitted in partial fulfillment of the requirements for the {{{NEWLINE}}} degree \textbf{Bachelor of Medical Science (Honours)} at Monash University. {{{NEWLINE}}} {{{NEWLINE}}} Word Count: 14,934 words
#+OPTIONS: date:nil toc:nil num:2 H:4
#+LATEX_CLASS: book
#+LATEX_CLASS_OPTIONS: [a4paper,11pt,openany]
#+LATEX_HEADER: \usepackage{helvet}
#+LATEX_HEADER: \usepackage{gensymb}
#+LATEX_HEADER: \usepackage{xcolor}
#+LATEX_HEADER: \usepackage{rotating}
#+LATEX_HEADER: \usepackage{appendix}
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usepackage{microtype}
#+LATEX_HEADER: \renewcommand{\familydefault}{\sfdefault}
#+LATEX_HEADER: \linespread{1.5}
#+LATEX_HEADER: \usepackage{tabularx}
#+LATEX_HEADER: \usepackage{tabu}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage[margin=1.4in]{geometry}
#+LATEX_HEADER: \usepackage[sort&compress,numbers]{natbib}
#+LATEX_HEADER: \usepackage[font=small,labelfont=bf]{caption}
#+MACRO: NEWLINE @@latex:\\@@

#+LATEX: \clearpage

#+LATEX: \section*{Abstract}

#+LATEX: \subsubsection*{Background}
Cortical visual prostheses are emerging devices which are implanted on the brain to restore visual sensations to blind people.
These prostheses are currently only expected to produce a limited number of small spots of light in the visual fields known as phosphenes, often likened to "stars in a night's sky".
Phosphenes, however, tend to be irregularly distributed and display different visual properties between people.
It is therefore not clear how phosphenes can be assembled and tailored for people's different visual percepts to convey useful information.
Other image processing domains have experienced success with image generation tasks using generative adversarial networks, a machine learning technique often used to assemble pixels into novel imagery.
However, generative adversarial networks have not previously been applied to prosthetic vision to assemble phosphenes for arbitrary visual properties.

#+LATEX: \subsubsection*{Aims}
We aimed to determine whether generative adversarial networks could be used to generate phosphene patterns for different simulations of prosthetic vision, and whether these generated phosphene patterns were useful to people when applied to a digit representation task.

#+LATEX: \subsubsection*{Methods}
We developed a prototype software implementation of a generative adversarial network training architecture and a dynamic phosphene simulator using the Python programming language.
We used the phosphene simulator to generate phosphene grids with different visual properties, and ran these grids through the generative adversarial network training architecture to produce phosphene patterns representing single digits, which were qualitatively analysed.
We then recruited 11 participants for a psychophysics experiment comparing their ability to learn and recognise digits using the generated phosphene patterns versus patterns produced by a simple mask-based control.

#+LATEX: \subsubsection*{Results}
The prototype implementation we developed was successfully able to produce phosphene patterns for phosphene grids with varying resolution, spatial distribution and sizes of phosphenes.
Qualitatively, the generated phosphene patterns were recognisable as digits at higher phosphene grid resolutions, but not at lower resolutions.
Despite this, the experimental results demonstrated that people achieved better digit recognition accuracy overall using the generated phosphene patterns versus the mask-based control.
While participants did not initially achieve better digit recognition accuracy, participants' accuracy increased faster when learning the generated phosphene patterns versus control.

#+LATEX: \subsubsection*{Conclusion}
We demonstrated that a generative adversarial network training architecture can be applied to generate phosphene patterns tailored for different phosphene grids.
However, our early implementation was not robust for low-resolution grids.
Experimentally, it appears that although low-resolution generated phosphene patterns were not immediately recognisable, they may still be useful to increase the discriminability between digits when learned.
As the first attempt at using generative adversarial networks to address the perceptual limitations of phosphenes, our work highlights potential targets for further improving how phosphenes are assembled for different phosphene grid simulations.
Further work should aim at improving robustness of the prototype architecture for low-resolution phosphene grids and quantifying the intrinsic ability of low-resolution grids to represent visual information.

#+COMMENT: NEED TO ADD NUMBERS IN THE ABSTRACT

#+LATEX: \clearpage

#+LATEX: \section*{Acknowledgements}

My experience this year has been humbling, and I am extremely grateful to the people who have made this year into the incredible learning experience it has become.

Firstly, I would like to thank my supervisor Dr Yan Wong for his guidance, encouragement and compassion throughout the year.
I have been very fortunate to have been able to discuss and learn from his expertise in both physiology and engineering, and I am immensely grateful for his kindness and understanding whenever things went pear-shaped.

I would also like to thank my co-supervisor Dr Nicholas Price for his advice, support and geniality.
I have greatly valued my discussions with him and his friendliness and enthusiasm during our conversations.

I would like to thank Julian Szlawski, who helped organise the very beginning of this project and has helped me feel welcome in the Neurobionics Lab ever since.
I would like to thank Dr Zongyuan Ge, who very generously gave his time to offer his advice on useful machine learning approaches for this project.
I would like to thank Mojtaba Kermani, Timothy Allison-Walker, Dr Elizabeth Zavitz and Dr Maureen Hagan who supported me in my department oral presentations and Dr Adam Morris who tried to be there too, and with all of whom I have shared interesting conversations and received valuable feedback.
I would like to thank Dr Anand Mohan, who patiently inducted me into the lab and made me feel welcome.

I would like to thank all the participants of the psychophysics experiment in this project who generously volunteered their time to participate.

I would like to thank all the members of Journal Club, from whom I learned a great deal about the critical appraisal of journal articles and whose members were brilliant at explaining concepts I previously did not understand.
I would like to thank Lucas, Mike and Andrew for having been with me in the Neurobionics Lab, and Victor for having been with me in the Physiology Honours Room.
We shared a common fate as students and I am grateful we could journey through our research together.

#+LATEX: \clearpage

#+LATEX: \section*{Declaration}

I declare that the contents of this thesis are my own work unless explicit reference indicating otherwise has been made in the text.
I declare that this work has not been submitted for the award of any other degrees, diplomas or publication.
The background of this thesis is predominantly comprised of a literature review previously submitted for assessment as part of this degree, which has been updated to reflect examiner feedback and recent evidence.

The original premise for this project was devised by Dr Yan Wong.
I, under the guidance of Dr Yan Wong and Dr Nicholas Price, developed the aims, hypothesis and methods for the work described in this thesis and wrote all the code required during its conduct and analysis.
I performed the data collection and analysis and the interpretation of results of this work were discussed and refined with Dr Yan Wong and Dr Nicholas Price.
I wrote and edited this thesis, and its contents were discussed and reviewed with Dr Yan Wong.

#+LATEX: \clearpage

#+LATEX: \setcounter{tocdepth}{3}
#+LATEX:\tableofcontents

#+LATEX: \chapter*{List of Abbreviations}

- CVP :: cortical visual prosthesis
- CNN :: convolutional neural networks
- GAN :: generative adversarial network
- GPU :: graphic processing unit
- HPC :: high performance computing
- MASSIVE :: multi-modal Australian ScienceS Imaging and Visualization Environment
- MNIST dataset :: modified National Institute of Standards and Technology dataset
- ReLU :: rectified linear unit

#+LATEX:\listoftables
#+LATEX:\listoffigures

* Background

** Introduction

Around 36 million people worldwide were blind in 2015. cite:bourne_magnitude_2017,flaxman_global_2017
Many patients are left permanently blind when there is irreversible damage to the visual system such as from glaucoma or trauma. cite:lee_glaucoma_2005,zachariades_blindness_1996
However these patients often still have portions of the visual system intact, which could be leveraged to restore vision artificially.
*Cortical visual prostheses* are devices which aim to achieve exactly that - devices implanted on the brain which stimulate neurons to directly inject visual sensations into awareness. cite:normann_toward_2009,lewis_restoration_2015,foroushani_cortical_2018

Studies in the late 20th century showed that electrically stimulating the brain could indeed produce spot-like sensations of light, known as *phosphenes*. cite:brindley_sensations_1968,dobelle_phosphenes_1974,bak_visual_1990,bosking_electrical_2017
But these studies also revealed that we can exert only very limited control over what phosphenes look like, where they are located, and how they interact. cite:rushton_properties_1978,dobelle_phosphenes_1974,schmidt_feasibility_1996
Of these studies, those conducted in small groups also demonstrated wide variation in the appearance of phosphenes between people. cite:dobelle_phosphenes_1974,bak_visual_1990
Therefore, whilst we now know we can produce these spots of light by stimulating the brain, it is not clear how to assemble these unyielding percepts into useful vision for all implantees. cite:fernandez_development_2005,beyeler_learning_2017

To address this, new proposals of cortical visual prostheses are exploring the use of sophisticated computer processing to assemble phosphenes /strategically/ so implantees can interpret them in useful ways. cite:foroushani_cortical_2018,barnes_role_2012
These methods generally focus on addressing two significant difficulties of prosthetic vision: its low resolution, and its lack of color and brightness levels, both of which are easy to simulate on graphical displays and test.  cite:buffoni_image_2005,chang_facial_2012,sharmili_comparative_2017
However it is unclear how well these image processing algorithms could cope with the much wider range of phosphene experiences reported by perceptual studies in humans.
Thus, the primary issue facing these algorithms is how they can be made flexible to accommodate the large, uncontrollable variation in what phosphenes look like.

Recently, there have been remarkable advances in the ability of computers to /derive/ means of processing data rather than having algorithms being programmed by hand.
In this paradigm, computers iteratively "learn" and integrate patterns between data inputs and outputs through a process known as *machine learning*.  cite:guo_deep_2016
By using machine learning methods in training architectures called *generative adversarial networks* (GANs) , computers have been successul at generating new image samples from a distribution cite:NIPS2014_54230, replicating image styles. cite:gatys_image_2016
Machine learning principles could be similarly applied to prosthetic vision, using inferred patterns to derive image processing algorithms from performance rather than building algorithms around conceptions of what phosphenes look like.
This could improve the flexibility of cortical implants for different perceptual experiences and improve the usefulness of cortical visual prostheses for future implantees.


#+LATEX: \clearpage

** Cortical Visual Prostheses
<<sec:what>>

*** Overview of Cortical Visual Prostheses

A *prosthesis* is an artificial, implanted device which aims to restore a lost function to the human body. cite:thurston_pare_2007
A *visual prosthesis* is such a device which aims to restore vision to people with visual impairment. cite:weiland_visual_2008,ong_bionic_2012
A *cortical visual prosthesis* specifically refers to a vision restoration device implanted on the cortical surface of the brain, as opposed to other portions of the visual system such as the eye, optic nerve or thalamus. cite:lewis_restoration_2015
As a cortical prosthesis is implanted directly on the brain, implantees only require a functional cortical portion of the visual system; the device bypasses any damage to the eyes or nerves leading up to the brain.
While visual prostheses at other locations in the visual pathway are also capable of producing visual sensations cite:humayun_visual_1996,stingl_interim_2017,veraart_visual_1998,panetsos_consistent_2011, their use-case, implementation and evoked vision differ from those produced by cortical prostheses and are outside the scope of this review.

Research into stimulating the brain to produce vision was first pioneered by Brindley & Lewin cite:brindley_sensations_1968 and later Dobelle & Mladejovsky cite:dobelle_phosphenes_1974 in the late 20th century.
Using rudimentary hardware, these early experiments showed that a temporary implant composed of a array of electrodes could stimulate the brain in an awake patient and make them see artificial sensations of light. cite:brindley_sensations_1968,dobelle_phosphenes_1974
These artificial sensations of light are known as *phosphenes*.
Phosphenes are highly variable, but most often appear as dots of light likened to "a star in the sky". cite:dobelle_phosphenes_1974
These early successes in evoking phosphenes were instrumental in demonstrating the feasibility of cortical prostheses cite:schmidt_feasibility_1996, which have adopted phosphenes as the fundamental building blocks of prosthetic vision.

In the past 50 years since, a small number of research groups have proposed modern cortical prostheses based on the principles of these early results.
These include the Gennaris bionic vision system cite:lowery_restoration_2015,lowery_monash_2017, the Intracortical Visual Prosthesis (ICVP) Project cite:troyk_intracortical_2017, CORTIVIS cite:fernandez_cortivis_2017 and the Orion Visual Cortical Prosthesis. cite:secondsight_second_nodate
To illustrate what modern conceptions of a visual cortical prosthesis may look like, Figure [[fig:headgear]] shows a simulated render of the headgear for the Gennaris bionic vision system.

#+NAME: fig:headgear
#+CAPTION[Modelled render of the Gennaris bionic vision headgear]: A modelled render of the Gennaris bionic vision headgear. Figure courtesy of Monash Vision Group.
file:./graphics/litreview/headgear2.jpg

These devices, while still early in development, may eventually be an option for restoring a crude form of vision to patients who would otherwise be left permanently blind.
The expectation is that these devices could provide gross light perception which might allow the recognition of basic forms and movement. cite:lowery_monash_2017,lowery_restoration_2015
Current technology cannot reproduce anything close to the trichromatic, approximately 15 million pixel resolution of the human eye cite:deering_limits_1998, and as such, these devices are not yet a full replacement for vision.
The development of specific cortical prostheses has been reviewed previously cite:niketeghad_brain_2019; we briefly summarise the current progress of these devices in Table [[tab:devices]].

#+LATEX: \renewcommand{\arraystretch}{1.5}

#+NAME: tab:devices
#+CAPTION[Current progress of cortical visual prostheses]: Current progress of cortical visual prostheses. The number of electrodes places a hard upper bound on the resolution (and visual acuity) these devices can provide, so are noted here.
#+ATTR_LATEX: :environment tabu :width \textwidth :align XXXl :font \scriptsize
| Device                                                     | Electrodes                            | Progress                                                                                                              | References                                                                    |
|------------------------------------------------------------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------|
| *Orion* \newline (Second Sight)                            | 60 subdural surface electrodes        | FDA-approved clinical trial ongoing 2018-2023, six patients implanted (public scientific results yet to be released). | cite:secondsight_early_nodate,secondsight_second_nodate,niketeghad_brain_2019 |
| *Gennaris* \newline (Monash Vision Group)                  | Up to 473 penetrating microelectrodes | Ethics approved to begin clinical trials, recruiting.                                                                 | cite:lowery_monash_2017,lowery_restoration_2015,anzctr_first_2018             |
| *ICVP Project* \newline (Illinois Institute of Technology) | 16 penetrating microelectrodes        | Preclinical phase                                                                                                     | cite:troyk_intracortical_2017                                                 |
| *CORTIVIS* \newline (Universidad Miguel Hernández)         | 100 penetrating microelectrodes       | Preclinical phase.                                                                                                    | cite:fernandez_cortivis_2017                                                  |

*** Mechanism of Cortical Visual Prostheses

While the specific hardware of each device differs, the fundamental mechanism of these devices is similar.
Patients must first undergo an operation to surgically implant an electrode array in the primary visual cortex at the back of the brain. cite:lewis_restoration_2015
Historically, these were subdural surface electrodes sitting atop (but not penetrating) the brain. cite:brindley_sensations_1968,dobelle_phosphenes_1974
However, modern prostheses tend to opt for penetrating microelectrodes cite:lowery_monash_2017,troyk_intracortical_2017,fernandez_cortivis_2017 which are finer and can operate succesfully at lower electrical current. cite:bak_visual_1990,schmidt_feasibility_1996

After implantation, the physical components of the system are in place.
The system will need to be calibrated and tested before use to determine electrical stimulation thresholds and the spatial correpondence between electrodes and phosphenes in the visual fields. cite:lowery_restoration_2015,fernandez_cortivis_2017

When in use, an external camera (e.g. on glasses worn by the user) first captures an image. cite:lowery_monash_2017,lewis_restoration_2015
This image is transmitted to a portable processor, and is converted into electrical parameters for each electrode in the implanted array.
Each electrode in the implanted array then delivers pulses of electrical charge into the brain based on its parameters, which electrically stimulates nearby neurons in the cortical tissue.
Stimulating neurons in the visual cortex produces patterns of phosphenes which the patient can then perceive and interpret. cite:brindley_sensations_1968,dobelle_phosphenes_1974,bak_visual_1990
While these patterns may be difficult to decipher at first, it is expected that patients will eventually learn to match phosphene patterns to useful information such as letterforms. cite:fernandez_cortivis_2017

This process from camera image to neural stimulation loops continuously to produce a stream of images like frames of a video.
Essentially, this system provides an artificial real-time link between environmental light and visual information; a link ordinarily present in natural vision, but not present in blindness.
Figure [[fig:flowchart]] depicts the basic process of prosthetic vision in comparison to normal vision.

#+NAME: fig:flowchart
#+CAPTION[Basic flowchart of the process of prosthetic vision compared to normal vision]: A basic flowchart of the process of prosthetic vision compared to normal vision. .
#+ATTR_LATEX: :options angle=90
[[file:./graphics/litreview/flowchart.png]]

** Qualities of Prosthetic Vision
<<sec:see>>

Because cortical visual prostheses use phosphenes as the fundamental building block of prosthetic vision, it is imperative that we be able to compose multiple phosphenes into meaningful imagery.
Whether phosphenes can be composed meaningfully depends on the visual and perceptual properties of phosphenes such as their size, color and interaction with other phosphenes.

Unfortunately, due to the technical and ethical issues surrounding stimulating peoples' brains, the number of studies characterising these properties of phosphenes in humans is understandably small.
Studies which characterise phosphenes evoked /in vivo/ typically fall into two distinct groups:

1. Historical experiments with rudimentary hardware on noble volunteers cite:brindley_sensations_1968,dobelle_artificial_1974,bak_visual_1990, or
2. Modern but conservative experiments in epilepsy patients who already have electrodes implanted for clinical monitoring. cite:lee_mapping_2000,winawer_linking_2016,murphey_perceiving_2009,bosking_electrical_2017,collins_preserved_2019

As the pool of phosphene studies in humans is small and the demographics of these studies are skewed towards specific populations, we summarise the pertinent methodological features of each study in Table [[tab:populations]].
These studies constitute the major perceptual evidence that a cortical visual prosthesis can produce vision, and point towards what type of vision might be possible.

There are several studies which also attempt to characterise phosphenes in non-human primates from trained behavioural responses. cite:tehovnik_phosphene_2005,tehovnik_phosphene_2007,tehovnik_microstimulation_2007,tehovnik_microstimulation_2009
While the qualitative perceptual information offered by these studies is limited, they provide some additional information about the spatial properties of phosphenes inferred from sacaddes (rapid eye movements).

#+LATEX: \linespread{1.1}
#+LATEX: \newgeometry{margin=2cm}
#+LATEX: \thispagestyle{empty}
#+NAME: tab:populations
#+CAPTION[Participant demographics of studies looking at cortical phosphenes evoked /in vivo/ in humans]: The participant demographics of studies which have looked at cortical phosphenes evoked /in vivo/ in humans.
#+ATTR_LATEX: :float sideways :environment tabu :align rlXX[2]X[2]X[2] :font \scriptsize
|       Date | Reference                                             | Setting                                                    | Electrodes                                                                                       | Parameters                                                                                     | Patient Demographic                                                                                                                                                                              |
|------------+-------------------------------------------------------+------------------------------------------------------------+--------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|       1968 | cite:brindley_sensations_1968                         | Acute                                                      | 80 surface electrodes (array) on occipital cortex                                                | Monophasic trains with 0.2ms pulses of unknown current (power 90mW) at 100Hz                   | 1 patient blind from  glaucoma and retinal detachment approx 1 year prior  (female, 52 years)                                                                                                    |
|       1974 | cite:dobelle_phosphenes_1974                          | Acute                                                      | Variable number of surface electrodes on occipial cortex                                         | Monophasic or biphasic trains with 0.25-2ms/phase pulses of up to 1-5mA at 30-200Hz for 1000ms | 15 patients with cerebral tumours and partial visual field defects or normal sight (11 male, 4 female, 20-71 years)                                                                              |
|       1974 | cite:dobelle_artificial_1974                          | Acute                                                      | 64 subdural surface electrodes (array) on occipital cortex                                       | Biphasic trains with 0.5ms/phase pulses of up to 8mA at 50Hz for unknown duration              | 1 patient blind from congenital cataract in one eye and glaucoma and retinal detachment in the other for 28 years (male, 45 years); and 1 patient blind from trauma for 7 years (male, 28 years) |
|       1978 | cite:rushton_properties_1978                          | Chronic \newline {\tiny 5\textonehalf  years post implant} | Unknown                                                                                          | Unknown-phase trains with with up to 2ms pulses of ?mA at 2-1000Hz for 1-8 pulses              | Unknown                                                                                                                                                                                          |
| 1976, 1979 | cite:dobelle_braille_1976,dobelle_mapping_1979        | Chronic \newline {\tiny unknown years post implant}        | 64 subdural surface electrodes (array) on occipital cortex                                       | Biphasic trains with 0.25ms/phase pulses of 0.5-4.0mA at 50Hz for 500-1000ms                   | 1 patient blind from trauma 10 years prior to implantation (male, 33 and 35 years)                                                                                                               |
|       1990 | cite:bak_visual_1990                                  | Acute                                                      | 1-3 intracortical penetrating microelectrodes on occipital cortex                                | Biphasic trains with 0.2ms/phase pulses of up to 200\mu A at 100Hz for 100-1000ms              | 3 sighted patients with epilepsy (unknown demographic?)                                                                                                                                          |
|       1996 | cite:schmidt_feasibility_1996                         | Acute                                                      | 38 intracortical penetrating microelectrodes on occipital cortex                                 | Biphasic trains with 0.2-0.8ms pulses of up to 80\mu A at 75-200Hz for 125-250ms               | 1 patient blind from glaucoma 22 years prior (female, 42 years)                                                                                                                                  |
| 1994, 1999 | cite:allison_face_1994,puce_electrophysiological_1999 | Acute                                                      | Unknown number of surface electrodes on extrastriate visual cortex                               | Biphasic trains with 0.2ms pulses of 2-10mA at 50Hz for 5000ms                                 | Unknown                                                                                                                                                                                          |
|       2000 | cite:dobelle_artificial_2000                          | Chronic \newline {\tiny 21 years post implant}             | 64 (subdural?) surface electrodes (array) on on occipital cortex                                 | Biphasic trains with 0.5ms/phase of (10-20V) at 30Hz for 1-50 pulses                           | 1 patient blind from trauma 5 years prior to implantation (male, 62 years)                                                                                                                       |
|       2000 | cite:lee_mapping_2000                                 | Acute                                                      | Total 271 subdural surface electrodes on occipital cortex and adjacent areas across all subjects | Biphasic trains with 0.3ms pulses of 1-15mA at 50Hz for 5000ms                                 | 23 sighted patients with epilepsy (12 male, 11 female, 16-41 years)                                                                                                                              |
|       2009 | cite:murphey_perceiving_2009                          | Acute                                                      | Total 50 subdural surface electrodes on 11 different visual areas across all subjects            | Biphasic trains with 0.2ms pulses of 0.49-7mA at 200Hz for 300ms                               | 10 sighted patients with epilepsy (6 male, 4 female, 19-67 years)                                                                                                                                |
|       2016 | cite:winawer_linking_2016                             | Acute                                                      | 1 or 2 subdural surface electrodes on V1 studied per subject                                     | Biphasic trains with 0.2-1ms pulses of 0.2-5mA at 5-100Hz for 200-1000ms                       | 4 sighted patients with epilepsy (3 male, 1 female, 24-40 years)                                                                                                                                 |
|       2017 | cite:bosking_saturation_2017                          | Acute                                                      | Up to 16 subdural surface electrodes (array) on early occipital cortex per subject               | Biphasic trains with 0.1ms/phase pulses of 0.3-4.0mA at 200Hz for 200-300ms                    | 15 sighted patients with epilepsy (5 male, 10 female, 22-61 years)                                                                                                                               |
|       2018 | cite:bosking_rules_2018                               | Acute                                                      | Up to 16 subdural surface electrodes (array) on early occipital cortex per subject               | Biphasic trains with 0.1ms/phase pulses of 0.3-4.0mA at 200Hz for 200-300ms                    | 8 sighted patients with epilepsy                                                                                                                                                                 |
|       2018 | cite:beauchamp_dynamic_2018                           | Acute                                                      | 16 or 24 subdural surface electrodes (array) on early occipital cortex per subject               | Biphasic trains with 0.1ms/phase pulses of 0.3-4.0mA at 200Hz for 50-300ms                     | 4 sighted patients with epilepsy (all male, 20-54 years) and 1 patient blind 8 years prior, unspecified reason (female, 35 years)                                                                |
|       2019 | cite:collins_preserved_2019                           | Acute                                                      | 16 subdural surface electrodes (array) on occipital cortex                                       | Biphasic trains with 1ms pulses of up to 11mA at 60Hz                                          | 1 patient with epilepsy and a partial visual fied defect for 30 years from AVM haemorrhage (male, 45 years)                                                                                      |
#+LATEX: \restoregeometry
#+LATEX: \linespread{1.5}

*** Phosphenes Produced by Stimulating a Single Electrode

The most common result of stimulating a single electrode is a single phosphene characterised as a small, localisable dot of light likened to a star. cite:brindley_sensations_1968,dobelle_phosphenes_1974,schmidt_feasibility_1996,lee_mapping_2000
Every study has, however, demonstrated large variation on this basic percept.

**** The Quality of a Phosphene

Being able to perceive qualities of light such as brightness and colour gives us richer, more specific information about the world. cite:solomon_machinery_2007,vladusich_brightness_2007
Composing images with phosphenes of different brightness and colour would enable us to mimic the richness of natural visual information.

It therefore seems promising that one of the most consistently reported features of phosphenes is that different levels of brightness /are/ perceivable and even modifiable.
The brightness of a phosphenes reproducibly increases with stimulation amplitude, pulse duration and pulse frequency. cite:dobelle_phosphenes_1974,dobelle_artificial_1974,rushton_properties_1978,schmidt_feasibility_1996,dobelle_artificial_2000,winawer_linking_2016
An early study estimated up to 12 distinguishable levels of brightness by varying the stimulation amplitude of a surface electrode. cite:rushton_properties_1978

The colour of phosphenes, however, is not as promising.
Phosphene colours range from colourless to vididly coloured with large inter-individual variation.
Some patients only report seeing white or colourless phosphenes. cite:brindley_sensations_1968,dobelle_phosphenes_1974,bak_visual_1990,dobelle_artificial_2000
Others have reported a spectrum across almost every reportable colour and beyond to 'other-wordly' colours. cite:dobelle_phosphenes_1974,rushton_properties_1978,bak_visual_1990,schmidt_feasibility_1996,puce_electrophysiological_1999,lee_mapping_2000,murphey_perceiving_2009
Sighted patients looking at a white background have also reported seeing black phosphenes, though this finding is not well reported elsewhere. cite:lee_mapping_2000
While it was previously speculated that blind patients saw colourless phosphenes due to long-term sight deprivation cite:dobelle_phosphenes_1974, this is not consistently the case, and coloured percepts have also been reported by a patient blind for 22 years. cite:schmidt_feasibility_1996
Least promising is that colour is not consistently modifiable using different parameters of electrical stimulation, meaning phosphenes are most often randomly coloured. cite:rushton_properties_1978

**** Spatial Properties of Phosphenes

Of great concern to cortical visual prostheses is how phosphenes are arranged in visual space, which may affect the shapes of patterns that can be formed by prosthetic devices.
The visual cortex, as a sensory surface, is mapped retinotopically i.e. such that regions in the visual field which are next to each other are also next to each other on the cortex (though they may be distorted). cite:fox_retinotopic_1987,engel_retinotopic_1997
Electrodes placed over visual cortex appear to follow this mapping, and relationships between adjacent electrodes are roughly conserved. cite:brindley_sensations_1968,dobelle_mapping_1979,beauchamp_dynamic_2018
Figure [[fig:map]] illustrates the mapping of a 64-electrode array to phosphene locations measured by perceptual testing.

#+NAME: fig:map
#+CAPTION[Spatial distribution of phosphenes mapped to the visual fields]: Spatial distribution of phosphenes mapped to the visual fields (left) in a patient implanted with an early 64-electrode array (right). Figure from Dobelle et al. 1979 cite:dobelle_mapping_1979
[[./graphics/litreview/map.png]]

However, while we can very grossly estimate positions of phosphenes in the visual field (especially in relation to the calcarine sulcus, below which phosphenes correspond to superior fields), the distortion of retinotopy on the visual cortex means /precise/ mapping is not possible until post-implantation.
In sighted patients, phosphene locations can be mapped with receptive fields in response to visual stimuli, to which they closely correspond. cite:bosking_saturation_2017,bosking_rules_2018,beauchamp_dynamic_2018
This is clearly not possible in blind patients, so phosphenes are often mapped by indicating directions or relative positions of pairwise phosphenes. cite:schmidt_feasibility_1996,beauchamp_dynamic_2018,brindley_sensations_1968,dobelle_mapping_1979
The implication is that while we can roughly determine the quadrant of a phosphene in the visual field at implantation, we cannot know precisely where it is located until stimulation is trialled.
In addition, sometimes stimulating one electrode produces more than one phosphene, which may be either adjacent or inverted about the horizontal meridian of the visual field. cite:brindley_sensations_1968,dobelle_phosphenes_1974,schmidt_feasibility_1996
This is most likely attributable to off-target stimulation of tissue across a sulcus, supported by observations that this phenomena occurs less severely with penetrating microelectrodes (which discharge less than surface electrodes). cite:dobelle_mapping_1979,schmidt_feasibility_1996

The space a phosphene occupies in the visual fields varies with eccentricity and stimulation current.
Early evaluations of phosphene size using various objects at arm's length cite:brindley_sensations_1968,dobelle_phosphenes_1974,dobelle_artificial_2000,dobelle_artificial_1974,schmidt_feasibility_1996 have generally been agreeable with more formal estimates using degrees of visual field. cite:bak_visual_1990,bosking_saturation_2017,winawer_linking_2016
Most phosphenes are 1-2\degree  of visual field in diameter and range from 0.1-10\degree  (a "grain of sago" to a coin at arm's length) . cite:bak_visual_1990,bosking_saturation_2017,brindley_sensations_1968
The size of phosphenes depends on where they are located in the visual fields; more peripheral phosphenes are larger and reportedly have less distinct borders. cite:rushton_properties_1978,winawer_linking_2016,bosking_saturation_2017
The variation of phosphenes with size is consistent with behavioural studies in non-human primary undergoing cortical stimulation cite:tehovnik_phosphene_2007 and the phenomenon of cortical mangnification, where the central visual field is overproportionately represented on the surface of the brain. cite:born_cortical_2015
Phosphenes also appear to increase in size with stimulation amplitude cite:rushton_properties_1978,winawer_linking_2016,bosking_saturation_2017, though one early report of microelectrode stimulation also described instances where phosphene size decreased which have not been subsequently reproduced. cite:schmidt_feasibility_1996

While circular phosphenes are ubiquitous cite:brindley_sensations_1968,dobelle_phosphenes_1974,bak_visual_1990,schmidt_feasibility_1996,lee_mapping_2000, other phosphenes shapes have been reported.
The most consistently reported shape other than circles are elongated elliptical or linear phosphenes oriented in horizontal, oblique or vertical orientations. cite:brindley_sensations_1968,dobelle_phosphenes_1974,rushton_properties_1978,bak_visual_1990,beauchamp_dynamic_2018
A few reports identify shapes ranging from triangles and stars, to checkerboards, to face or eye-like hallucinatory sensations. cite:lee_mapping_2000,murphey_perceiving_2009
Often, more abstract phosphenes appear on stimulation of later visual areas of the brain, which may not be relevant for prostheses targeting only primary visual cortex. cite:murphey_perceiving_2009
While phosphene shapes appear loosely related to the putative role of different brain regions cite:lee_mapping_2000, no studies have been able to deliberately control the shape of phosphenes.

Finally, phosphenes have been repeatedly shown to move with eye movements and have been likened to the movement of retinal afterimages. cite:brindley_sensations_1968,dobelle_artificial_1974,schmidt_feasibility_1996
It appears the whole map of phosphenes moves as multiple phosphenes maintain their relative positions after movement. cite:dobelle_artificial_1974,schmidt_feasibility_1996

**** Temporal Properties of a Phosphene

Phosphenes generally appear synchronous with stimulation. cite:schmidt_feasibility_1996,beauchamp_dynamic_2018
It is difficult to measure the latency of percepts without also including motor reaction time, but studies comparing phosphene onset reaction times to auditory stimuli suggest that additional latency is minimal. cite:rushton_properties_1978
In multiple studies, phosphenes have been sporadically reported to persist for up to 20 minutes after stimulation ceased, particularly after a high-discharge stimulation prior. cite:brindley_sensations_1968,dobelle_artificial_1974,rushton_properties_1978,schmidt_feasibility_1996
Perhaps paradoxically, phosphenes purposefully sustained by continuous stimulation demonstrate significant fading in as little as 15 seconds. cite:dobelle_phosphenes_1974,schmidt_feasibility_1996
The fading effect of phosphenes is also reflected over separate trials, where phosphenes progressively dim in each subsequent trial (though they "reset" the next day). cite:schmidt_feasibility_1996

On a shorter time scale, phosphenes elicited by surface stimulation may also flicker.
The phosphene flicker produced by surface electrodes is fixed, fast, and asynchronous with hardware or physiological pulses. cite:brindley_sensations_1968,dobelle_artificial_1974,dobelle_phosphenes_1974,rushton_properties_1978,dobelle_artificial_2000
This differs from the "flicker" produced by two separate successive stimulations, which disappears at stimulation frequencies of approximately 33Hz (though an overlying intrinsic flicker remains). cite:rushton_properties_1978
It is unknown whether flicker also occurs in stimulation with microelectrodes; of the few studies of stimlation with penetrating microelectrodes, flicker was not reported.   cite:bak_visual_1990,schmidt_feasibility_1996

In summary, we can exert only very limited control over what individual phosphenes look like.
Phosphenes are also highly variable, both between-individuals and between-electrodes.
While there are points of agreement between studies, such as the effect of stimulation current on brightness, other phenomena, such as colour and flickering, remain contentious.
It remains unclear whether these disagreements are due to differences in stimulation parameters, hardware, participants or pathology.

*** Phosphenes Produced by Stimulating Multiple Electrodes

The appearance of images containing multiple phosphenes is fundamental to modern cortical prostheses as very little information can be transmitted through only a single electrode at once. cite:niketeghad_brain_2019,lewis_restoration_2015
The intention for cortical prostheses is to produce perceivable /patterns/ which can be interpreted.
The eventual hope is to approximate natural images with phosphenes used like pixels of a graphical display.
Early chronic implants operated on this principle, albeit with very low resolution. cite:dobelle_artificial_2000

However, the empirical evidence on /what/ is perceived when multiple electrodes are stimulated is surprisingly scarce.

At the most simple level, two electrodes which produce individual phosphenes appear to also produce two separate perceivable phosphenes when stimulated simultaneously.  cite:brindley_sensations_1968,dobelle_phosphenes_1974
Sometimes, the size of each phosphene decreases compared to individual stimulation, and the distance between phosphenes may increase. cite:bosking_rules_2018
When close together, these phosphenes may fuse together into a single percept. cite:brindley_sensations_1968,dobelle_phosphenes_1974
However, this is not always the case; in fact, dimmer phosphenes may not be perceived at all cite:bosking_rules_2018,dobelle_artificial_1974,dobelle_phosphenes_1974, though there is some evidence that increasing the stimulation amplitude may reintroduce the dimmer percept. cite:schmidt_feasibility_1996
As a result, increasing the number of electrodes may not linearly increase the number of perceived phosphenes.

Several studies have characterised greater numbers of simultaneous phosphenes.
Early evidence suggested that four-phosphene patterns (e.g. a square) could be recognised, but not reliably as spurious phosphenes appeared and some expected phosphenes were not perceived. cite:dobelle_artificial_1974
Another patient was able to perceive a six-phosphene vertical line. cite:schmidt_feasibility_1996
Modern studies, however, have provided conflicting results.
In one study in an epilepsy patient, five electrodes stimulated at once were only able to produce two perceivable phosphenes that was not simply the aggregate of each of the five phosphenes. cite:beauchamp_dynamic_2018
In non-human-primates, stimulation of visual cortex simultaneously at two spacially distant points did not sum to an joint signal, further suggesting a separation of processing of simultaneous stimulation. cite:ghose_strong_2012

The reasoning behind this difficulty is thought to be because cortical visual prostheses unselectively stimulate local regions of the brain.
In normal primary visual cortex, neurons are typically selectively stimulated by specific image features such as the orientation of lines in the visual fields. cite:ben-yishai_theory_1995
When electrodes instead unselectively stimulate neurons, the pattern of neural stimulation is unnatural and later visual areas may not immediately be able to decode the unrecognisable stimulus. cite:beauchamp_dynamic_2018

Despite the difficulties of these temporary experiments, chronic studies suggest that patients are able to use this information usefully after a learning period.
There are brief reports of a patient with a chronic implant being able to read phosphene patterns on a 64-electrode implant at 30 letters per minute, similar to Braille cite:dobelle_braille_1976
Reports on a different patient from the same group described the ability to recognise symbols and letters at an estimated visual acuity of 20/1200 (seeing at 20 metres what could normally be seen at 1200 metres). cite:dobelle_artificial_2000
However, due to the absence of any further chronic studies of implants in blind patients, the upper limit to which people can learn to recognise phosphene patterns is unknown.

Given these limited studies of combinations of phosphenes, there is a tremendous gap between the current knowledge of phosphene patterns and the proposed mechanism of cortical prostheses.
It is entirely unclear whether people can perceive phosphene patterns on the order of tens or hundreds, whether people can learn to perceive these patterns in useful ways, or to what degree these patterns may change.

*** Summary of the Perceptual Limitations of Phosphenes

The major issues surrounding the current literature on phosphenes are therefore:

1. *There are no modern studies of phosphenes evoked in blind but otherwise-healthy patients, the primary demographic of cortical visual prostheses*.
   There are also scant chronic studies, none of which have been conducted with penetrating microeletrodes. cite:rushton_properties_1978,dobelle_artificial_2000,dobelle_braille_1976
   Emerging clinical trials will help resolve this issue, but until such studies bear fruit, our knowledge on what cortical prosthetic vision looks like may not be readily applicable to new devices.
2. *Phosphenes are highly variable*.
   Almost all features of phosphenes display uncontrollable variability, and the only two properties of phosphenes we have been shown to reliably control are phosphene brightness and size. cite:rushton_properties_1978
   This variability permeates between electrodes, between patients and between studies.
   The heterogeneity, low sample size and skewed populations of the literature have made it difficult to distinguish the root cause of such variation.
3. *The interpretibility of patterns formed by multiple phosphenes is unclear.*
   There is conflict amongst studies on whether multiple phosphenes at once can be integrated simultaneously, or whether people can learn can compensate for initial difficulties with interpreting phosphenes.

As a result, there is considerable uncertainty on exactly what visual sensations modern devices can give to implantees on a case-by-case basis.

\clearpage

** Making Prosthetic Vision Useful
<<sec:useful>>

Our ability to control the appearance of individual phosphenes and their patterns is clearly limited.
In this section, we briefly review current literature on how images can be represented strategically in phophene space to overcome these limitations.

*** The Role of Simulated Prosthetic Vision

Because of the difficulties of implanting electrode arrays, little research has been conducted on what methods of representing information in phosphene space are most useful /in vivo/.
The only cortical implant which has been connected to a camera in humans was the Dobelle Implant in 2000. cite:dobelle_artificial_2000.
The Dobelle Implant used direct image processing techniques fitting of the software capabilities of the time, which essentially downsampled the camera image and directly mapped the brightness to implant electrodes. cite:dobelle_artificial_2000.
The group briefly entertained the idea of using edge-detection for more selective stimulation, but no subsequent studies reported the outcomes of this idea.

To allow the testing of new image processing algorithms in the absence of access to real implantees, research in image processing algorithms has largely moved to simulations of prosthetic vision. cite:chen_simulating_2009-1,chen_simulating_2009
Simulated prosthetic vision is the primary vehicle through which most new image processing algorithms are tested.
The features of simulated prosthetic vision have been reviewed previously. cite:chen_simulating_2009-1
Briefly, camera information is processed and rendered onto a head-mounted or other display as simulated phosphenes.
Typically, these simulated phosphenes are rendered as the most commonly reported percept - white dots with a Gaussian blur filter applied. cite:chen_simulating_2009-1
In this way, phosphene "images" are displayed for the user with the aim to approximate the prosthetic vision of an implantee.
Examples of these simulated renders are shown in Figure [[fig:simulated]].

#+NAME: fig:simulated
#+CAPTION[Examples of different simulated renders of phosphenes]: Examples of different simulated renders of phosphenes. Figure from Chen et al. 2009. cite:chen_simulating_2009-1
[[file:./graphics/litreview/simulated.png]]

*** A Brief Outline of Current Image Processing Approaches
**** Direct Mask-Based Methods

The prevailing paradigm of image processing for early cortical prostheses was to directly map camera images to a grid of electrodes as though they were superimposed. cite:schmidt_feasibility_1996,dobelle_artificial_2000
This produces a phosphene image like a mask full of holes placed on top of the original image.
In this way, prosthetic vision began by attempting to emulate natural vision as closely as possible.

Such an approach may work with a large number of electrodes if all phosphenes could be interpreted correctly as "pixels".
One study estimated that approximately 625 phosphenes would be sufficient to reach a visual acuity of 20/30, suitable for most general tasks cite:cha_simulation_1992.

However, there are several issues of direct methods when compared with the perceptual limitations of phosphenes:
Direct mask-based methods, by virtue of keeping faithful to the original image, tend to produce simulated phosphene renders with large numbers of "on" phosphenes, particularly in well-lit environments.
As the ability to interpret multiple phosphenes simultaneously is not well established, stimulating many electrodes at once may not produce the expected visual percept.
Because of the high variability of phosphenes, it is also unlikely that the quality and spatial distribution of pixels of a transformed image could be reproduced as faithfully as intended.

Moreover, the quality of these methods very rapidly degrades once resolution drops. cite:li_image_2018
No new implants are capable of producing 625 distinct phosphenes. cite:lewis_restoration_2015
Without the resolution to support the interpretation of low-level features of directly processed images, images can be uninterpretable.

**** Edge-Based Methods

Edge-detection refers to image processing algorithms methods which highlight the edges of objects only. cite:canny_readings_1987
Edges require less phosphenes at once and may reduce the amount of redundant information in an image.
This is important when we can consider that the number of perceivable phosphenes may not increase linearly with the number of stimulated electrodes. cite:bosking_rules_2018

Edge detectors such as the Canny cite:canny_readings_1987 edge detector are widely used.
These edge detectors are able to detect fast pixel gradients in images, which typically occur at boundaries.
If additional inputs to the processing algorithm are possible, then more sophisticated techniques can be used.
For example, the use of a range camera or other depth sensing devices can be used to more intelligently find non-background edges. cite:lui_transformative_2012
State-of-the-art convolutional neural networks (CNNs) have also been applied to edge detection for prosthetic vision by semantic pixel labelling of images of rooms and determining edges by boundaries between walls. cite:sanchez-garcia_structural_2018
An illustration of an edge-detection algorithm combined with a object-filling algorithm is shown in Figure [[fig:edgeandfill]].

#+NAME: fig:edgeandfill
#+CAPTION[Example of using an edge-detection algorithm to render a clean simulated phosphene image]: An example of an edge-detection algorithm with a CNN-driven object-filling algorithm to render a clean simulated phosphene image. Figure from cite:sanchez-garcia_structural_2018
[[file:./graphics/litreview/edgeandfill.png]]

The difficulty with edge-based methods is that edges easily degrade when resolution drops, similar to direct methods. cite:buffoni_image_2005
One method which aimed to resolve the fragility of edges combined edge-based methods with saliency-based methods to give greater form to objects. cite:han_object_2015
Such hybrid methods may be more robust than the use of pure edges when faced with significant downsampling.

**** Saliency-Based Methods

As opposed to naively translating brightness values of camera images to electrode stimulation, saliency-based measures more intelligently identify the semantics of objects in a scene.
Using this semantic structure, the image can be divided or /segmented/ into regions of interest which carry a common semantic meaning (e.g. "background" or "foreground"). cite:pal_review_1993

With this approach, more deliberate differences between foregound and background can be made as depicted in Figure [[fig:saliency]]. cite:guo_optimization_2018
When applied to an image classification task, saliency-based methods improved the recognition accuracy of common objects. cite:han_object_2015,li_image_2018

Object detection neural networks have also been applied to highlight particular salient features of an image. cite:mace_simulated_2015
In these methods, powerful image classification algorithms are able to detect a specified object and solely highlight that object on the simulated phosphene render.
While these methods were constrained to only specific objects, they demonstrate leverage of modern progressions in image processing to intelligently identify objects.

#+NAME: fig:saliency
#+CAPTION[Example of using a saliency-based algorithm to highlight a region of interest]: An example of a saliency-based algorithm to highlight a region of interest of the image; different panels show progressive stages in the process. Figure from cite:li_image_2018
[[file:./graphics/litreview/saliency.png]]

The methods help highlight what is most likely to be relevant in an image and suppress background, which may otherwise interfere.
However the usefulness of this masking approach is importantly constrained by the implantee's ability to subsequently recognise what is being shown.
While simulations have demonstrated the utility of this approach, the fidelity of the mask form when phosphenes are irregularly shaped and sized is not clear.
While these methods are advantageous compared to direct methods in that irrelevant information may be reduced, it faces the same limitations that low-level forms may be obscured by phosphene distortions.

**** Transformative Methods

Recent advances in machine learning have meant that computers are now reaching human-level abilities for tasks such as image classification using deep learning methods. cite:rawat_deep_2017,guo_deep_2016
Because the processor in a cortical visual prosthesis has access to the full camera image (as opposed to the user, who can only see the phosphene version), the processor has more information available to interpret.
Instead of expecting the user to interpret high-level information from degraded phosphene images, some interpretation could be relegated to the computer which can then intelligently re-encode the information in a deliberate manner.

Numerous patents have been filed for such a system. cite:chichilnisky_eduardo-jose_smart_2018,li_going_2013
In these systems, important visual cues such as stairs, faces and bank notes are recognised by the computer, which can then remove unnecessary low-level detail and produce compact, abstract images that represent the /concept/ of what is seen, not what is actually seen.
For example, faces can be recognised and re-encoded as emoticons which cleanly fit in low-resolution space. cite:lui_transformative_2012
An example of this approach is shown in [[fig:transformative]]

#+NAME: fig:transformative
#+CAPTION[Example of using a transformative approach to image processing to re-encode information]: An example of a transformative approach to image processing to re-encode information. Figure from cite:lui_transformative_2012
[[file:./graphics/litreview/transformative.png]]

The chief benefit of such methods is that useful information can be communicated with less phosphenes.
Since many low-level details (e.g. "is this bank note folded at the corner?") are not always relevant, the information burden to the user can be reduced to only what is necessary.

The perceptual issues facing these methods are that these typically rely on producing phosphene images that "mimic" real life (e.g. emoticons, which attempt to mimic faces). cite:lui_transformative_2012
However, the perceptual distortions and variability of phosphenes make it unclear whether these mimics could be replicated and thus the ability to specifically evoke these "mimics" /in vivo/ with phosphenes is not well established.
What may appear cleanly represented in simulated phosphene space may be heavily distorted and even unrecognisable in real implantee settings.
A potential rebuttal is that as these methods re-encode information at the bequest of the algorithm implementer, they could be optimised on a case-by-case basis for the particular phosphenes an implantee sees.
Individual-level implementations of image processing algorithms are yet to be explored.

**** Temporal Methods

Given the challenges already faced by patients when trying to interpret multiple simultaneous phosphenes, some groups have begun to explore non-simultaneous methods of conveying patterned information.
/Dynamic current steering/, where phosphene patterns are "traced" in quick succession rather than presented all at once have shown sigificant benefits for letter recognition tasks. cite:beauchamp_dynamic_2018,spencer_creating_2018
Figure [[fig:temporal]] illustrates the principle behind this approach.
Patients, without prior training, were able to trace the path of phosphenes and interpret simple letterforms accurately.
Unlike most of these other methods, dynamic current steering /has/ been tested /in vivo/ in patients with epilepsy with subdural surface electrodes and in fact arose out of perceptual difficulties noted by the investigators. cite:beauchamp_dynamic_2018
This highlights one of the issues with simulated prosthetic vision tests; fundamental perceptual differences of electrical neural stimulation may not be discovered and accounted for until tested in real patients.

#+NAME: fig:temporal
#+CAPTION[Example of conveying information temporally through phosphenes]: An example of a temporal approach to conveying information through phosphenes, where electrodes are stimulated in succession instead of simultaneously. A) implant locations, B) phosphene map, C) stimuation order, D) phosphene form, E) participant interpretation. Figure taken from cite:beauchamp_dynamic_2018
[[file:./graphics/litreview/temporal.png]]

These methods attempt to resolve the issues surrounding interpreting multiple simultaneous phosphenes by tapping into the brain's natural ability to interpret gross motion. cite:grossman_brain_2002
While temporal methods may be slower at conveying information per unit time, they have the advantage of not requiring simultaneous presentations of phosphenes, and requiring less current as only a limited number of phosphenes need to be conveyed for a single frame.
It is clear, however, that the difference between temporal methods and form-based methods cannot be assessed in simulated prosthetic vision as the differences between these two methods is based on differences in /in vivo/ perception.
More clinical trials are needed to establish whether temporal methods of information transfer as opposed to spatial methods are better suited for cortical prosthetic devices.

*** The Limitations of Simulated Prosthetic Vision

The crux of many of these methods rests on tests of simulated prosthetic vision, faces significant limitations for generalisability to /in vivo/ implants:

1) *The possible difficulties /in vivo/ of interpreting phosphenes are not accounted for.*
   One of the key unknowns in phosphene space are how well the brain can learn to decipher the unselective unnatural stimulation of visual cortex by multiple electrodes simultaneously. cite:beyeler_learning_2017
   Because our uncertainty in this area relates to the unnatural neural stimulation of the visual cortex, this cannot be assessed in a simulated setting.
   When phosphenes are simply shown on a display, a sighted subject naturally is able to process the displayed patterns making full use of the retinal and neural circuity distal to the brain.
   This is most certainly not the case for an implantee.
   While normal-sighted subjects have often been shown to be able to recognise complex patterns with many simultaneous phosphenes in a simulated setting cite:chen_simulating_2009, it is still unclear whether this can be replicated in real implantees.
2) *Phosphene simulation often does not account for all known properties of phosphenes.*
   The properties we have described above are rarely all accounted for.
   For example, temporal effects such as fading and accommodation are not implemented in most simulations.
   Additionally, most simulations render phosphene images as low-resolution greyscale images with uniform circular cite:mccarthy_mobility_2014,hu_recognition_2014,sanchez-garcia_structural_2018,li_image_2018 or hexagonal pixels. cite:chen_effect_2004, though some studies have also incorporated biologically-based retinotopic distortion. cite:josh_real-time_2011,josh_psychophysics_2013
   Indeed, many simulated prosthetic vision algorithms approach the problem as chiefly one of low resolution, loss of colour and distortion.
   This does not accurately reflect the rich (but uncontrollable) perceptual experiences previously reported by /in vivo/ experiments in shape, size, colour or flicker.
3) *Many simulations use regular and higher-resolution phosphene grids than have previously been achieved.*
   Sometimes, psychophysical experiments render on the order of a thousand phosphenes cite:sanchez-garcia_structural_2018,li_image_2018,guo_optimization_2018
   This far outstrips the number of phosphenes which have been tested simultaneously /in vivo/ so far (less than a hundred) cite:dobelle_artificial_2000, and also surpasses the estimates of capabilities of modern prostheses. cite:lewis_restoration_2015
   Ultimately, the ability to reliably evoke many cortical phosphenes regularly and of the calibre of many simulations has not been established.
4) *Most simulations do not specifically target cortical phosphenes.*
   Most advances in image processing methods for simulated prosthetic vision are targeted at retinal prostheses, for which there are already commercially available devices. cite:stingl_interim_2017,luo_argus_2016,markowitz_rehabilitation_2018
   The lack of distinction between different biological methods of evoking phosphenes in some experiments of simulated prosthetic vision mean results may not be directly translatable.

The implication of these issues is that studies of image processing algorithms in simulated prosthetic vision are not flexible for different phosphene percepts.
The methods we have described are dependent on being able to replicate the simulated renders in /in vivo/ implants, but little research has been conducted on this area.
It is unclear how the image processing algorithms produced by studies of simulated prosthetic vision could be made flexible for the variability in phosphenes previously described.

*** Future Directions from Advances in Machine Learning

The remaining goal for better addressing the perceptual limitations for phosphenes is to find flexible ways to reconcile image processing algorithms in simulated phosphene space with the wide variability in what phosphenes look like.
Image processing outside of prosthetic vision has experienced a wealth of improvements from advances in machine learning, where computers learn patterns from data without prior knowledge of those patterns. cite:guo_deep_2016
Machine learning for image processing is most often applied to the training of *convolutional neural networks* (CNNs), which are layered architectures of image filters modelled after the function of physiological neurons. cite:rawat_deep_2017
CNNs have already discovered widespread use in image recognition tasks cite:krizhevsky_imagenet_2012, video recognition tasks cite:karpathy_large-scale_2014 and style transfer tasks, where images are modified to mimic the style of other images. cite:gatys_image_2016

The benefit of CNNs is that they produce image processing algorithms that are trainable from data without requiring explicit programming.
This could be applied to the current gap between simulated prosthetic vision and perceptual limitations of phosphenes.
By training CNNs on task performance data rather than programming algorithms to produce explicit patterns of phosphenes, CNNs could learn to process images in phosphene-agnostic ways.
CNNs could also be tailored for individuals' perceptions of phosphenes, as the dependency of the algorithm shifts towards task performance tests rather than perceptual tests.
This essentially posiions the problem of deriving image processing algorithms from bottom-up (from phosphenes to algorithm) to top-down (from performance to algorithm).

*Generative adversarial networks* are a special type of training architecture for developing CNNs. cite:NIPS2014_5423
GANs train two networks - a /Generator/ and a /Discriminator/ - to perform opposing tasks - one to generate novel imagery, and one to determine whether the generated novel imagery is fake.
In this way, CNNs can be trained to produce new images when given samples of images, such as digits. cite:1511.06390,1611.01673,1411.1784
Importantly, these novel images are not copies of the provided image samples but represent entirely new samples which exhibit general features similar to the training data.
The technical implementation of GANs is beyond the scope of this review, but we describe our implementation in contrast with traditional GAN architectures in Section [[sec:main_methods]]

While GANs have been applied to image-based tasks in other domains, it is not clear how they should be applied to prosthetic vision.
An important difference between GANs in other domains versus prosthetic vision is that typically GANs directly manipulate every pixel in images they generate. cite:NIPS2014_5423
This gives GANs complete control over what its generated images look like.
However, this is not desirable in simulated prosthetic vision where we want to simulate visual experiences which we /cannot/ fully control.
GANs must instead be used to generate /instructions/ to simulated electrodes, which produce a simulated prosthetic vision render indepenendtly from the GAN.
A useful GAN implementation for simulated prosthetic vision should therefore /not/ be based on direct pixel manipulation (as is typically the case).

To our knowledge, GANs have not previously been applied to derive image processing algorithms for prosthetic vision, and thus research on this topic is scant.
While neural network architectures have been applied to prosthetic vision to construct new algorithms cite:mace_simulated_2015,ge_spiking_2017,sanchez-garcia_structural_2018, these have typically been object detection networks involved in segmentation-based processing.
There has also been growing interest in using machine learning recognition algorithms for transformative techniques in prosthetic vision. cite:chichilnisky_eduardo-jose_smart_2018
With continuous improvements in the capacity of hardware to support advanced processing cite:moore_cramming_1998, it is possible that emergent research in using machine learning to produce flexible, trainable algorithms could improve the utility of a cortical visual prosthesis.



\clearpage

** Aims and Hypothesis

*** Research Questions

1) Can GANs be used to train a neural network to produce phosphene patterns representing digits, given different simulated properties of prosthetic vision and, if so, how could this be implemented?
2) When applied to a digit representation taask, do people find it easier to recognise digits from phosphene patterns derived from training via GANs, compared to a basic comparison (directly masking digits with a phosphene grid)?

Given that the usefulness of an image processing method is highly subjective and task-dependent, we formulated the second research question to confine our assessment of usefulness to a simple single-digit recognition task.
Generating images of single digits is well-described in literature of traditional GANs, and the a digit recognition task is both focused and concrete.

*** Aims and Hypothesis

- Aim 1 :: Develop a preliminary software implementation of a GAN training architecture for generating phosphene patterns, which could be applied to different phosphene simulations with minimal modification. This was the primary aim.
- Aim 2 :: Experimentally test whether people find it easier to recognise digits from phosphene patterns derived from our prototype training implementation, compared to a basic mask-based comparison. This was the secondary aim.

Aim 2 is /not/ intended to provide conclusive or compelling evidence on the usefulness of GANs in general for simulated prosthetic vision; it is purely intended as a short-term checkpoint on the performance output of the implementation in Aim 1.
Given the novelty of the project, it is not expected that this project will produce a necessarily useful software implementation in Aim 1.
The preliminary validation results may instead be used to guide how to better refine the software implementation for future use.

Aim 2 relies on the implementation produced by Aim 1, and can be hypothesis-tested against the folowing hypothesis:

1) Sighted participants, under simulated conditions, have a higher overall digit recognition accuracy when viewing phosphene patterns derived from our prototype GAN training architecture, compared to when viewing phosphene patterns produced by basic image processing using masking.

* Methods
** Generative Adversarial Network Architecture
<<sec:main_methods>>

*** Development Context

The code for this project was written by the author using the Python programming language and accompanying package dependencies listed in Appendix [[sec:package_dependencies]].
All code written during the project is made available on a public GitHub repository listed in Appendix [[sec:appendix_code]]; several small code snippets have also been included.
All code was developed on a Windows 10 personal computer, and executed both on the development computer and remotely on the M3 multi-modal Australian ScienceS Imaging and Visualization Environment (MASSIVE) high-performance-computing (HPC) platform. cite:Goscinski2014

*** Architecture Overview

A high-level overview of the implemented GAN training architecture is provided in Figure [[fig:method_flowchart]].

#+NAME: fig:method_flowchart
#+CAPTION[Overview of GAN training architecture implementation]: Overview of our GAN training architecture implementation demonstrating the flow of data through the architecture. Real images of handwritten digits and "fake" images produced by a /Generator/ (red) are supplied to a /Discriminator/ and subsequently trained on its output. In this architecture, the /Generator/ and /Discriminator/ are dynamically updated with each image sample, whereas the /Simulator/ and /Modifier/ are static (indicated by locks).
#+begin_sidewaysfigure
[[file:images/methods_training_architecture.png]]
#+end_sidewaysfigure

The overarching goal of this architecture is to train the /Generator/ neural network (which is initialised to produce random noise before training) to generate simulation /encodings/ for any specified /Simulator/ which result in phosphene patterns that look like digits.

The flow of data through the training process proceeds as:
1) A /Simulator/ is first chosen for this training run.
   This can be likened to preparing a phosphene generator for a new implantee with unique phosphene perceptions.
   Once a /Simulator/ is chosen, it is used for the duration of training.
2) A real image sample of a digit is taken from a pool of real digits.
3) The image category is fed into the /Generator/, which produces a candidate encoding for the specified /Simulator/ for this training run.
4) The /Simulator/ takes the encoding and simulates it as phosphenes, producing a simulated prosthetic vision image.
5) The /Modifier/ takes the simulated prosthetic vision image and modifies it to more closely align it with the domain of the real images and reduce systematic differences between the simulated image and the real images.
6) The /Discriminator/ takes the real images and the modified simulated prosthetic images and produces predictions whether the digit is real (and what digit it is), or whether it is fake.
7) The predictions of the /Discriminator/ are used to adjust the properties of the /Generator/ and the /Discriminator/ to perform their tasks.
   The /Generator/ wants the /Discriminator/ to classify its generated images as real digits; it therefore compares the /Discriminator/'s actual predictions to a hypothetical case in which the /Discriminator/ predicted all the generated images were real, and uses the difference to rebalance its internal weights.
   The /Discriminator/ wants to correctly classify the real images as real, and the generated images as fake; it therefore compares its output with a hypothetical case in which its predications are all correct, and likewise uses the difference to rebalance its internal weights.

As with classical GAN architectures, the /Generator/ and /Discriminator/ participate in a zero-sum game where each network has opposing goals; the /Generator/ aiming to fool the /Discriminator/ into thinking the /Simulator/'s odified output is real, and the /Discriminator/ aiming to identity the /Generator/'s fakes amongst the real samples.
This architecture closely follows classical GAN architectures, except with the novel additions of the /Simulator/ and /Modifier/ steps.
These two steps differentiate this GAN architecture from traditional GANs, which usually have full pixel-access to the generated images; in this architecture, the GAN instead produces image-agnostic encodings that are supplied to the simulator.
The decoupling of the GAN from the generated image pixels allows the GAN to be applied to this simulated prosthetic vision task for different types of simulators, which can be readily substituted.

This basic process is repeated many times over many digit image samples to slowly optimise the /Generator/ to produce useful simulator encodings.
Each step of this process is discussed in further detail in the sections below.

*** Real Images and Image Categories

The /Generator/ begins training as a naive neural network with no conception of what digits are or what they look like.
In order for the architecture to train the /Generator/ to produce digit-like encodings, the /Discriminator/ must be provided samples of what real digits actually look like.
Importantly, these image samples are /not/ simply copied or masked by the /Generator/; instead, general features of the image samples are extracted (such as lines and circles) and used to guide generation.

This project used handwritten digit samples from the public modified National Institute of Standards and Technology (MNIST) dataset digit dataset. cite:Lecun1998
The MNIST digit dataset consists of 60,000 grayscale images stored as 2D pixel arrays with dimensions 27x27 pixels.

The MNIST digits are advantageous because:
1) It is a publicly-available, comprehensive, labelled, clean, and well-validated dataset (often the /de facto/ dataset for benchmarking machine-learning tasks).
2) Handwritten digits ensure there is sufficient variation within the dataset so that the /Generator/ and /Discriminator/ are incentivised to learn general features of digits (as opposed to rote-memorising whole digits, as might occur with using digits in a standard font).

In order for the /Discriminator/ to produce useful optimisations, there must be no obvious systematic difference between simulated images produced by the /Simulator/ and the real images.
If such systematic differences were present, the /Discriminator/ would have no difficulty discriminating real and generated images and the /Generator/'s output would therefore shift aimlessly as it tries to converge to an impossible goal.

A series of adjustments were therefore performed on the real images to mitigate systematic differences with images achievable by the /Simulator/.
In order to more faithfully simulate a single unilateral cortical prosthetic vision device (which only stimulates half the visual fields), we restricted simulations to a single half of the image (the right visual field).
We conducted the same adjustment to the MNIST digits by scaling each images' width by half and aligning the result with the right image edge.
Each MNIST image's pixel value was also normalised between -1 and 1 (from 0 to 255) and the entire image upscaled to a resolution of 48x48 pixels to equal the pixel resolution of simulated images.
Figure [[fig:mnist_sample]] shows a number of the original samples from the MNIST dataset, and the samples after this preprocessing step was applied.

#+NAME: fig:mnist_sample
#+CAPTION[A subset of MNIST real handwritten digit samples before and after preprocessing]: 16 randomly-selected digits from the MNIST handwritten digit dataset, before (A) and after (B) preprocessing (realignment and normalisation) was applied.
#+ATTR_LATEX: :height 0.7\textheight
[[file:./images/methods_mnist.png]]

The MNIST digit dataset includes a digit label for each handwritten data image in the range 0-9 inclusive.
These image categories are simply encoded as integers corresponding to their digit label, and fed directly to the /Generator/.

*** /Generator/

The /Generator/ is a neural network which takes a single digit in the range 0-9, and returns a simulator encoding to be fed into the /Simulator/.
The simulator encoding is a vector (list) of numbers in the range 0-1, with one number for each simulated electrode in the /Simulator/; each number specifies the strength with which its corresponding simulated electrode should be stimulated to produce a phosphene pattern.
Notably, there is no information encoded about what electrode actually /does/, nor where it is located; the /Generator/ network is effectively blind to these elements of the simulation.

The /Generator/ neural network is illustrated in Figure [[fig:methods_generator]] and consists of:
1. An input layer, containing a single neuron taking a digit from 0-9 as input.
2. An embedding layer, which maps the single categorical digit class into a 10-element continuous vector.
   The embedding layer is passed through a batch normalisation step to normalise mini-batches of image samples, and a leaky rectified linear unit (ReLU) activation function to dampen negative values and stabilise the layer.
3. A dense layer, which fully connects each neuron in the previous layer to output a vector of simulated electrode encodings.
   The dense layer is passed through a sigmoid activation function to constrain all outputs to lie between 0 and 1.

This network was implemented using the machine learning package TensorFlow cite:tensorflow2015-whitepaper; the code for this network is shown in Appendix [[sec:appendix_code]].

This network is simple with minimal hidden layers, which we justified as being sufficient for a digit-to-encoding task and suitable to reduce training times, given the time limitations of the project.
There is a important difference in the /Generator/ network described above compared to traditional /Generators/ in GANs: the lack of a random numerical seed input.
This was an intentional choice.
The use of a random input seed into /Generator/ networks in traditional GAN architectures permits the production of multi-modal images - i.e. multiple image patterns for a single image category (such as more than one pattern for a digit 0).
This is often desirable when we want GANs to produce many different novel images instead of just one; however, for use in simulated prosthetic vision, multi-modal output is a low priority.
It is actually desirable to limit the output to one "good" phosphene pattern per digit; in practice, subjects are likely to find it easier to learn one phosphene pattern per digit only, rather than having to learn multiple patterns.

#+NAME: fig:methods_generator
#+CAPTION[Schematic of the GAN Generator neural network]: Schematic of the GAN generator neural network; a digit class is supplied to an initial embedding layer, and fully connected to the output layer of neurons to produce the simulator encoding. bbb
#+ATTR_LATEX: :height 0.5\textheight
[[file:./images/generator.png]]

*** Simulator

The /Simulator/ is a custom simulation of prosthetic vision, which takes instructions for simulated electrodes as input, and outputs the simulated render as a grayscale image.
The /Simulator/ is a simulated stand-in for real implantees and their perception of vision through a CVP.

The /Simulator/ itself is a 3D volume of pre-rendered 2D grayscale image slices (illustrated in Figure [[fig:methods_simulator]]).
Each slice corresponds to the visual percept of a single electrode.
These pre-rendered grayscale slices allow each electrode to produce a distinct, unique percept; for example, one electrode in the /Simulator/ may produce a small phosphene located near the center of vision, while another may produce a much larger phosphene in the peripheries.
The /Simulator/ takes the simulator encodings from the /Generator/ and weights each electrode's slice in the 3D volume according to its corresponding encoded strength.
The slices are then aggregated together by summing along the last axis, and clipping values between 0 and 1 to produce a final grayscale render.
The final render is then rescaled between -1 and 1 before being supplied to the discriminator.

#+NAME: fig:methods_simulator
#+CAPTION[Illustration of the phosphene simulation process]: Prosthetic vision is simulated by weighting each pre-rendered slice of a 3D volume by provided simulated encodings, and aggregating the weighted slices.
#+begin_sidewaysfigure
[[file:./images/methods_simulator.png]]
#+end_sidewaysfigure

Because the /Simulator/ base is a 3D volume of pre-rendered slices, different subjects and their phosphene maps can be simulated by substituting different pre-rendered volumes without requiring any additional changes.
In addition, the use of pre-rendered slices makes simulation efficient, as rendering images consists merely of bulk matrix operations which are easy to parallelise on graphics processing units (GPUs).

In order to generate pre-rendered slices to test, we implemented two different types of phosphene maps:
1) *Cartesian* maps, which place phosphenes in a regular Cartesian plane and assigns the same size and strength to each phosphene.
   This was chosen as a basic control as this map approximates a normal (albeit low-resolution) pixel image.
2) *Polar* maps*, which place phosphenes in regular Polar grid and assigns each phosphene a size corresponding to its location, as given by a log-polar equation $log(k \times (x^2 + y^2) + a)$ for parametric constants $k$ and $a$ (see Appendic [[sec:appendix:polar_grid]] for the code implementation).
   This was chosen to more closely reflect the visual qualities of phosphenes discussed in Section [[sec:see]].

Both options constrained phosphenes in the right visual field.
Each of the two options above also had the option of creating randomly-positioned phosphenes to reflect the randomness of /in vivo/ phosphene maps.

The /Simulator/ can theoretically simulate any number of phosphenes; we focused on phosphene maps containing 64, 144 and 576 phosphenes only to represent low, medium and high resolution phosphene maps.
Examples of these phosphene maps are shown in Figure [[fig:methods_maps]].

#+NAME: fig:methods_maps
#+CAPTION[Phosphene maps used in the prosthetic vision simulation]: Phosphene maps used in the prosthetic vision simulation; regular and random Cartesian and Polar maps were modelled. Shown are four examples of maps with varying phosphene resolutions.
#+ATTR_LATEX: :height 0.4\textheight
[[file:./images/methods_maps.png]]

*** Modifier

In order for GANs to work effectively for the next step (feeding into the /Discriminator/), the generated simulations must have the capability of looking as close to the real images as possible.
Because the generated renders necessarily create simulated phosphenes, there would usually be no chance of the renders actually looking like digits unless the rendering resolution (i.e. number of phosphenes) is unrealistically high.
To resolve this problem, a /Modifier/ network can be added in order to bring the domain of the generated renders closer to the domain of real handwritten digit images to simulate basic "inference" that humans might make.
This amounts to simple "hole-filling" for most purposes - i.e. connecting spaces between phosphenes, as a human might be able to do, and outputting a new "inferred" image corresponding to a holistic outline and form.

#+NAME: fig:methods_modifier
#+CAPTION[Schematic of /Modifier/ architecture. ]: Schematic of /Modifier/ neural network architecture bring phosphene images closer to the domain of the real handwritten digits with simple inference.
#+begin_sidewaysfigure
[[file:./images/methods_modifier.png]]
#+end_sidewaysfigure

For this project, the /Modifier/ consisted of a simple style-transfer neural network which mapped between phosphene images and "filled" forms, illustrated in Figure [[fig:methods_modifier]] and listed in Appendix [[sec:modifier]].
Style-transfer neural networks are well-studied architectures, and the training architecture of the /Modifier/ closely followed a traditional style-transfer GAN architecture comprising:
1. An /input/ layer, taking the entire 2D grayscale render image (reshaped as a 3D slice with a depth of 1).
2. Multiple /2D convolutional/ layers (48, 96, 128 and 128 filters), each followed by a /batch normalization/ and a /leaky ReLU/ activation layer, which progressively downsample the image into extracted features.
3. Two /2D deconvolutional/ layers (48 and 1 filter/s), which progressively upsample the extracted features into a new modified image render.
   The last layer is passed through a $tanh$ activation function to remap the output between -1 and 1, as in the original render.

In order to train the /Modifier/, MNIST digits were passed under 4800 random polar grids of 64 phosphenes with a simple masking operation to generate pairs of phosphene images and corresponding real digit images.
These phosphene-real pairs were used to train the weights of the /Modifier/ network so it could map between novel phosphene image and real-like digit image pairs.
Once the /Modifier/ was trained, its weights were saved and frozen for subsequent use as a simple phoshene-to-forms image processor.
Examples of the style transfer output of the trained /Modifier/ is shown in Figure [[fig:methods_modifier_examples]].

#+NAME: fig:methods_modifier_examples
#+CAPTION[Examples of passing phosphene images under the /Modifier/ network]: Examples of passing phosphene images under the /Modifier/ network: simulated phosphenes are shown in A, and their corresponding modified renders in B.
#+begin_sidewaysfigure
[[file:./images/methods_modifier_examples.png]]
#+end_sidewaysfigure

*** Discriminator

The /Discriminator/ is the final major processing step of the GAN training architecture, and the major element involved in actually training the /Generator/.
The /Discriminator/ takes a modified rendered grayscale image, and outputs an 11-element vector of probabilities.
The first ten elements correspond to the probability the /Discriminator/ believes the input image is a 0, 1, 2 etc. until the digit 9.
The final element corresponds to the probability the /Discriminator/ believes that the input image is not a digit at all, and is instead fake.
The /Discriminator/ is therefore a classifier which classifies images as either belonging to one of 10 digit classifications, or as being fake instead.

The /Discriminator/ itself is a convolutional neural network with a relatively small number of layers, illustrated in Figure [[fig:methods_discriminator]] and implemented in Appendix [[sec:discriminator]]. It comprises:
1. An /input/ layer, taking the 2D grayscale modified render image (reshaped as a 3D slice with a depth of 1).
2. Two /2D convolutional/ layers (64 and 128 filters), each followed by a /leaky ReLU/ activation function and a 25% /dropout/ layer (randomly discarding 25% of prior inputs to improve the robustness of the network).
3. A /flatten/ and a /dense/ layer, to output an 11-element vector, which is then passed through a /softmax/ activation function to normalise all probabilities to sum to 1.

#+NAME: fig:methods_discriminator
#+CAPTION[Schematic of /Discriminator/ architecture]: Schematic of the /Discriminator/ convolutional neural network architecture which classifies images into probabilities of belong to each digit class, or being fake.
#+begin_sidewaysfigure
[[file:./images/methods_discriminator.png]]
#+end_sidewaysfigure

*** Loss Functions and Training

Once an image, generated or real, has passed through to the /Discriminator/ to output probabilities, these probabilities can then be used as the basis for adjusting the /Generator/ and /Discriminator/ to optimise them toward their respective goals.
This is the foundational basis of any GAN training architecture.

The /Generator/'s goal is to have the modified renders produced from its encodings look as indistinguishable from real digits as possible.
When its modified renders are passed through the /Discriminator/, it should therefore be optimised to produce a probability of 1 corresponding to the original /Generator/'s digit class input and a 0 for every other possibility.
The /loss/ of the /Generator/ is given by the categorical cross-entropy between a one-hot vector corresponding to the original image class input, and the /Discriminator/'s predicted output.

The /Discriminator/'s goal is to correctly classify real handwritten digits into their respective digit, as well as to correctly identify the modified renders produced by the /Generator/ as "garbage".
The /loss/ of the /Discriminator/ is therefore given by the sum of categorical cross-entropies between:
1. A one-hot vector corresponding to the real digit class of real images, and the predicted output of real images, and:
2. A one-hot vector corresponding to the fake class, and the predicted output of images from modifier renders produced by the /Generator/ and /Simulator/.

These two losses are backpropagated through the network for each image sample and the weights within the /Generator/ and /Discriminator/ networks are optimised according to their respective gradients over their loss function.
For this project, the optimiser for gradient descent used was the Adam optimiser cite:1412.6980 with a learning rate between of 0.001.

*** Performing Training and Qualitative Assessment

Training was performed in batches of 500 MNIST images, over all 60,000 images for up to 30 to 50 epochs (repetitions of processing all 60,000 images).
All training was performed on the M3 MASSIVE high-performance-computing service, and required approximately 12 minutes per epoch.
The weights of the /Generator/ and intermediate phosphene render results were saved after each epoch to permit early stopping and reuse of any epochs renders.
Overall, the training was conducted on over 100 different grids during development; 11 of which were randomly chosen for primary validation in the psychophysics experiment.
Qualitative results of training were evaluated to guide development at multiple stages during the development process.

The trained phosphene patterns were qualitatively assessed for:

1. Variation over training epochs
2. Variation over different phosphene map resolutions
3. Variation over different phosphene map arrangements (regular and random polar and cartesian maps)
4. Variation over different digits

** Psychophysics Experiment
*** Development

A psychophysics experiment was developed in order to perform preliminary validation of the GAN prototype discussed prior.
Ethics approval was obtained prior from the Monash University Human Research Ethics Committee to conduct prosthetic vision simulations in normal-sighted indiiduals for the development of a bionic eye (MUHREC Project Number 12525).
The psychophysics experiment was coded by the author using the Python programming language and the Psychopy psychophysics software package cite:Peirce2019, and is included in the repository listed in Appendix [[sec:appendix_code]].

*** Recruitment

11 participants (9 male, 2 female; ages 21-40) with normal or corrected-to-normal vision were recruited from students and staff at Monash University on a volunteer basis with a poster advertisement consistent with the ethics approval for this project.
Participants were briefed on the purpose and conduct of the experiment and signed a consent form for participation.

*** Setting

Psychophysics experiments were held in a single darkened room at Monash University for all participants.
The room contained a table, a chair, a computer, headphones, and a chin-rest mounted on the table.
Other items, including office stationery and books, were present in the room but were not significant for the experiment and not interacted with by participants.
During the experiment, the lights were switched off, the blinds were shut, the door closed and the participant instructed to silence their mobile device, leaving the computer monitor as the only major source of light.
The computer monitor's brightness and contrast were adjusted at the beginning of experimentation and were the same for all participants.
Participants were allowed to adjust the height of the chair, and vertical height of the chin-rest (but not the horizontal distance from the monitor) as comfortable.
All participants used the same pair of headphones, sanitised between each use, and were allowed to adjust the volume of the headphones as comfortable for their hearing level.

*** Conduct

Prior to the psychophysics experiment commencement, a random grid was allocated to each participant.
The experiment consisted of a single 60 to 90 minute session for each participant.
All sessions concluded after 90 minutes whether the experiment was complete or not.
At the start of the experiment, the participant was asked to sit on a chair in front of a computer screen.
The participant was then asked to rest their chin in a chin rest approximately an arms length away from the computer screen.
The participant was then given a pair of headphones to wear, and the volume was adjusted for the participant's comfort.
The experimental trial blocks, beginning with an example block, were then commenced.

#+NAME: fig:methods_psychophysics
#+CAPTION[Flowchart of psychophysics experiment.]: Flowchart of the conducted psychophysics experiment. Participants completed both a control block (A) and a test block (B) of digit classification trials with auditory feedback.
#+begin_sidewaysfigure
[[file:./images/method_psychophysics.png]]
#+end_sidewaysfigure


**** Trials and Trial Blocks

The experiment consisted of a basic digit classification task, divided into *cues* (single phosphene pattern to classify), *trials* (streams of consecutive digits to classify) and *trial blocks* (consecutive digits to clasify with rest in between).

These are defined as:
- Trial block ::  an uninterrupted set of classification trials, interspersed with grey screens which participants were instructed they could use to take a short pause.
- Trial ::  an uninterrupted set of phosphene-represented digit /cues/ which the participant was asked to identify.
- Cue ::  a single black screen with a phosphene pattern located in the central quarter of the screen.

A *condition* for this experiment consisted of either the control condition (a brightness-based mask over phosphenes, where the phosphene map is placed directly over the digits and projected through the map) or the test condition (the trained /Generator/ for the grid allocated to the participant).
The digit images used as the underlying basis for the controls were white Arial sans-serif digits on black backgrounds, aligned to the right border of the image as with the MNIST digits; these are included in the code repository listed in Appendix [[sec:appendix_code]].

Each experimental trial block consisted of 12 to 20 trials; each trial consisted of 20 to 25 phosphene representations of digits which a participant was asked to classify, and participants performed either 2 or 4 blocks depending on time constraints; exact numbers of trials are listed in Section [[sec:trials]].
At the start of a trial, the participant would be shown a grey progress screen, instructing them to press any key to continue.
At a keypress, the participant would be shown the first cue, and the program would await a digit keypress (0-9 on the number pad) from the participant.
When the participant pressed a digit, they would immediately hear audio feedback simultaneously playing a tone indicating if they were correct (high) or incorrect (low), and an English voice telling them what the true digit was.
The next cue would then immediately be shown, and the program would await the next digit keypress.
This would repeat 20-25 times to conclude one trial.
At the conclusion of a trial, another grey progress screen would be shown, and the trial process would again repeat.
At the conclusion of a trial block, the participant would be given a short break before the next trial block was begun.
The condition participants started with was balanced between both conditions for both scenarios above; participants performing 4 blocks performed the two conditions interleaved.

Prior to the commencement of the trial blocks, the participant was given an example trial block to familiarise themself with the procedure of the experiment.
The example trial block consisted of an identical format to the trial blocks above, but contained only 2 trials of 10 digits each, for 1 single block.

*** Analysis

Results of the psychophysics experiment were first analysed with regards to the mean overall accuracy of digit recognition for each condition, both pooled across participants and within each participant.

To statistically address the second aim of this project and its corresponding hypothesis and to also incorporate learning effects over time, the classification data was modelled using a logistic regression model, including the effects of:
- Condition, the primary variable of interest.
- Cue, trial and trial block and their corresponding interaction effects, which we expected to reflect participant's ability to learn patterns over repeated trials.
- Digit class, which may reflect intrinsic differences between digits' abilities to be represented in arbitrary grids.
- Participant, which was expected to reflect inter-individual variation.

Effects with a p-value below 0.05 were considered significant.

In addition, we briefly analysed:
- Differenced in duration spent classifying digits, a possible confounder, and
- Learning effects by condition, to see if the test versus control condition affected learning.

* Results
** Training Results

We have selected a representative sample of different simulated grid properties to highlight several general features of the training results.
The output here is appraised qualitatively for a broad range of grids, with statistical validation done for a small samples of low-resolution grids in Section [[sec:experiment_results]].
The qualitative output of all training runs is included in the code repository in Appendix [[sec:appendix_code]].

*** Variation over Epochs

Phosphene patterns initially improved with epochs, but drifted away from recognisable patterns as the number of epochs increased further.
This effect permeated all runs of the training architecture for every grid, and the epoch at which digits appeared "best" varied considerably.
Figure [[fig:epochs_576]] demonstrates the evolution of the generated phosphene pattern for the digit 4 for a regular cartesian grid of 576 phosphenes.
Although the phosphene pattern approaches several different depictions of a 4 at epochs 6, epoch 20 and epoch 39, it does not stabilise at a particular point but continues to attempt to reconfigure itself.

#+NAME: fig:epochs_576
#+CAPTION: Phosphene patterns generated for the digit 4 for a regular cartesian grid of 576 phosphenes. The digit form approaches a recognisable digit 4 until approximately epoch 20, before the subsequently drifting away.
#+ATTR_LATEX: :height \textheight
[[file:./images/results_over_epochs_576.png]]

*** Variation over Resolution

In general, phosphene forms more closely resembled digits at higher phosphene resolutions.
At the lowest resolution tested (64 phosphenes), phosphenes ceased to resemble digits even for the most simple grid, the regular Cartesian grid.
The phosphene patterns generated for a regular Cartesian grid of 64, 144 and 576 phosphenes at 2, 4 and 8 epochs for the digit 0 is shown in Figure [[fig:general_resolution]].

#+NAME: fig:general_resolution
#+CAPTION: Phosphene patterns generated for different phosphene resolutions of a regular Cartesian grid for the digit 0. The first column shows the phosphene locations and relative sizes; the second, third and fourth columns show the simulated patterns at the 2nd, 4th and 8th epoch respectively.
[[file:./images/results_different_resolution.png]]

*** Variation over Grid Arrangements

Phosphene patterns were successfully generated for both cartesian and polar grids with regularly and randomly spaced phoshenes.
However, the appearance of these phosphene patterns were often non-optimal at high-resolutions, and non-recognisable at low-resolutions.
Figure [[fig:grids_576]] shows the generated phosphene patterns for a grid of 576 phosphenes at 10, 20 ad 30 epochs for the digit 6.
Note that the polar grids have considerably larger phosphenes in general (particularly at the peripheries), leading to greater difficulty for the training architecture in producing /any/ recognisable forms.

#+NAME: fig:grids_576
#+CAPTION: Phosphene patterns generated for the digit 6 for 4 different grid arrangements with 576 phosphenes at 10, 20 and 30 epochs with the base phosphene map on the left for comparison.
[[file:./images/results_different_grids_576.png]]

*** Variation over Digits

The ability to generate phosphene patterns resembling digits varied dramatically for different resolutions and arrangements of grids.
Figure [[fig:digits_576]] shows the generated phosphene patterns at the 16th epoch for a regular cartesian grid of 576 phosphenes, the highest resolution and simplest arrangement tested.
Phosphene patterns at this stage were recognisable, though there are notable imperfections such as interrupted lines.

#+NAME: fig:digits_576
#+CAPTION: Phosphene patterns generated for each digit 0-9 for a regular cartesian grid of 576 phosphenes at the 16th epoch.
[[file:./images/results_over_digits_576.png]]

** Experimental Results
<<sec:experiment_results>>

*** Grid Selection

While the most recognisable phosphene patterns were produced using regular cartesian grids at higher resolution of 576 phosphenes, these simulations are not the primary area of interest for simulated prosthetic vision where grids are irregular and expected to be much lower resolution.
We therefore selected the lowest resolution grids (64 phosphenes) with randomly-arranged polar phosphenes which best reflected the current reality of CVP capabilities.
Due to time constraints, we were not able to refine the GAN architecture further and often the digit patterns produced at these low resolutions did not look like digits.
However, we proceeded with the experiment given that we still wished to determine whether participants found these generate patterns easier to discern than a simple control.

*** Participant Trial Characteristics
<<sec:trials>>

All participants provided at least one complete block each of digit recognition data for both the control processor and the trained encoder for their grid.
The first four participants provided two trial blocks of data with each block containing 20 trials of 25 digit cues.
After the first four participants, we shifted the number of digits per block into 20 trials of 20 digit cues and added two additional blocks, as we noticed a significant learning effect during intermediate analysis and wished to take extra measures to interleave the control and test blocks.
The first participant to try this arrangement was unable to complete the four blocks in the allotted time, so the number of trials per block was reduced to 12.
One additional participant after this change were unable to complete the two extra blocks in the allotted time, so only the data from the first two blocks was used as with the first five participants.

The characteristics of the trials for each participant are summarised in Table [[tab:participant_characteristics]].

#+NAME: tab:participant_characteristics
#+CAPTION: Characteristics of trials for each participant. Control indicates the direct masking processor, and Test indicates the trained phosphene patterns using the GAN architecture.
#+ATTR_LATEX: :environment tabu :width \textwidth :align XlXXXXX :font \footnotesize
| Participant | Trials x Cues per Block | Block 1 | Block 2 | Block 3 | Block 4 | Total Cues |
|-------------+-------------------------+---------+---------+---------+---------+------------|
|          01 | 20 trials x 25 cues     | Control | Test    |         |         |        500 |
|          02 | 20 trials x 25 cues     | Test    | Control |         |         |        500 |
|          03 | 20 trials x 25 cues     | Control | Test    |         |         |        500 |
|          04 | 20 trials x 25 cues     | Test    | Control |         |         |        500 |
|          05 | 20 trials x 20 cues     | Control | Test    |         |         |        400 |
|          06 | 12 trials x 20 cues     | Test    | Control | Test    | Control |        480 |
|          07 | 12 trials x 20 cues     | Control | Test    | Control | Test    |        480 |
|          08 | 12 trials x 20 cues     | Test    | Control |         |         |        240 |
|          09 | 12 trials x 20 cues     | Control | Test    | Control | Test    |        480 |
|          10 | 12 trials x 20 cues     | Test    | Control | Test    | Control |        480 |
|          11 | 12 trials x 20 cues     | Control | Test    | Control | Test    |        480 |

*** Overall Accuracy
**** Pooled

The mean digit recognition accuracy of the test group was higher on average (75.7%, standard error of mean 3.75%) than the mean accuracy of the control group (59.8%, standard error of mean 4.17%).
Figure [[fig:results_overall]] compares the mean digit recognition accuracy for each condition over the pooled participant data.

#+NAME: fig:results_overall
#+CAPTION[Mean digit recognition accuracy by condition, pooled participant data]: Mean digit recognition accuracy by condition, pooled participant data. Error bars show standard error of means.
[[file:./images/results_mean_accuracy_overall.png]]

**** By Participant

Of 11 participants, 9 participants performed better with the trained processor than the control processor despite variations between participants of baseline performance.
Figure [[fig:results_participant]] compares the mean digit recognition accuracy for each condition, stratified by participant.

#+NAME: fig:results_participant
#+CAPTION: Mean digit recognition accuracy by participant and condition.
[[file:./images/results_mean_accuracy_by_participant.png]]

*** Statistical Effects

The correctness of participant's digit classification was modelled using logistic regression. These results are summarised in Table [[tab:statistical_effects]].
The test condition significantly improved classification accuracy in this linear regression model (p < 0.001); trial (p < 0.001) and block (p < 0.001) and its interaction effect (p < 0.05) also significantly improved performance.
There was significant inter-individual variation in baseline performance (p < 0.001).
Several digits were significantly associated with higher classification accuracy including digits 1, 4 and 7 (p < 0.001).

#+NAME: tab:statistical_effects
#+CAPTION: Predictors of a correct digit classification modelled as a logistic regression. P-values below 0.05, 0.01 and 0.001 are marked with (=*=), (=**=) and (=***=) respectively.
#+ATTR_LATEX: :environment tabu :width \textwidth :align XXXl :font \scriptsize
|---------------------+-------------+---------+--------------|
| Predictor           | Coefficient | P-Value | Significance |
|---------------------+-------------+---------+--------------|
| condition           |    0.959624 |  <1e-77 | =***=        |
|---------------------+-------------+---------+--------------|
| cue                 |   0.0184719 |  0.2237 |              |
| trial               |    0.176255 |  <1e-12 | =***=        |
| block               |    0.758783 |  <1e-17 | =***=        |
| cue & trial         |  0.00174088 |  0.3815 |              |
| cue & block         |  0.00346854 |  0.6395 |              |
| trial & block       |   0.0140752 |  0.2867 |              |
| cue & trial & block | -0.00226474 |  0.0400 | =*=          |
|---------------------+-------------+---------+--------------|
| participant: P02    |   -0.746689 |   <1e-9 | =***=        |
| participant: P03    |   -0.391573 |  0.0016 | =**=         |
| participant: P04    |    -1.15042 |  <1e-21 | =***=        |
| participant: P05    |     -1.1289 |  <1e-18 | =***=        |
| participant: P06    |   -0.682311 |   <1e-7 | =***=        |
| participant: P07    |   -0.914266 |  <1e-12 | =***=        |
| participant: P08    |   -0.632393 |   <1e-5 | =***=        |
| participant: P09    |  -0.0339482 |  0.7966 |              |
| participant: P10    |    -1.32654 |  <1e-26 | =***=        |
| participant: P11    |    -2.32435 |  <1e-74 | =***=        |
|---------------------+-------------+---------+--------------|
| digit: 1            |      1.7064 |  <1e-40 | =***=        |
| digit: 2            |    0.233331 |  0.0300 | =*=          |
| digit: 3            |    0.314268 |  0.0035 | =*=          |
| digit: 4            |    0.645442 |   <1e-8 | =***=        |
| digit: 5            |    0.105155 |  0.3319 |              |
| digit: 6            |  -0.0685792 |  0.5264 |              |
| digit: 7            |     1.02026 |  <1e-18 | =***=        |
| digit: 8            |    0.275374 |  0.0120 | =*=          |
| digit: 9            |    0.065778 |  0.5451 |              |
|---------------------+-------------+---------+--------------|
| (Intercept)         |    -2.08177 |  <1e-20 | =***=        |
|---------------------+-------------+---------+--------------|

Figure [[fig:statistical_effects]] plots the same data on a scatter plot.

#+NAME: fig:statistical_effects
#+CAPTION: Predictors of a correct response in a logistic regression model. Error bars indiate 95% confidence intervals.
[[file:./images/statistical_effects.png]]

*** Response Time

The mean response time for a single digit was 2.6 (SEM 0.32 seconds) seconds for the trained processor, versus 3.3 seconds for the trained processor.
Figure [[fig:response_time_by_participant]] shows the response time per cue for each participant.

#+NAME: fig:response_time_by_participant
#+CAPTION: Response time per cue by participant. Error bars show standard deviation.
[[file:./images/response_time_by_participant.png]]

*** Learning Effects

Figure [[fig:learning_curves]] shows the a rolling average with a windows size of 50 over pooled trial blocks for each condition and participant.
Participants performed with similar accuracies at the start of trial blocks, and both condition displayed a prominent learning effect.

#+NAME: fig:learning_curves
#+CAPTION: Rolling average of accuracy over pooled trial blocks by condition and participant.
[[file:./images/learning_curves.png]]

* Discussion
** Aims

The GAN training architecture described in Section [[sec:main_methods]]  was tested on a large variety of phosphene grids varying in phosphene location, sizing, regularity and resolution.
We were successful in applying the training process to different types of phosphene grids, which varied primarily in spatial distribution, phosphene size and phosphene resolution.

A prototype GAN training architecture was successfully developed and was capable of producing phosphene encodings for different simulated grid properties.
We have approached this by implementing independent Rendering and Modification steps inside the GAN training architecture, and by allowing the Renderer be supplied different simulated phosphenes for each electrode.
In this respect, the implementation presented here serves as a proof-of-concept that a generalisable training framework is indeed possible.

Unfortunately, the output of this GAN training architecture does not always resemble their intended forms (digits).
Because this difficulty was present even in a regular Cartesian grid (which approximates pixels at higher resolutions), this is likely due to more than just intrinsic difficulties in representing digits in a grid.
Clearly, there is still a significant portion of work required before this training architecture can be considered usable for its intended purpose.

The biggest issue confronting the GAN training architecture was stability; phosphene forms did not converge over epochs, as demonstrated by Figure 8.1, and indeed drifted.
While we made our best effort to align the domains of generated images with real images by using a modifier and preprocessing techniques, this was clearly not enough.
It is not clear whether this limitation will severely impact on the ability of GANs to be applied to simulated prosthetic vision, as there may be other methods of combatting issue; however we were not able to do so.

At low resolutions, is was also clear that phosphene patterns were nigh unrepresentable on some grids, due to the inherent distribution of phosphenes on the grid themselves.
This may be telling that GANs are not suitable to the problem of low-resolution phosphene grids, but may still be useful to tailor phosphene patterns for higher-resolution (but distorted) grids.

Figure 8.3 display roughly equivalent forms over the different types of phosphene grids; this is encouraging, though we note that these did not exactly resemble digits.
This effect was notable for all qualitative results - while forms looked similar across grids, these appeared to exhibit merely general features of digits (such as the circle of a six with a vertical line) rather than holistically combining into digits themselves.
This is a known problem with GANs cite:NIPS2014_5423, which may become too focused on minutiae and lose larger forms.
Figure 8.4 demonstrated that some digits were additionally much harder to represent (such as an 8), even in a simple Cartesian grid.
This is likely due to the fineness detail of the training data after preprocessing, and could possibly be mitigated by further thickening lines in the training data samples.

Due to the time limitations of this project, we proceeded with the psychophysics evaluation despite not achieving particularly "good-looking" digits.
Although the phosphene forms did not look like digits, it was still possible that people might find them easier to learn than the control.

Participants achieved a greater overall digit classification accuracy with the trained processer versus the brightness-based control.
This effect was statistically significant when analysed using a multiple logistic regression, supporting the hypothesis that the trained processor confers better digit classification accuracy than the brightness-based control.

Notably, participant performance at the beginning of each trial block was similar between the trained and control processor.
This suggests that the trained processor phosphene forms looked no more like digits than the control processor.

It therefore appears that the better participant performance was due to a better ability to learn and distinguish the phosphene forms, rather than the the ability to find the phosphene forms immediately recognisable.
We did not statistically assess the learning effect of condition as the qualitative results displayed in Figure 9.5 were sufficient information for guiding its use as a preliminary validation measure.
We emphasise that the primary contribution of this project lies in Aim 1 - the development phase - with the statistical results merely hoping to guide further development in the future.


** Implications

To our knowledge, this is the first attempt at tailoring phosphene patterns to specific grids through an automated process.
Although previous research has demonstrated how image processing techniques can be applied to simulated prosthetic vision, these techniques are often coupled with the simulations they run in.
This makes it hard to generalise how such techniques can be applied when the simulations strongly differ from reality.
This project, by using a dynamic simulator capable of representing arbitrary arrangements and properties of phosphenes, is therefore novel in investigating an approach to image processing decoupled from the simulation itself.

The approach we have taken in this project is an early implementation using GANs.
We have demonstrated that GANs are capable of producing digit-like forms with "high" resolution cartesian grids, though with imperfections.
This closely reflects the domain in which GANs are known to excel; where GANs have close to pixel-level access on generated images, and can optimise at a pixel-wise level.

However, we were not successful in producing reasonable digit-like forms at "low" resolution grids.
The GAN training architecture was able to produce a phosphene pattern output, but these did not resemble digits.
This is outside the domain for which GANs have previously been applied - when there is greater distance between the output of generated encodings and the simulated render.
Because GANs have not been applied in this area before, it was unknown how they would cope in this type of task.
This project therefore provides some information on the potential issues with using GANs for an encoding task with great distance between generator and renderer, as well as a basis for suggestions addressing these issues.

A prominent issue arising from this project is how to evaluate how intrinsically capable different grids are for representing recognisable digits.
It is not altogether clear whether the issues with representing digits arose due to limitations of the GAN architecture, or the impossibility of representing digits for some types of grids in the first place.
A contrived example is where a grid is composed of phosphenes arranged entirely in a single vertical line; it would be impossible to make any recognisable digit from this grid except for a digit 1.
This is especially a problem with low-resolution grids with other distortions (such as size variance), where the contribution and valuability of each phosphene is relatively high.
This problem - the lack of a means of evaluating the informative capacity of different phosphene grids - has not previously been addressed.

Nevertheless, we have demonstrated that computer-generated phosphene forms may still be useful to improve the discriminability of phosphene patterns, even though they do not look like digits.
The results of our psychophysics experiment, which we undertook at low-resolution, irregular and distorted grids, demonstrate that although people did not recognise the forms as digits, they were able to learn their meanings better than a direct mask-based control.
There is no doubt that manually-crafted phosphene patterns would have easily surpassed the performance of our GAN implementation as it currently stands.
However, the usefulness of this GAN implementation is that it is an automated process, and not exclusive to digit forms (provided there is training data of other forms which you wish to represent).
There is therefore an advantage to refining automated processes of generating phosphene forms rather than relying on the ability of humans to manually craft phosphene patterns representing digits.

The current results of this project are not compelling for, nor against, the use of GANs in addressing the perceptual limitations of prosthetic vision.
We have demonstrated how a GAN can be implemented to perform a phosphene generation task for different simulated grids, and how this performance varies with different simulated properties.
The GAN was not robust against all different simulated properties.

** Limitations

*** Simulated Prosthetic Vision

One of the criticisms highlighted in the Background section was the difficulty of dealing with differences between simulated prosthetic vision and prosthetic vision /in vivo/.
While this project addresses one of these differences - coping with different /spatial/ phosphene properties - many differences remain for which the translation of techniques from simulated space to /in vivo/ space is unclear.
These include:
1. *Temporal qualities of phosphenes, such as flickering and dimming over time*.
   This is important in understanding how vision can be provided to implantees continuously like natural vision, and how temporal properties could be used to improve implantees' visual understanding.
   Our training implementation does not currently allow for producing or testing anything more than static patterns.
   Notably however, static patterns are not necessarily a poor choice since continuously-changing phosphene patterns might provoke confusion in implantees, particularly in low-resolution implants as is currently expected.
2. *Effects of simultaneously eliciting phosphenes.*
   In this experiment, simultaneously stimulated phosphenes were shown simultaneously for simplicity; however, empirical evidence suggests that simultaneously elicited phosphenes may not combine additively.
   This aspect of the simulation could theoretically be incorporated in our GAN training architecture, but was not trialled in this experiment.
3. *The movement of phosphenes with eye movements.*
   Phosphenes are known to move with eye movements, meaning that peripheral phosphenes would always be perceived peripherally.
   Our psychophysics experiment did not incorporate eye movement tracking as it was intended purely as a preliminary validation measure rather than a true test of applicability.
   Participants were therefore able to scan and study phosphene patterns, which may not be possible /in vivo/.
   This feature could, however, be modelled in a future experiment with greater resources and refinement of phosphene patterns as a stronger test of applicability.

The results presented here should therefore still be considered within the context of simulated prosthetic vision and all its caveats, as it primarily focused on addressing one particular issue of simulated prosthetic vision rather than all.

*** Experimental Features

The sample size of the psychophysics experiment used in this project was notably small with 11 participants only.
The rationale for this sample size was that it served as an intermediate checkpoint to point-out weaknesses in the current architecture and guide further development.
The investment required for recruiting and conducting the experiment on more participants was not justified by the status of the GAN implementation as an early prototype.
However, should the prototype be further developed, it is important to validate in a greater number of participants.

*** Choice of Task

The digit recognition task in this experiment was chosen to optimise data collection so as many data samples could be collected as possible from each participant.
The task - recognising a single digit known to belong to the class of 0-9 - is therefore extremely simple and somewhat contrived.

The use of a simple, constrained task was necessary for this project as it investigated a single feature - the discriminability of phosphene patterns.
At the current stage of implementation, it is not as informative for development to use "natural" tasks - such as navigation, reading or unconstrained object identification - which often incorporate multiple aspects of visual information as well as other senses.
However, the reality is that natural visual tasks are of greatest interest to future implantees.
Whilst in early development, the use of simple, constrained tasks is crucial to directly highlight strengths and weaknesses of image processing techniques, these must eventually be tested in more natural environments.
This project is not yet at that stage.

** Future Directions

Given that this is a prototypical implementation, it is important that the results of this work can be used to improve research on the use of GANs in simulated prosthetic vision even though the early results are not conclusive.

*** Training Data

We used MNIST digits as the source of real digits for our training implementation and modified them slightly to suit the visual properties of our simulation.
We also attempted to bring the renders closer to the domain of MNIST digits using a style-transfer network to "fill holes" in the phosphene and extract forms.

However it does not appear our measures were enough for the network to remain stable.
It was evident from the training loss curves that the /Generator/ was never able to fool the /Discriminator/ reliably.
As a result, the /Generator/'s produced encodings often drifted rather than converging to an optimal solution.

An easy improvement to make (and one which does not require changing the actual training architecture itself) is to use digit forms closer to the image domain of simulated renders as training data.
This could be sourced by creating

In addition, training data representing other forms could be used instead, such as outlines of common objects (such as chairs) or letterforms.
Using different forms of training data does not require any major changes to the architecture, as the training data is not part of the architecture itself (and therefore can simply be "mixed and matched").
A potential source of outline data is the Google Quick, Draw! Dataset, which contains millions of simple, line-based drawings of objects and animals.
As the ability to substitute different forms of training data is one of the benefits of using GANs, this could potentially provide further information on whether GANs are justified for simulated prosthetic vision.

*** Grid Evaluation

This project faced major difficulties in creating recognisable phosphene patterns for low-resolution distorted grids.
Part of this difficulty is due to intrinsic limitations on /what/ can be represented on grids with a small quantity of phosphenes.
Certain types of grid, such as a single linear "grid" of phosphenes, clearly constrain the types of shapes that can be represented.

It would be informative and time-saving to be able to evaluate grids beforehand to quantify their usefulness for representing specific types of phosphene patterns.
This could be as simple as a measure of dispersion of phosphenes in 2D space for simple grid simulations.
There is scant literature on the use of image processing techniques for grid resolutions as low as those tested in this project (64 phosphenes), which may partially be why this has not previously been an issue.
However, given that first-generation CVPs are only designed to produce up to a maximum of 473 phosphenes (with most implants far below this number), being able to quantify phosphene grids' representability would be useful for early /in vivo/ experiments.

** Conclusion

This project has explored how generative adversarial networks (GANs) may be used to generate phosphene patterns for arbitrary simulated properties of prosthetic vision, and to our knowledge is the first attempt at addressing phosphene pattern generation decoupled from simulation properties. 
We developed a prototypical software implementation of a GAN training architecture to generate phosphene patterns of digits, and tested whether these helped people recognise digits in a psychophysics experiment compared to a mask-based control.
The early training implementation we developed was successfully able to produce phosphene digit patterns for different grid properties including phosphene distribution, sizing and resolution.
However, the produced phosphene digit patterns were only recognisable as digits at high grid resolutions and not at low grid resolutions, and the generated patterns proved unstable over epochs.
Nevertheless, at low resolution, participants were still able to learn the generated phosphene patterns better than the mask-based control and achieved a higher recognition accuracy overall.
The results of this checkpoint suggest that the current GAN implementation may be helpful in increasing the discriminability of phosphene patterns for a simple classification task, but are not yet robust against low-resolution grids and spatial distortions.
This project provides useful information on potential targets for improving a GAN implementation; further work should focus on improving the stability of GAN training and quantifying intrinsic capabilities of low-resolution grids to represent form-based information.

#+LATEX: \begin{appendices}
* Appendices
** Selected Package Dependencies
<<sec:package_dependencies>>

A selected number of direct dependencies of the code implementation of this project and their version numbers are listed below.
A full listing of dependent packages, including indirect dependencies, is available from the file =requirements.txt= in the linked GitHub repository.

#+LATEX: \linespread{1.1}
#+LATEX: \thispagestyle{empty}
#+NAME: tab:dependencies
#+CAPTION[Selected package dependencies for the project.]: Selected package dependencies for the code written in this project.
#+ATTR_LATEX: :environment tabu :align Xl :font \small
| *Package Dependency* |    *Version Number*  |
|----------------------+----------------------|
| h5py                 |                2.9.0 |
| imageio              |                2.5.0 |
| imageio-ffmpeg       |                0.3.0 |
| Keras-Applications   |                1.0.8 |
| Keras-Preprocessing  |                1.1.0 |
| matplotlib           |                3.1.0 |
| numpy                |               1.16.4 |
| numpydoc             |                0.9.1 |
| opencv-python        |             4.1.0.25 |
| pandas               |               0.25.1 |
| Pillow               |                6.1.0 |
| PsychoPy             |                3.1.5 |
| pygame               |                1.9.6 |
| pyglet               |                1.3.0 |
| Pygments             |                2.4.2 |
| pypiwin32            |                  223 |
| PyQt5                |                  5.9 |
| PyQt5-sip            |              4.19.18 |
| pywin32              |                  224 |
| pyWinhook            |                1.6.1 |
| pywinpty             |                0.5.5 |
| QtAwesome            |                0.5.7 |
| qtconsole            |                4.5.1 |
| QtPy                 |                1.8.0 |
| scikit-image         |               0.15.0 |
| scikit-learn         |               0.20.3 |
| scipy                |                1.3.1 |
| tb-nightly           |      1.15.0a20190806 |
| tblib                |                1.4.0 |
| tensorflow           |             2.0.0rc0 |
| tf-estimator-nightly | 1.14.0.dev2019080601 |
| wxPython             |                4.0.6 |

** Selected Code
<<sec:appendix_code>>

The entire codebase is available from [[https://github.com/wjmn/bmedsc-exploratory]].

This codebase includes all code written during the course of this project, including:
- Prototypes
- Exploratory data analysis
- The generative adversarial network architecture
- Psychophysics experiments
- Data analysis
- Plotting code

As the codebase is too large to include in hardcopy, only a few small, selected snippets are reproduced here where relevant.

*** Models

Each of the below shows the code implementation of each of the neural networks involved in the GAN training architecture.

**** Generator

#+LATEX: \begin{small}
#+begin_src python :eval no
from tensorflow.keras import Sequential, Model
from tensorflow.keras.layers import *

# For a phosphene map with 64 phosphenes
NUM_PHOSPHENES = 64

def generator_basic():
    input_labels = Input(shape=(1,))
    layer_labels = Embedding(10, 10)(input_labels)
    layer_labels = Flatten()(layer_labels)
    layer_labels = BatchNormalization()(layer_labels)
    layer_labels = LeakyReLU()(layer_labels)
    layer_labels = Dense(NUM_PHOSPHENES, activation=tf.nn.sigmoid)(layer_labels)
    return Model(input_labels, layer_labels)
#+end_src
#+LATEX: \end{small}

**** Modifier
<<sec:modifier>>

#+LATEX: \begin{footnotesize}
#+begin_src python :eval no
from tensorflow.keras import Sequential, Model
from tensorflow.keras.layers import *

def modifier():
    in_image = Input(shape=(renderSize1, renderSize2))

    l_image  = Reshape((renderSize1, renderSize2, 1))(in_image)
    l_image  = Conv2D(48, (7,7), strides=(1,1), padding='same')(l_image)
    l_image  = BatchNormalization()(l_image)
    l_image  = LeakyReLU()(l_image)

    l_image  = Conv2D(96, (3,3), strides=(2,2), padding='same')(l_image)
    l_image  = BatchNormalization()(l_image)
    l_image  = LeakyReLU()(l_image)

    l_image  = Conv2D(128, (3,3), strides=(1,1), padding='same')(l_image)
    l_image  = BatchNormalization()(l_image)
    l_image  = LeakyReLU()(l_image)

    l_image  = Conv2D(128, (3,3), strides=(1,1), padding='same')(l_image)
    l_image  = BatchNormalization()(l_image)
    l_image  = LeakyReLU()(l_image)

    l_image  = Conv2DTranspose(48, (3,3), strides=(2,2), padding='same')(l_image)
    l_image  = BatchNormalization()(l_image)
    l_image  = LeakyReLU()(l_image)

    l_image  = Conv2DTranspose(1, (3,3), strides=(1,1), padding='same', activation="tanh")(l_image)
    l_image  = Reshape((renderSize1, renderSize2))(l_image)

    model    = Model([in_image], l_image)
   
    return model
#+end_src
#+LATEX: \end{footnotesize}

**** Discriminator
<<sec:discriminator>>

#+LATEX: \begin{small}
#+begin_src python :noeval:
from tensorflow.keras import Sequential, Model
from tensorflow.keras.layers import *

X_RENDER = 48
Y_RENDER = 48

def discriminator_basic():
    input = Input(shape=(Y_RENDER, X_RENDER))
    layer = Reshape((Y_RENDER, X_RENDER, 1))(input)
    layer = Conv2D(64, (8,8), padding='same', strides=(2,2))(layer)
    layer = LeakyReLU()(layer)
    layer = Dropout(0.25)(layer)
    layer = Conv2D(128, (4,4), padding='same', strides=(2,2))(layer)
    layer = LeakyReLU()(layer)
    layer = Dropout(0.25)(layer)
    layer = Flatten()(layer)
    layer = Dense(11, activation=tf.nn.softmax)(layer)
    return Model(inputs=input, outputs=layer)

#+end_src
#+LATEX: \end{small}

*** Grids

Each of the below shows the code implementation of the Cartesian and Polar phosphene map generators; these inherit from an Abstract base class implementation included in the GitHub repository as it is too long to reproduce here..
**** Cartesian Grid

#+LATEX: \begin{footnotesize}
#+begin_src python :eval no
class CartesianGrid(AbstractGrid):

    def __init__(
            self,
            x_phosphenes: int,
            y_phosphenes: int,
            x_render: int,
            y_render: int,
            half: bool = False,
            random: bool = False
    ) -> None:
        """ Create a cartesian grid of phosphenes (regularly sized).

        :param x_phosphenes: the number of phosphenes in the x dimension.
        :param y_phosphenes: the number of phosphenes in the y dimension.
        :param x_render: the number of pixels in the x dimension of the render.
        :param y_render: the number of pixels in the y dimension of the render.
        :param half: whether to only fill the right-half of the render (default False).
        :param random: Whether to randomise the locations of phosphenes (default False).
        """

        self._set_id()
        self.grid_type = GridType.CARTESIAN
        self.num_phosphenes = x_phosphenes * y_phosphenes
        self.render_shape = (x_render, y_render)
        self.half = half
        self.random = random

        if random:
            self.locations = np.random.rand(x_phosphenes * y_phosphenes, 2)
        else:
            self.locations = np.array([
                (x / (x_phosphenes - 1), y / (y_phosphenes - 1))
                for x in range(x_phosphenes)
                for y in range(y_phosphenes)
            ])

        if half:
            self.locations[:, 0] = self.locations[:, 0] + 1 / 2

        self.sizes = np.array([
            ((x_render // x_phosphenes) // 2,
             (y_render // y_phosphenes) // 2)
            for x in range(x_phosphenes)
            for y in range(y_phosphenes)
        ])

        self.strengths = np.array([
            1
            for x in range(x_phosphenes)
            for y in range(y_phosphenes)
        ])

        self.volume = self.prerender_volume(
            x_render,
            y_render,
        )

#+end_src
#+LATEX: \end{footnotesize}

**** Polar Grid
<<sec:appendix:polar_grid>>

#+LATEX: \begin{footnotesize}
#+begin_src python :eval no
class PolarGrid(AbstractGrid):

    def __init__(
            self,
            n_theta: int,
            n_radius: int,
            x_render: int,
            y_render: int,
            half: bool = False,
            random: bool = False,
    ) -> None:
        """ Create a polar grid of phosphenes (size varies with eccentricity).

        :param n_theta: number of angles in the grid.
        :param n_radius: number of radii in the grid.
        :param x_render: the number of pixels in the x dimension of the render.
        :param y_render: the number of pixels in the y dimension of the render.
        :param half: whether to fill the right-half of the render (default False)
        :param random: whether to randomise the locations of phosphenes (default False).
        """

        self._set_id()
        self.grid_type = GridType.POLAR
        self.num_phosphenes = n_theta * n_radius
        self.render_shape = (x_render, y_render)
        self.half = half
        self.random = random

        if random:
            self.locations = np.random.rand( n_theta * n_radius, 2 )
            if half:
                self.locations[:, 0] = self.locations[:, 0] + 1 / 2
        else:
            self.locations = np.array([
                (0.5 + (i_radius / n_radius * np.cos(self.i_angle(i_theta, n_theta, half))) / 2,
                 0.5 + (i_radius / n_radius * np.sin(self.i_angle(i_theta, n_theta, half))) / 2,)
                for i_radius in range(1, n_radius + 1)
                for i_theta in range(n_theta)
            ])

        k = x_render / 2 + y_render / 2
        a = e * (x_render + y_render) / 32

        self.sizes = np.array([
            (np.log(k * ((x - 0.5) ** 2 + (y - 0.5) ** 2) + a),
             np.log(k * ((x - 0.5) ** 2 + (y - 0.5) ** 2) + a))
            for (x, y) in self.locations
        ])

        self.strengths = np.array([
            1
            for _ in range(n_theta)
            for _ in range(n_radius)
        ])

        self.volume = self.prerender_volume(
            x_render,
            y_render,
        )

    def i_angle(self, i_theta: float, n_theta: float, half: bool = False) -> float:
        """ Calculates the angle for angle of index i in range n.
        """
        if half:
            angle = (np.pi / (n_theta - 1) * i_theta) - (np.pi / 2)
        else:
            angle = 2 * np.pi / n_theta * i_theta
        return angle
#+end_src
#+LATEX: \end{footnotesize}

** Supplementary Qualitative Results

Below are several randomly selected additional qualitative training results; qualitative results from all training runs can be found in the code repository listed in Appendix [[sec:appendix_code]].

#+NAME: fig:examples_appendix
#+CAPTION[Supplemental qualitative results.]: Supplemental qualitative results of phosphene pattern outputs for the trained architecture, randomly sampled; patterns are shown in rows of 10, each correspoding to the output of digits 0 - 9 respectively.
[[file:./images/example_appendix.png]]

* References
bibliographystyle:vancouver
bibliography:refs.bib
#+LATEX: \end{appendices}
