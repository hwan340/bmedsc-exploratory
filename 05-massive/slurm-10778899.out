2019-08-09 10:01:27.977658: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-08-09 10:01:28.037803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-08-09 10:01:28.038925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:00:07.0
2019-08-09 10:01:28.046196: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-08-09 10:01:28.094772: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-08-09 10:01:28.116113: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-08-09 10:01:28.129805: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-08-09 10:01:28.182590: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-08-09 10:01:28.207907: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-08-09 10:01:28.272202: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-08-09 10:01:28.272418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-08-09 10:01:28.274553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-08-09 10:01:28.275556: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-08-09 10:01:28.276975: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-08-09 10:01:28.428463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-08-09 10:01:28.430401: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x24f3b60 executing computations on platform CUDA. Devices:
2019-08-09 10:01:28.430462: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2019-08-09 10:01:28.449952: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2693655000 Hz
2019-08-09 10:01:28.450121: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x255dd40 executing computations on platform Host. Devices:
2019-08-09 10:01:28.450176: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-08-09 10:01:28.450367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-08-09 10:01:28.451387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:00:07.0
2019-08-09 10:01:28.451442: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-08-09 10:01:28.451460: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-08-09 10:01:28.451475: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-08-09 10:01:28.451514: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-08-09 10:01:28.451530: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-08-09 10:01:28.451544: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-08-09 10:01:28.451560: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-08-09 10:01:28.451625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-08-09 10:01:28.453880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-08-09 10:01:28.454864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-08-09 10:01:28.456524: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-08-09 10:01:28.458029: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-08-09 10:01:28.458060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-08-09 10:01:28.458077: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-08-09 10:01:28.460796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-08-09 10:01:28.461880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-08-09 10:01:28.462902: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30555 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:00:07.0, compute capability: 7.0)
WARNING: Logging before flag parsing goes to stderr.
W0809 10:01:31.709824 140580982118208 hdf5_format.py:171] No training configuration found in save file: the model was *not* compiled. Compile it manually.
W0809 10:27:11.629300 140580982118208 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2019-08-09 10:27:13.876604: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] constant folding failed: Invalid argument: Unsupported type: 21
2019-08-09 10:27:14.094111: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] constant folding failed: Invalid argument: Unsupported type: 21
2019-08-09 10:27:14.301622: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1483] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2019-08-09 10:27:14.430247: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-08-09 10:27:15.208267: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-08-09 14:36:11.005260: W tensorflow/core/framework/op_kernel.cc:1546] OP_REQUIRES failed at logging_ops.cc:161 : Not found: ./output/C-2019-08-09-10-01-27-873887/encoder_losses.log; No such file or directory
2019-08-09 14:36:11.027799: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Not found: ./output/C-2019-08-09-10-01-27-873887/encoder_losses.log; No such file or directory
	 [[{{node PrintV2}}]]
	 [[PrintV2_1/_73]]
2019-08-09 14:36:11.027917: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Not found: ./output/C-2019-08-09-10-01-27-873887/encoder_losses.log; No such file or directory
	 [[{{node PrintV2}}]]
Starting epoch 0 at Fri Aug  9 10:27:10 2019
Time for epoch 0 is 744.7904448509216 seconds.
Starting epoch 1 at Fri Aug  9 10:39:35 2019
Time for epoch 1 is 734.3264825344086 seconds.
Starting epoch 2 at Fri Aug  9 10:51:49 2019
Time for epoch 2 is 740.5284721851349 seconds.
Starting epoch 3 at Fri Aug  9 11:04:10 2019
Time for epoch 3 is 736.1432628631592 seconds.
Starting epoch 4 at Fri Aug  9 11:16:26 2019
Time for epoch 4 is 739.974377155304 seconds.
Starting epoch 5 at Fri Aug  9 11:28:46 2019
Time for epoch 5 is 739.2745914459229 seconds.
Starting epoch 6 at Fri Aug  9 11:41:05 2019
Time for epoch 6 is 738.1076276302338 seconds.
Starting epoch 7 at Fri Aug  9 11:53:23 2019
Time for epoch 7 is 738.0819613933563 seconds.
Starting epoch 8 at Fri Aug  9 12:05:41 2019
Time for epoch 8 is 746.7103457450867 seconds.
Starting epoch 9 at Fri Aug  9 12:18:08 2019
Time for epoch 9 is 740.1114821434021 seconds.
Starting epoch 10 at Fri Aug  9 12:30:28 2019
Time for epoch 10 is 737.226735830307 seconds.
Starting epoch 11 at Fri Aug  9 12:42:46 2019
Time for epoch 11 is 741.6439092159271 seconds.
Starting epoch 12 at Fri Aug  9 12:55:07 2019
Time for epoch 12 is 747.1942920684814 seconds.
Starting epoch 13 at Fri Aug  9 13:07:34 2019
Time for epoch 13 is 744.2727739810944 seconds.
Starting epoch 14 at Fri Aug  9 13:19:59 2019
Time for epoch 14 is 743.5254805088043 seconds.
Starting epoch 15 at Fri Aug  9 13:32:22 2019
Time for epoch 15 is 740.0465092658997 seconds.
Starting epoch 16 at Fri Aug  9 13:44:42 2019
Time for epoch 16 is 744.9114670753479 seconds.
Starting epoch 17 at Fri Aug  9 13:57:07 2019
Time for epoch 17 is 738.7115468978882 seconds.
Starting epoch 18 at Fri Aug  9 14:09:26 2019
Time for epoch 18 is 744.5733587741852 seconds.
Starting epoch 19 at Fri Aug  9 14:21:50 2019
Time for epoch 19 is 743.1309697628021 seconds.
Starting epoch 20 at Fri Aug  9 14:34:14 2019
Traceback (most recent call last):
  File "./train.py", line 145, in <module>
    train()
  File "./train.py", line 138, in train
    train_step(real_images_slice, real_labels_slice)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py", line 404, in __call__
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py", line 1335, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py", line 589, in _filtered_call
    (t for t in nest.flatten((args, kwargs), expand_composites=True)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py", line 671, in _call_flat
    outputs = self._inference_function.call(ctx, args)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py", line 445, in call
    ctx=ctx)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File "<string>", line 3, in raise_from
tensorflow.python.framework.errors_impl.NotFoundError: 2 root error(s) found.
  (0) Not found:  ./output/C-2019-08-09-10-01-27-873887/encoder_losses.log; No such file or directory
	 [[node PrintV2 (defined at ./train.py:87) ]]
  (1) Not found:  ./output/C-2019-08-09-10-01-27-873887/encoder_losses.log; No such file or directory
	 [[node PrintV2 (defined at ./train.py:87) ]]
	 [[PrintV2_1/_73]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_step_565370]

Errors may have originated from an input operation.
Input Source operations connected to node PrintV2:
 map/while (defined at ./train.py:80)

Input Source operations connected to node PrintV2:
 map/while (defined at ./train.py:80)

Function call stack:
train_step -> train_step

